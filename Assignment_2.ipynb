{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZNSDOBJcBm"
      },
      "source": [
        "# Assignment II: Pretraining & Fine-Tuning of Language Models\n",
        "\n",
        "In this second assignment we will continue to work with PyTorch and Open AI's early Open Source Model GPT2 to develop a deeper understanding and intuition of how language models are trained. We will look at a specific simple task, Sentiment Classification, and see in two ways how we can use language models for this problem.\n",
        "\n",
        "The structure of the Assignment is as follows:\n",
        "\n",
        "1. **Continued GPT-2 Pretraining of GPT with a Movie Plots dataset**  \n",
        "\n",
        "   Here we will explore how continued pretraining affects a Language Model. This gives us a good idea how pretraining morks, and more specifically, we will look how additional domain-specific pretraining affects the perplexity for text within this domain vs outside of the domain. We will use the *CMU Movie Summary Corpus* (https://www.cs.cmu.edu/~ark/personas/), released under the Creative Commons Attribution-ShareAlike License (http://creativecommons.org/licenses/by-sa/3.0/us/legalcode).\n",
        "   We will learn that additional pretraining generally helps language models for in-domain tasks.\n",
        "\n",
        "3. **Fine-tuning of GPT-2 for Sentiment Analysis of the IMDB dataset (using Pre/Post-Modifiers)**  \n",
        "   We will then use both the original GPT-2 model and the model that was further pretrained on the CMU Movie Summary dataset for a Sentiment Analysis of the IMDB movie review dataset, which is part of Hugging Face datasets. We will (hopefully(!)... there are statistical fluctuations) see that fine-tuning the model that was further pre-trained on the movie plot dataset behaves somewhat better than the original gpt-2 model fine-tuned.\n",
        "\n",
        "4. **IMDB Sentiment Classification with a Masked Language Model (BERT)**\n",
        "   Finally, we will also look at a Masked Language Model, specifically BERT, as a tool for Sentiment Classification of the same dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0IwczMZFtrF"
      },
      "source": [
        "For reference, please consider the Lecture material for weeks 2 & 3 as well as the two Special Session notebooks:\n",
        "\n",
        "* Intro to PyTorch I (Basics)\n",
        "* Intro to PyTorch II (Huggingface & Language Models)\n",
        "* All lesson material and notebooks to date\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "\n",
        "* This notebook needs to be run using a GPU. If you use Google Colab, a T4 chip is the recommendation.\n",
        "  \n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the answers file as you did in a1. Please do not remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n",
        "\n",
        "* \\### YOUR CODE HERE indicates that you are supposed to write code. All the way up to \\### END YOUR CODE     \n",
        "\n",
        "* **Important!!:** When you are done please re-run your notebook from beginning to end to that all of the seeds apply! This is very important!\n",
        "\n",
        "**AUTOGRADER:**\n",
        "\n",
        "- In each code block, do NOT delete the ### comment at the top of a cell (it's needed for the auto-grading!)\n",
        "  - Full autograder tests and results are on gradescope.\n",
        "  - You will get the first 3 points from the autograder for this assignment.\n",
        "  - You may upload and run the autograder as many times as needed in your time window to get full points\n",
        "  - The assignment needs to be named Assignment_2.ipynb to be graded from the autograder!\n",
        "  - The examples given are samples of how we will test/grade your code.\n",
        "    - Please ensure your code outputs the exact same information / format!\n",
        "    - In addition to the given example, the autograder will test other examples\n",
        "    - Each autograder test tells you what input it is using\n",
        "  - Once complete, the autograder will show each tests, if that test is passed or failed, and your total score\n",
        "  - The autograder fails for a couple of reasons:\n",
        "    - Your code crashes with that input (for example: `Test Failed: string index out of range`)\n",
        "    - Your code output does not match the 'correct' output (for example: `Test Failed: '1 2 3 2 1' != '1 4 6 4 1'`)\n",
        "- Please format your input and output strings to be user friendly\n",
        "- Adding comments in your code is strongly suggested but won't be graded.\n",
        "- Do not delete the output cells.  We want to see your code AND the results it produced when it ran.\n",
        "- If you are stuck on a problem or do not understand a question - please come to office hours or ask questions (please don't post your code though). If it is a coding problem send a private message on Ed Discussion or send and email to your instructor.\n",
        "- We also have TA tutors for extra help and 1 on 1 sessions!\n",
        "- You may use any libraries from the Python Standard Library for this assignment: https://docs.python.org/3/library/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peRi8BBqFtrF"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "Let us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GuN074G-JK_T"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "#!pip install torch           # not required for colabs. Uncomment if needed otherwise\n",
        "#!pip install transformers    # not required for colabs. Uncomment if needed otherwise\n",
        "#!pip install numpy           # not required for colabs. Uncomment if needed otherwise\n",
        "!pip install portalocker\n",
        "!pip install -U datasets fsspec huggingface_hub # Hugging Face's dataset library\n",
        "#!pip install pandas          # not required for colabs. Uncomment if needed otherwise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymhMnEdJFtrF"
      },
      "source": [
        "Next, we will import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NySGJMIlFtrF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "#from torchtext import data as torchtext_data\n",
        "from torch import nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZmkkCXxuf0o"
      },
      "source": [
        "Let's make sure we will later put data and models on the proper device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9piY_5XnuVa9",
        "outputId": "2e6cd93f-da28-45fa-9d8a-ca195d4a2f44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device(\"mps\")               # in case you run on a local Mac with metal performance shaders (setup/support is up to you)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t91i97P4FtrG"
      },
      "source": [
        "Now we will define various functions that we will use in this notebook. You can jump over this part... but you don't have to.\n",
        "\n",
        "We'll define:\n",
        "\n",
        "1. perplexity(text, model) - a calculation giving us the perplexity for a given text and model. 'How certain is the model in picking the actual nect token?'\n",
        "2. ClassificationData class - a class that created the Dataset for our IMDB classification with GPT-2. It has a number of options that we'll use to augment the text to make the classification easier for the model.\n",
        "3. BERT ClassificationData class - same for a BERT model.\n",
        "4. create_temp_set(base_data, split) - a function used to massage the IMDB dataset, just as we did in PyTorch Intro I.\n",
        "5. random_huggingface_blog_text - A list of text of random Hugging Face blog snippets for some validation tests.\n",
        "6. fake_data_loader -  a function that converts an array of text (fixed batch size for now) into a format that the perplexity calculation can use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vbplEALAFtrG"
      },
      "outputs": [],
      "source": [
        "#@title Some Definitions\n",
        "\n",
        "def perplexity(text_data, model):\n",
        "\n",
        "    loss = []\n",
        "    for step, batch_data in enumerate(text_data):\n",
        "\n",
        "      batch_input = batch_data['input']\n",
        "      batch_labels = batch_data['labels']\n",
        "\n",
        "      if step % 100 == 0:\n",
        "          print('Current batch: ', step)\n",
        "\n",
        "      with torch.no_grad():\n",
        "            try:\n",
        "              batch_output = model(batch_input)\n",
        "              batch_output_reshaped = batch_output.reshape((batch_size * (max_len - 1), -1))\n",
        "\n",
        "              batch_labels_reshaped = batch_labels.reshape((batch_size * (max_len - 1),))\n",
        "              batch_loss = loss_fn(batch_output_reshaped, batch_labels_reshaped)\n",
        "\n",
        "              loss.append(batch_loss)\n",
        "            except:\n",
        "              continue\n",
        "\n",
        "    avg_cost = np.mean([x.cpu().detach() for x in loss])\n",
        "    avg_perplexity = np.exp(avg_cost)\n",
        "\n",
        "    return avg_perplexity\n",
        "\n",
        "\n",
        "class ClassificationData(Dataset):\n",
        "    def __init__(self,\n",
        "                 base_data,\n",
        "                 tokenizer,\n",
        "                 max_len,\n",
        "                 use_prompt=False,\n",
        "                 prompt_pre_text='',\n",
        "                 prompt_post_text='',\n",
        "                 classification_tokenset={1: 'good', 0: 'bad'},\n",
        "                 num_examples=-1):\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n",
        "\n",
        "\n",
        "        prompt_pre_text = prompt_pre_text.strip()\n",
        "        prompt_post_text = prompt_post_text.strip()\n",
        "\n",
        "\n",
        "        for num_example, example in enumerate(base_data):\n",
        "\n",
        "            if num_examples != -1 and num_example >= num_examples:\n",
        "              break\n",
        "\n",
        "            if num_example == 0:\n",
        "              print(example)\n",
        "\n",
        "            stripped_example = example['text'].strip()\n",
        "\n",
        "            token_encoder = self.tokenizer(' ' + stripped_example)['input_ids'] # simulating that the text will not be at the beginning\n",
        "\n",
        "            if len(token_encoder) <= self.max_len:\n",
        "                continue    # avoids complications with short sentences. No padding is needed then.\n",
        "\n",
        "            truncated_encoding = token_encoder[:self.max_len]\n",
        "            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n",
        "\n",
        "\n",
        "            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n",
        "\n",
        "\n",
        "            if use_prompt:\n",
        "\n",
        "                additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(' ' + prompt_post_text)['input_ids'])  # simulating that the prompt_post_text will not be at the beginning\n",
        "\n",
        "                cutoff = self.max_len + additional_token_length\n",
        "\n",
        "                prompted_text_line = prompt_pre_text + truncated_example + ' ' + prompt_post_text\n",
        "\n",
        "            else:\n",
        "                cutoff = self.max_len\n",
        "                prompted_text_line = truncated_example\n",
        "\n",
        "            if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "            tokenized_example = self.tokenizer(prompted_text_line,\n",
        "                                               return_tensors=\"pt\",\n",
        "                                               max_length=cutoff,\n",
        "                                               truncation=True,\n",
        "                                               padding='max_length').to(device)\n",
        "\n",
        "            if example['label'] == 1:\n",
        "              token = classification_tokenset[1]\n",
        "            else:\n",
        "              token = classification_tokenset[0]\n",
        "\n",
        "            token_id = self.tokenizer.encode(' ' + token)[0]\n",
        "            label = torch.tensor(token_id, dtype=torch.int64, device=device)\n",
        "\n",
        "            self.data.append({'label': label,\n",
        "                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n",
        "                              })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.data[index]['input_ids'],\n",
        "            'label': self.data[index]['label']\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.data[index]['input_ids'],\n",
        "            'label': self.data[index]['label']\n",
        "        }\n",
        "\n",
        "\n",
        "class BERTClassificationData(Dataset):\n",
        "    def __init__(self,\n",
        "                 base_data,\n",
        "                 tokenizer,\n",
        "                 max_len,\n",
        "                 num_examples=-1):\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n",
        "\n",
        "\n",
        "\n",
        "        for num_example, example in enumerate(base_data):\n",
        "\n",
        "            if num_examples != -1 and num_example >= num_examples:\n",
        "              break\n",
        "\n",
        "\n",
        "            token_encoder = self.tokenizer(example['text'])['input_ids']\n",
        "\n",
        "            if len(token_encoder) <= self.max_len:\n",
        "                continue    # avoids complications with short sentences. No padding is needed then.\n",
        "\n",
        "            truncated_encoding = token_encoder[1:self.max_len + 1]\n",
        "            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n",
        "\n",
        "            cutoff = self.max_len\n",
        "            prompted_text_line = truncated_example\n",
        "\n",
        "            tokenized_example = self.tokenizer(prompted_text_line,\n",
        "                                               return_tensors=\"pt\",\n",
        "                                               max_length=cutoff,\n",
        "                                               truncation=True,\n",
        "                                               padding='max_length').to(device)\n",
        "\n",
        "            label_val = example['label']\n",
        "\n",
        "            label = torch.tensor(label_val, dtype=torch.int64, device=device)\n",
        "\n",
        "            self.data.append({'label': label,\n",
        "                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n",
        "                              })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.data[index]['input_ids'],\n",
        "            'label': self.data[index]['label']\n",
        "        }\n",
        "\n",
        "\n",
        "def create_temp_set(base_data, num_examples=1000000000):\n",
        "    num_positive = 0\n",
        "    num_negative = 0\n",
        "    num_other = 0\n",
        "\n",
        "    temp_data = []\n",
        "    out_data = []\n",
        "\n",
        "    for example_num, example in enumerate(base_data):\n",
        "\n",
        "      temp_data.append(example)\n",
        "\n",
        "    random.shuffle(temp_data)\n",
        "\n",
        "    for example_num, example in enumerate(temp_data):\n",
        "\n",
        "      if num_examples != -1 and example_num > num_examples:\n",
        "        break\n",
        "\n",
        "      if example['label'] == 0:\n",
        "        num_negative += 1\n",
        "      elif example['label'] == 1:\n",
        "        num_positive += 1\n",
        "      else:\n",
        "        num_other += 1\n",
        "\n",
        "      out_data.append(example)\n",
        "\n",
        "\n",
        "    print('positive: ', num_positive)\n",
        "    print('negative: ', num_negative)\n",
        "    print('other: ', num_other)\n",
        "\n",
        "    return out_data\n",
        "\n",
        "\n",
        "random_huggingface_blog_text = [\"\"\"1. Aspect candidate extraction\n",
        "\n",
        "In this work we assume that aspects, which are usually features of products and services, are mostly nouns or noun compounds (strings of consecutive nouns). We use spaCy to tokenize and extract nouns/noun compounds from the sentences in the (few-shot) training set. Since not all extracted nouns/noun compounds are aspects, we refer to them as aspect candidates.\n",
        "\n",
        "2. Aspect/Non-aspect classification\n",
        "\n",
        "Now that we have aspect candidates, we need to train a model to be able to distinguish between nouns that are aspects and nouns that are non-aspects. For this purpose, we need training samples with aspect/no-aspect labels. This is done by considering aspects in the training set as True aspects, while other non-overlapping candidate aspects are considered non-aspects and therefore labeled as False:\n",
        "\n",
        "Training sentence: \"Waiters aren't friendly but the cream pasta is out of this world.\"\n",
        "Tokenized: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n",
        "Extracted aspect candidates: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n",
        "Gold labels from training set, in BIO format: [B-ASP, O, O, O, O, O, B-ASP, I-ASP, O, O, O, O, O, .]\n",
        "Generated aspect/non-aspect Labels: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n",
        "Now that we have all the aspect candidates labeled, how do we use it to train the candidate aspect classification model? In other words, how do we use SetFit, a sentence classification framework, to classify individual tokens? Well, this is the trick: each aspect candidate is concatenated with the entire training sentence to create a training instance using the following template:\"\"\",\n",
        "             \"\"\"Normalization interrogations\n",
        "During our first deeper dive in these surprising behavior, we observed that the normalization step was possibly not working as intended: in some cases, this normalization ignored the correct numerical answers when they were directly followed by a whitespace character other than a space (a line return, for example). Let's look at an example, with the generation being 10\\n\\nPassage: The 2011 census recorded a population of 1,001,360, and the gold answer being 10.\n",
        "\n",
        "Normalization happens in several steps, both for generation and gold:\n",
        "\n",
        "Split on separators |, -, or The beginning sequence of the generation 10\\n\\nPassage: contain no such separator, and is therefore considered a single entity after this step.\n",
        "Punctuation removal The first token then becomes 10\\n\\nPassage (: is removed)\n",
        "Homogenization of numbers Every string that can be cast to float is considered a number and cast to float, then re-converted to string. 10\\n\\nPassage stays the same, as it cannot be cast to float, whereas the gold 10 becomes 10.0.\n",
        "Other steps A lot of other normalization steps ensue (removing articles, removing other whitespaces, etc.) and our original example becomes 10 passage 2011.0 census recorded population of 1001360.0.\n",
        "However, the overall score is not computed on the string, but on the bag of words (BOW) extracted from the string, here {'recorded', 'population', 'passage', 'census', '2011.0', '1001360.0', '10'}, which is compared with the BOW of the gold, also normalized in the above manner, {10.0}. As you can see, they don’t intersect, even though the model predicted the correct output!\n",
        "\n",
        "In summary, if a number is followed by any kind of whitespace other than a simple space, it will not pass through the number normalization, hence never match the gold if it is also a number! This first issue was likely to mess up the scores quite a bit, but clearly it was not the only factor causing DROP scores to be so low. We decided to investigate a bit more.\n",
        "\n",
        "Diving into the results\n",
        "Extending our investigations, our friends at Zeno joined us and undertook a much more thorough exploration of the results, looking at 5 models which were representative of the problems we noticed in DROP scores: falcon-180B and mistral-7B were underperforming compared to what we were expecting, Yi-34B and tigerbot-70B had a very good performance on DROP correlated with their average scores, and facebook/xglm-7.5B fell in the middle.\n",
        "\n",
        "You can give analyzing the results a try in the Zeno project here if you want to!\n",
        "\n",
        "The Zeno team found two even more concerning features:\n",
        "\n",
        "Not a single model got a correct result on floating point answers\n",
        "High quality models which generate long answers actually have a lower f1-score\n",
        "At this point, we believed that both failure cases were actually caused by the same root factor: using . as a stopword token (to end the generations):\n",
        "\n",
        "Floating point answers are systematically interrupted before their generation is complete\n",
        "Higher quality models, which try to match the few-shot prompt format, will generate Answer\\n\\nPlausible prompt for the next question., and only stop during the plausible prompt continuation after the actual answer on the first ., therefore generating too many words and getting a bad f1 score.\n",
        "We hypothesized that both these problems could be fixed by using \\n instead of . as an end of generation stop word.\"\"\",\n",
        "             \"\"\"Text generation is a rich topic, and there exist several generation strategies for different purposes. We recommend this excellent overview on the subject. Many generation algorithms are supported by the text generation endpoints, and they can be configured using the following parameters:\n",
        "\n",
        "do_sample: If set to False (the default), the generation method will be greedy search, which selects the most probable continuation sequence after the prompt you provide. Greedy search is deterministic, so the same results will always be returned from the same input. When do_sample is True, tokens will be sampled from a probability distribution and will therefore vary across invocations.\n",
        "temperature: Controls the amount of variation we desire from the generation. A temperature of 0 is equivalent to greedy search. If we set a value for temperature, then do_sample will automatically be enabled. The same thing happens for top_k and top_p. When doing code-related tasks, we want less variability and hence recommend a low temperature. For other tasks, such as open-ended text generation, we recommend a higher one.\"\"\",\n",
        "             \"\"\"Recently, we released our Object Detection Leaderboard, ranking object detection models available in the Hub according to some metrics. In this blog, we will demonstrate how the models were evaluated and demystify the popular metrics used in Object Detection, from Intersection over Union (IoU) to Average Precision (AP) and Average Recall (AR). More importantly, we will spotlight the inherent divergences and pitfalls that can occur during evaluation, ensuring that you're equipped with the knowledge not just to understand but to assess model performance critically.\n",
        "\n",
        "Every developer and researcher aims for a model that can accurately detect and delineate objects. Our Object Detection Leaderboard is the right place to find an open-source model that best fits their application needs. But what does \"accurate\" truly mean in this context? Which metrics should one trust? How are they computed? And, perhaps more crucially, why some models may present divergent results in different reports? All these questions will be answered in this blog.\n",
        "\n",
        "So, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderboard! If you prefer to skip the introduction and learn how object detection metrics are computed, go to the Metrics section. If you wish to find how to pick the best models based on the Object Detection Leaderboard, you may check the Object Detection Leaderboard section.\"\"\"]\n",
        "\n",
        "def fake_data_loader(text, tokenizer, max_len):\n",
        "  return [{'input': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, :max_len-1].to(device),\n",
        "            'labels': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, 1:max_len].to(device)}]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEh7So11FtrG"
      },
      "source": [
        "This should say 'cpu' if using a CPU, or 'cuda', if a GPU is used (or 'mps' per the comment about macs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiXfjjjGFtrG"
      },
      "source": [
        "Now we are ready to move to Language Models.\n",
        "\n",
        "## 1. Continued Pretraining of GPT2 with a Movie Plots dataset\n",
        "\n",
        "We are now downloading GPT-2 from Hugging Face, i.e. the tokenizer and the model. We will i) make sure that it is on the proper device, and ii) copy the model to a second one that will see additional pre-training before being used for a classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rTut1Px2FtrG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "torch.manual_seed(10)\n",
        "random.seed(10)\n",
        "np.random.seed(10)\n",
        "\n",
        "gpt_2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "base_gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
        "additinal_pretrain_gpt2_model = copy.deepcopy(base_gpt2_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gav_H2C8FtrH"
      },
      "source": [
        "We will now continue to pretrain the model *additinal_pretrain_gpt2_model* on a dataset of a specific domain - movies. We use the *CMU Movie Summary Corpus* (https://www.cs.cmu.edu/~ark/personas/, license: http://creativecommons.org/licenses/by-sa/3.0/us/legalcode). It contains 40k+ unlabeled movie plot summaries. As such, they represent domain-specific text which is available at many companies and institutions using their internal documents.\n",
        "\n",
        "Get the dataset by uncommenting the first line below (we have it commented here because you may need to rerun the notebook multiple times when you already have the dataset. When you do that... make sure you comment out this line again):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y6V4R2VDFZ3h",
        "outputId": "9d5c59b4-011a-4add-aaa2-3484e2e6b4ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-15 04:10:56--  https://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48002242 (46M) [application/x-gzip]\n",
            "Saving to: ‘MovieSummaries.tar.gz’\n",
            "\n",
            "MovieSummaries.tar. 100%[===================>]  45.78M   598KB/s    in 80s     \n",
            "\n",
            "2025-06-15 04:12:17 (583 KB/s) - ‘MovieSummaries.tar.gz’ saved [48002242/48002242]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
        "# We reccomend downloading the file! You can then upload the file later when needed from your local computer. Go to the folder icon on the left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R3VRuFsevZb8",
        "outputId": "90a82a0a-46fe-4741-9425-045df9da33aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MovieSummaries/\n",
            "MovieSummaries/tvtropes.clusters.txt\n",
            "MovieSummaries/name.clusters.txt\n",
            "MovieSummaries/plot_summaries.txt\n",
            "MovieSummaries/README.txt\n",
            "MovieSummaries/movie.metadata.tsv\n",
            "MovieSummaries/character.metadata.tsv\n"
          ]
        }
      ],
      "source": [
        "# uncomment this tar command to run once. After you have the raw files you can don't need to untar again\n",
        "!tar -xvf MovieSummaries.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YSDM6Z9FtrH"
      },
      "source": [
        "Next, we will create a list of 15k plots and convince ourselves that the data looks roughly as expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e9Z8Ivc0FqiH",
        "outputId": "770d3ff7-8d10-4130-c8f9-6d52a9d3ade3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole survivor is rewarded with fame and wealth. In her first Reaping, 12-year-old Primrose Everdeen is chose'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "plots =  pd.read_csv('MovieSummaries/plot_summaries.txt', delimiter='\\t')\n",
        "plots.columns = ['id', 'plot']\n",
        "plot_summary_list = [x for x in plots['plot']][:15000]\n",
        "plot_summary_list[0][:400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAR7ej4nlHdb"
      },
      "source": [
        "Next, we will create a Dataset class that takes text data and returns input token ids and labels for the next word predictions (simply the input token ids shifted one to the left). For simplicity, we will throw out any examples that are shorter than our desired length and truncate all other examples to this length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N9xh-xTXRzYD"
      },
      "outputs": [],
      "source": [
        "#@title Class for Creation of Continued Pretraining Dataset (Movie Plots)\n",
        "\n",
        "class ContinuedPretrainData(Dataset):\n",
        "    def __init__(self, base_data, tokenizer, max_len, device):\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        tokenized_examples = tokenizer(base_data,\n",
        "                                       max_length=max_len,\n",
        "                                       truncation=True, padding='max_length',\n",
        "                                       return_tensors=\"pt\")\n",
        "\n",
        "        tokens = tokenized_examples['input_ids'][tokenized_examples['attention_mask'][:, max_len - 1] > 0]\n",
        "\n",
        "        self.data = tokens.to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {'input': self.data[index][:self.max_len - 1],\n",
        "                'labels': self.data[index][1:]\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nFW8dovRG0D"
      },
      "source": [
        "Now, please build a simple neural net that serves for continued pre-training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OKG3AXjSRzpZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "class ContinuedTrainingNetwork(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Build a simple PyTorch network that takes a batch (from a Dataloader)\n",
        "    as input and returns the logits of each next word prediction.\n",
        "    When instantiated, you need to pass in a pretrained base model.\n",
        "    You need to define both, the __init__ and the forward methods.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrainModel):\n",
        "        ### YOUR CODE HERE\n",
        "        super().__init__()\n",
        "        self.pretrainModel = pretrainModel\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, x):               # x stands for the input that the network will use/act on later\n",
        "        # get the logits for all tokens in all examples and call it logits.\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        logits = self.pretrainModel(x).logits\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return logits\n",
        "\n",
        "pretrain_network = ContinuedTrainingNetwork(pretrainModel=additinal_pretrain_gpt2_model)\n",
        "\n",
        "pretrain_network.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv4d8UFCFtrH"
      },
      "source": [
        "Then we create the training sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JpoJG71tRzg-"
      },
      "outputs": [],
      "source": [
        "max_len=100\n",
        "\n",
        "train_data = ContinuedPretrainData(plot_summary_list[:10000], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)\n",
        "test_data = ContinuedPretrainData(plot_summary_list[10000:], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PUss4GF7Rzjt",
        "outputId": "a6aec97b-e705-479b-ebe9-8289361da9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': tensor([  464,  3277,   286,  5961,   368, 10874,   286,   257, 11574, 13241,\n",
              "           290, 14104, 26647, 12815,    13,  1081,  9837,   329,   257,  1613,\n",
              "         21540,    11,  1123,  4783,  1276,  2148,   257,  2933,   290,  2576,\n",
              "           220,  1022,   262,  9337,   286,  1105,   290,  1248,  6163,   416,\n",
              "         22098,   220,   329,   262,  5079, 32367,  5776,    13,   383,   256,\n",
              "          7657,  1276,  1907,   284,   262,  1918,   287,   281, 13478,    26,\n",
              "           262,  6195, 23446,   318, 20945,   351, 16117,   290,  5129,    13,\n",
              "           554,   607,   717,   797,  9269,    11,  1105,    12,  1941,    12,\n",
              "           727, 11460, 13698, 10776, 39060,   318,  7147,   422,  5665,  1105,\n",
              "            13,  2332,  4697,  6621,  8595,    77,   747, 11661,   284],\n",
              "        device='cuda:0'),\n",
              " 'labels': tensor([ 3277,   286,  5961,   368, 10874,   286,   257, 11574, 13241,   290,\n",
              "         14104, 26647, 12815,    13,  1081,  9837,   329,   257,  1613, 21540,\n",
              "            11,  1123,  4783,  1276,  2148,   257,  2933,   290,  2576,   220,\n",
              "          1022,   262,  9337,   286,  1105,   290,  1248,  6163,   416, 22098,\n",
              "           220,   329,   262,  5079, 32367,  5776,    13,   383,   256,  7657,\n",
              "          1276,  1907,   284,   262,  1918,   287,   281, 13478,    26,   262,\n",
              "          6195, 23446,   318, 20945,   351, 16117,   290,  5129,    13,   554,\n",
              "           607,   717,   797,  9269,    11,  1105,    12,  1941,    12,   727,\n",
              "         11460, 13698, 10776, 39060,   318,  7147,   422,  5665,  1105,    13,\n",
              "          2332,  4697,  6621,  8595,    77,   747, 11661,   284,  1011],\n",
              "        device='cuda:0')}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Z4ZV6GgTAH"
      },
      "source": [
        "Here is the data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yEFJx0qiRzmJ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(10)\n",
        "batch_size = 4\n",
        "train_texts = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_texts = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s50t7hGkFtrH"
      },
      "source": [
        "Next, we construct a network that takes the input from the loaders and returns the logits of each next word prediction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i-Z0llqFtrH"
      },
      "source": [
        "Test the shape of the output. Is it correct? We first need to grab an example and then look at the shape of the model output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "V6yUS2tQRztk",
        "outputId": "a3db0365-c50b-433d-b7fb-79aa95a39b9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 99, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "example_data = next(iter(test_texts))\n",
        "\n",
        "# Please call your model output pretrain_model_output\n",
        "\n",
        "### YOUR CODE HERE\n",
        "pretrain_model_output = pretrain_network(example_data['input'])\n",
        "### END YOUR CODE\n",
        "pretrain_model_output.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zik6v4lIFtrH"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.a. What do the numbers above refer to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StU3e-D3f-0c"
      },
      "outputs": [],
      "source": [
        "### Q1-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "4: batch size\n",
        "99: sequence length\n",
        "50257: vocabulary size\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygyfBr_2FtrH"
      },
      "source": [
        "Next, we will calculate the initial perplexity for the test set of the movie plot summaries. We need the loss function for this. Please use the cross entropy to define the loss function *loss_fn*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uSQ_n6vDRzyx",
        "outputId": "eec4f4a4-3a43-4f03-aa7f-d0a0ad19ab61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3036)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "\"\"\"\n",
        "Define the loss function loss_fn as the cross entropy and validate/report on the\n",
        "calculation for the average loss for the two examples\n",
        "\n",
        "example 1:\n",
        "  label: 0\n",
        "  logits: [-3.1, -2.4]\n",
        "example 2:\n",
        "  label: 1\n",
        "  logits: [2.4, -3.1]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "### YOUR CODE HERE\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "test_input = torch.tensor([[[-3.1, -2.4], [2.4, -3.1]]], dtype=torch.float32)\n",
        "test_target = torch.tensor([[0, 1]], dtype=torch.int64)\n",
        "### END YOUR CODE\n",
        "\n",
        "loss_fn(test_input,test_target)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0_QsbCVDGcU"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.b. What is the average loss for these two examples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb0lvm5GgNcT"
      },
      "outputs": [],
      "source": [
        "### Q1-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "3.3036\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evqpJ8gIgMpY"
      },
      "source": [
        "1.c. (Ideally, by just looking at the labels and logits), which example contributes the higher loss? Choose from 'first' or 'second'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNZKTISJgOIb"
      },
      "outputs": [],
      "source": [
        "### Q1-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "second\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxR6qSGOFtrI"
      },
      "source": [
        "Also, consider https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html and investigate the dimensions of the input! We will need to reshape the output and the labels, because CrossEntropy expects a tensor of individual decisions, not a tensor of decision sequences. Similar for the labels. Recall how to reshape from the previous notebook.\n",
        "\n",
        "For the example data we calculate the loss. Then you need to calculate the perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XGgN9KjUMkWQ",
        "outputId": "ff360ca5-1a47-450a-ace6-f158423a506e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of reshaped outputs:  torch.Size([396, 50257])\n",
            "Shape of reshaped labels:  torch.Size([396])\n"
          ]
        }
      ],
      "source": [
        "# Reshape pretrain_model_output and example_data['labels']. Name them reshaped_pretrain_model_output and reshaped_pretrain_model_labels\n",
        "\n",
        "### YOUR CODE HERE\n",
        "reshaped_pretrain_model_output = pretrain_model_output.view(-1, pretrain_model_output.shape[-1])\n",
        "reshaped_pretrain_model_labels = example_data['labels'].view(-1)\n",
        "### END YOUR CODE\n",
        "\n",
        "print('Shape of reshaped outputs: ', reshaped_pretrain_model_output.shape)\n",
        "print('Shape of reshaped labels: ', reshaped_pretrain_model_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u_AaKxpYFtrI",
        "outputId": "2bf56011-da40-416a-db2c-d8e719bae311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial batch loss:  tensor(3.8234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Initial batch perplexity:  tensor(45.7585, device='cuda:0', grad_fn=<ExpBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Now use the loss function loss_fn to first calculate - for this batch - the loss and then the perplexity.\n",
        "\n",
        "### YOUR CODE HERE\n",
        "initial_batch_loss = loss_fn(reshaped_pretrain_model_output, reshaped_pretrain_model_labels)\n",
        "initial_batch_perplexity = torch.exp(initial_batch_loss)\n",
        "### END YOUR CODE\n",
        "\n",
        "print('Initial batch loss: ', initial_batch_loss)\n",
        "print('Initial batch perplexity: ', initial_batch_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQpSH7TUFtrI"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.d. What is the perplexity of this batch before the training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqtDy3ssgj0V"
      },
      "outputs": [],
      "source": [
        "### Q1-d Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "45.7585\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLSOPMqLFtrI"
      },
      "source": [
        "Next, we will calculate the perplexity of the whole test set. For that we will use the perplexity function defined at the outset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "imll7TlSrRmY",
        "outputId": "c5dd7ac4-8870-41bb-f0fb-947ca1da9a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch:  0\n",
            "Current batch:  100\n",
            "Current batch:  200\n",
            "Current batch:  300\n",
            "Current batch:  400\n",
            "Current batch:  500\n",
            "Current batch:  600\n",
            "Current batch:  700\n",
            "Current batch:  800\n",
            "Current batch:  900\n",
            "CPU times: user 22.6 s, sys: 11.2 s, total: 33.7 s\n",
            "Wall time: 33.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(49.22481)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "test_movie_plot_perplexity_before =  perplexity(test_texts, pretrain_network)\n",
        "test_movie_plot_perplexity_before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTBvxzNwksC8"
      },
      "source": [
        "**QUESTION**:\n",
        "\n",
        "1.e. What is the perplexity of the test set before further training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8du-OST5gu5X"
      },
      "outputs": [],
      "source": [
        "### Q1-e Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "49.22481\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABTv8wdBQfhz"
      },
      "source": [
        "Ok. What about random text not from this domain? Let us look at the random snippets from Hugging Face blogs defined above in random_huggingface_blog_text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0Sq1xBa3JMBv",
        "outputId": "b9eb814d-cbdf-401b-a3f0-5fbe23af4688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch:  0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(48.412525)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "test_hf_perplexity_before = perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\n",
        "test_hf_perplexity_before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLLMsPPtFtrI"
      },
      "source": [
        "Good, about the same. (As hoped/expected. The model should not have any particular better understanding for either type of text.)\n",
        "\n",
        "Next, we need to create the optimizer and generate a training loop. Nothing to do here for you, but take a look if interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "HQQdjKecFtrI"
      },
      "outputs": [],
      "source": [
        "pretrain_optimizer = torch.optim.AdamW(pretrain_network.parameters(), lr=0.00001)\n",
        "\n",
        "def continued_train_loop(dataloader,\n",
        "               model,\n",
        "               loss_fn,\n",
        "               optimizer,\n",
        "               reporting_interval=100,\n",
        "               max_len=100,\n",
        "               steps=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Write the training loop for continued pre-training. In particular, you need to:\n",
        "    - initialize the epoch_loss to 0 and set the model into training mode\n",
        "    - iterate over the batches:\n",
        "      - break if you are at 'steps' number of batches\n",
        "      - get the inputs X and labels y (which in this case will be the actual next token)\n",
        "      - get the model outputs\n",
        "      - reshape y and model outputs in proper format for cross entropy calculation\n",
        "      - zero out the gradient\n",
        "      - calculate loss\n",
        "      - propagate loss (loss.backward) and apply optimizer step\n",
        "      - add the loss to the epoch_loss\n",
        "    Reporting:\n",
        "      - report the current average loss and perplexity every 'reporting_interval' batches\n",
        "      - report the average loss and perplexity at the end of the epoch (done for you)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    batch_perplexity = 0\n",
        "    batch_loss = 0\n",
        "\n",
        "    for batch, data in enumerate(dataloader):\n",
        "        if (steps is not None) and (batch >= steps):\n",
        "          break\n",
        "\n",
        "        inputs = data['input']\n",
        "        labels = data['labels']\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        reshaped_outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        reshaped_labels = labels.view(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_fn(reshaped_outputs, reshaped_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if ((batch+1) % reporting_interval) == 0:\n",
        "            avg_loss = epoch_loss/(batch+1)\n",
        "            avg_perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "            print(f\"Batch {batch + 1}: Avg Loss = {avg_loss:.8f}, Avg Perplexity = {avg_perplexity:.8f}\")\n",
        "\n",
        "    ### END YOUR CODE\n",
        "    print(batch)\n",
        "    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n Avg train perplexity: {np.exp(epoch_loss/batch):>8f} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBuMQv3-FtrI"
      },
      "source": [
        "Now we do the training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9DahA3McFtrI",
        "outputId": "dd6dcfe0-0652-4837-fdaa-cd2b8294aad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 100: Avg Loss = 3.72844400, Avg Perplexity = 41.61431122\n",
            "Batch 200: Avg Loss = 3.72947317, Avg Perplexity = 41.65715408\n",
            "Batch 300: Avg Loss = 3.71865743, Avg Perplexity = 41.20903397\n",
            "Batch 400: Avg Loss = 3.71755587, Avg Perplexity = 41.16365814\n",
            "Batch 500: Avg Loss = 3.71375749, Avg Perplexity = 41.00760269\n",
            "Batch 600: Avg Loss = 3.70597850, Avg Perplexity = 40.68983841\n",
            "Batch 700: Avg Loss = 3.70399657, Avg Perplexity = 40.60928345\n",
            "Batch 800: Avg Loss = 3.70448581, Avg Perplexity = 40.62915421\n",
            "Batch 900: Avg Loss = 3.70154895, Avg Perplexity = 40.51000595\n",
            "Batch 1000: Avg Loss = 3.69879588, Avg Perplexity = 40.39862823\n",
            "1000\n",
            "Training Results: \n",
            "  Avg train loss: 3.698796 \n",
            " Avg train perplexity: 40.398630 \n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    # we just train for 1000 batches\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    continued_train_loop(train_texts, pretrain_network, loss_fn, pretrain_optimizer, steps=1000)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq71uMOeFtrI"
      },
      "source": [
        "How did the perplexity of the test set change after the additional pre-training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1YAz5ORhFtrJ",
        "outputId": "4d242ad5-1731-4c67-a6e1-9a94b29e23f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch:  0\n",
            "Current batch:  100\n",
            "Current batch:  200\n",
            "Current batch:  300\n",
            "Current batch:  400\n",
            "Current batch:  500\n",
            "Current batch:  600\n",
            "Current batch:  700\n",
            "Current batch:  800\n",
            "Current batch:  900\n",
            "CPU times: user 24.8 s, sys: 11.2 s, total: 36 s\n",
            "Wall time: 37.3 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(41.759407)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "test_movie_plot_perplexity_after = perplexity(test_texts, pretrain_network)\n",
        "test_movie_plot_perplexity_after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY0Ri85JRWht"
      },
      "source": [
        "What about the Hugging Face blog snippets that were not in the movie domain:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wQfYKFVgRW1w",
        "outputId": "bfc6b87f-c358-4377-8f8d-a3ea24c39d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch:  0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float32(72.38276)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "test_hf_perplexity_after =  perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\n",
        "test_hf_perplexity_after"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SgmygRcuVa5"
      },
      "source": [
        "\n",
        "**QUESTION:**\n",
        "\n",
        "1.f. What is your observation about the perplexity change for the test movie plot set texts after the additional pre-training? About the same ('within 2'), higher, or lower?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR1-as2Eg3li"
      },
      "outputs": [],
      "source": [
        "### Q1-f Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5dAROz3g34c"
      },
      "source": [
        "1.g. What is your observation about the perplexity change for the Hugging Face texts after the additional pre-training? About the same ('within 2'), higher, or lower?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-YW0jhZg4FE"
      },
      "outputs": [],
      "source": [
        "### Q1-g Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkaprBcg4Nm"
      },
      "source": [
        "1.h. (Free form) What would these observations imply in terms of where/how this model could be used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls5mzAmPg4WG"
      },
      "outputs": [],
      "source": [
        "### Q1-h Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9VdYiNgw_jm"
      },
      "source": [
        "## 2. Sentiment Classification of the IMDB Movie dataset using GPT2 and Prompts\n",
        "\n",
        "We will now get the IMDB dataset, just like we did in the PyTorch Intro II notebook. Refer to it for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU4fUhe6BEJZ"
      },
      "outputs": [],
      "source": [
        "imdb_dataset = load_dataset(\"IMDB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF3CSiIKD752"
      },
      "outputs": [],
      "source": [
        "imdb_dataset['train'][10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5UaatHFEjEK"
      },
      "outputs": [],
      "source": [
        "imdb_train_set = create_temp_set(imdb_dataset['train'])\n",
        "imdb_test_set = create_temp_set(imdb_dataset['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzsNC6aFtrJ"
      },
      "source": [
        "Usually we would first get the Dataset and the Dataloader, however for reasons that hopefully become apparent, in this case we first want to build the network. You should think of the network as taking input_ids $x$ from a batch of suitably tokenized sentences, and the **output should be the logits of the last token for each example in the batch.**\n",
        "\n",
        "Please fill in the missing line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGSGslw1BJtk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "class TokenPredictionNetworkClass(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Define a simple PyTorch network that takes a batch (from a Dataloader)\n",
        "    as input and returns the logits for the last next-token prediction.\n",
        "    When instantiated, you need to pass in a pretrained base language model\n",
        "    (the 'logit_model').\n",
        "    You need to define both, the __init__ and the forward methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, logit_model):\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get the logits for the last position of each each example. (Call them last_token_logits.). This will be just one line.\n",
        "        # Use self.logit_model(x) to get the model output\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return last_token_logits\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "token_prediction_network_base_model = TokenPredictionNetworkClass(logit_model=base_gpt2_model)\n",
        "token_prediction_network_addnl_pretrain_model = TokenPredictionNetworkClass(logit_model=additinal_pretrain_gpt2_model)\n",
        "\n",
        "token_prediction_network_base_model.to(device)\n",
        "token_prediction_network_addnl_pretrain_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dad6GNLWBdvb"
      },
      "source": [
        "We now construct our training and test sets for the Sentiment Classification. We want to follow a different approach than we have in the PyTorch intro II notebook. We want to leverage the language model and what it is good at - predicting the next tokens - to the maximum. So why not put a 'wrapper' around the review in a way that the proper sentiment would be naturally the next word?\n",
        "\n",
        "As a simple example, rather than trying to use the last output vector and add a classification layer let's try to reframe the problem like this (as an illustrative example):\n",
        "\n",
        "  \n",
        " \"This is a review: <truncated review text>... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\n",
        "\n",
        " or\n",
        "\n",
        " \"This is a review: <truncated review text>... The reviewer has positive or negative sentiments about movies. In this case the sentiment was\"\n",
        "\n",
        " ...\n",
        "\n",
        " One would think that the LM should already do a decent job getting the proper sentiment simply using the next word prediction task it is trained on!\n",
        "\n",
        " How could we test this? We could simply consider the cross entropy loss for the next token relative to the actual sentiment, i.e. the next word we would expect for a positive or a negative review.\n",
        "\n",
        " So we can experient with:\n",
        "\n",
        " * The pre-fix before the review\n",
        " * The text after the review\n",
        " * The words we would expect for pos/neg reviews\n",
        "\n",
        "Try a few combinations and see which ones give you the lowest loss.\n",
        "\n",
        "**NOTE:** Usually we also would do a good chunk of text pre-processing (take out html, etc.), but for simplicity we will ignore this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0yNQL1WFuJu"
      },
      "outputs": [],
      "source": [
        "class ClassificationData(Dataset):\n",
        "    def __init__(self,\n",
        "                 base_data,\n",
        "                 tokenizer,\n",
        "                 max_len,\n",
        "                 use_prompt=False,\n",
        "                 prompt_pre_text='',\n",
        "                 prompt_post_text='',\n",
        "                 classification_tokenset={1: 'good', 0: 'bad'},\n",
        "                 num_examples=-1):\n",
        "\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n",
        "\n",
        "\n",
        "\n",
        "        for num_example, example in enumerate(base_data):\n",
        "\n",
        "            if num_examples != -1 and num_example >= num_examples:\n",
        "              break\n",
        "\n",
        "            if num_example == 0:\n",
        "              print(example)\n",
        "\n",
        "\n",
        "            token_encoder = self.tokenizer(example['text'])['input_ids']\n",
        "\n",
        "            if len(token_encoder) <= self.max_len:\n",
        "                continue    # avoids complications with short sentences. No padding is needed then.\n",
        "\n",
        "            truncated_encoding = token_encoder[:self.max_len]\n",
        "            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n",
        "\n",
        "            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n",
        "\n",
        "\n",
        "            if use_prompt:\n",
        "\n",
        "                additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(prompt_post_text)['input_ids'])\n",
        "                cutoff = self.max_len + additional_token_length - 1\n",
        "\n",
        "                prompted_text_line = prompt_pre_text + truncated_example + prompt_post_text\n",
        "\n",
        "            else:\n",
        "                cutoff = self.max_len\n",
        "                prompted_text_line = truncated_example\n",
        "\n",
        "            if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "            tokenized_example = self.tokenizer(prompted_text_line,\n",
        "                                               return_tensors=\"pt\",\n",
        "                                               max_length=cutoff,\n",
        "                                               truncation=True,\n",
        "                                               padding='max_length').to(device)\n",
        "\n",
        "            #if num_example == 0:\n",
        "            #  print(self.tokenizer.decode(tokenized_example['input_ids'][0]))\n",
        "\n",
        "            if example['label'] == 1:\n",
        "              token = classification_tokenset[1]\n",
        "            else:\n",
        "              token = classification_tokenset[0]\n",
        "\n",
        "            token_id = self.tokenizer.encode(' ' + token)[0]\n",
        "            label = torch.tensor(token_id, dtype=torch.int64, device=device)\n",
        "\n",
        "            self.data.append({'label': label,\n",
        "                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n",
        "                              })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.data[index]['input_ids'],\n",
        "            'label': self.data[index]['label']\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIavTztxFtrJ"
      },
      "outputs": [],
      "source": [
        "#Suggested, but try a bunch!\n",
        "# prompt_pre_text = 'Here is a movie review: '\n",
        "#prompt_post_text = ' ...  The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n",
        "#classification_tokenset = {1: 'good', 0: 'bad'}\n",
        "\n",
        "prompt_pre_text = 'Here is a movie review: '\n",
        "prompt_post_text = ' ...  The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n",
        "classification_tokenset = {1: 'good', 0: 'bad'}\n",
        "\n",
        "# make a modification to the prompt_pre_text, prompt_post_text, and classification_tokenset\n",
        "# that gets the loss below 1.7\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "play_data = ClassificationData(imdb_train_set,\n",
        "                                tokenizer=gpt_2_tokenizer,\n",
        "                                max_len=100,\n",
        "                                use_prompt=True,\n",
        "                                prompt_pre_text = prompt_pre_text,\n",
        "                                prompt_post_text = prompt_post_text,\n",
        "                                classification_tokenset=classification_tokenset,\n",
        "                                num_examples=20\n",
        "                                )\n",
        "\n",
        "batch_size = 4\n",
        "toy_texts = DataLoader(play_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "loss = 0\n",
        "predicted_tokens = []\n",
        "labels = []\n",
        "\n",
        "for batch_num, toy_text_batch in enumerate(toy_texts):\n",
        "    sample_output = token_prediction_network_base_model(toy_text_batch['input_ids']).to(device)\n",
        "    sample_labels = toy_text_batch['label']\n",
        "    loss += loss_fn(sample_output, sample_labels).detach()\n",
        "\n",
        "    predicted_tokens += gpt_2_tokenizer.decode(torch.argmax(sample_output, dim=-1)).split()\n",
        "    labels += gpt_2_tokenizer.decode(sample_labels).split()\n",
        "\n",
        "loss /= (batch_num + 1)\n",
        "\n",
        "\n",
        "\n",
        "print('Average loss: ', loss)\n",
        "print('Predicted tokens vs labels: ', [(x, y) for x,y in zip(predicted_tokens, labels)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB_NC-2yFtrJ"
      },
      "source": [
        "Ok, the accuracy using the old GPT2 is not exactly amazing (newer and larger models would be MUCH better out of the box). However, even for GPT2 at least a token of the right type is predicted. Fine-tuning should make this much better!\n",
        "\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.a. Write down two different prompt_pre_text/prompt_post_text combinations and their respective average loss. Pick one that sounds reasonable but is quite a bit worse (say, average loss > 3), and another that gets the loss below 1.7. (Note, for the later you probably havde to counteract a bit the model's tendency to be positve. You may also want to be more clear where the review\n",
        "starts and ends.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CD1XwvWh-vj"
      },
      "outputs": [],
      "source": [
        "### Q2-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEYWYxPnh6-x"
      },
      "source": [
        "Now let's do the fine-tuning that is supposed to help! Start by getting the full dataset and dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6seAp7dmFtrJ"
      },
      "outputs": [],
      "source": [
        "imdb_train_data = ClassificationData(imdb_train_set,\n",
        "                                tokenizer=gpt_2_tokenizer,\n",
        "                                max_len=100,\n",
        "                                use_prompt=True,\n",
        "                                prompt_pre_text = prompt_pre_text,\n",
        "                                prompt_post_text = prompt_post_text,\n",
        "                                classification_tokenset=classification_tokenset,\n",
        "                                num_examples=-1\n",
        "                                )\n",
        "\n",
        "imdb_test_data = ClassificationData(imdb_test_set,\n",
        "                                tokenizer=gpt_2_tokenizer,\n",
        "                                max_len=100,\n",
        "                                use_prompt=True,\n",
        "                                prompt_pre_text = prompt_pre_text,\n",
        "                                prompt_post_text = prompt_post_text,\n",
        "                                classification_tokenset=classification_tokenset,\n",
        "                                num_examples=-1\n",
        "                                )\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "imdb_train_loader = DataLoader(imdb_train_data, batch_size=batch_size, shuffle=True)\n",
        "imdb_test_loader = DataLoader(imdb_train_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lMTx6IUMFyv"
      },
      "source": [
        "Let's set up the optimizers as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zer3mvWGK89C"
      },
      "outputs": [],
      "source": [
        "adam_optimizer_base_model = torch.optim.AdamW(token_prediction_network_base_model.parameters(), lr=0.00001)\n",
        "adam_optimizer_addtl_pretrain_model = torch.optim.AdamW(token_prediction_network_addnl_pretrain_model.parameters(), lr=0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wA2F_LP-5Wqe"
      },
      "source": [
        "Here is the new training loop. Please fill in the lines for optimizer zeroing, the prediction calculation, and the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5B2VrbX1uLN"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n",
        "\n",
        "    \"\"\"\n",
        "    Write the training loop to fine-tune the model for sentiment\n",
        "    classification using the final next-token-prediction task.\n",
        "    In particular, you need to:\n",
        "    - initialize the epoch_loss to 0 and set the model into training mode\n",
        "    - iterate over the batches:\n",
        "      - break if you are at 'steps' number of batches\n",
        "      - get the inputs X and labels y (which in this case will be the actual next token)\n",
        "      - get the model outputs\n",
        "      - reshape y and model outputs in proper format for cross entropy calculation\n",
        "      - zero out the gradient\n",
        "      - calculate loss\n",
        "      - propagate loss (loss.backward) and apply optimizer step\n",
        "      - add the loss to the epoch_loss\n",
        "    Reporting:\n",
        "      - report the current average loss every 'reporting_interval' batches\n",
        "      - report the average loss at the end of the epoch (done for you)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n",
        "    \"\"\"\n",
        "    Write the test loop to fine-tune the model for sentiment classification using the final next-token-prediction task.\n",
        "    In particular, you need to:\n",
        "\n",
        "    - set the model into eval mode and initialize the test_loss to 0. Also, set the number of correct &\n",
        "      total test examples to 0, like:\n",
        "      'test_loss, correct_token_predictions,  correct_label_class, total = 0, 0, 0, 0'\n",
        "        (See the two approaches for accuracy below for correct_token_predictions\n",
        "        and correct_label_class)\n",
        "\n",
        "    - use torch.no_grad to iterate over the batches:\n",
        "        - break if you are at 'steps' number of batches\n",
        "        - from the batch, get the test inputs X and labels y (which in this case will be the actual next token). You may want to look at the format of batches by using 'next(iter(imdb_test_loader))' in a separate cell\n",
        "        - get the model outputs\n",
        "        - calculate loss and add to test_loss (reshaping should not be necessary)\n",
        "        - For the accuracy, we can try two approaches (and in this case they should turn out to be\n",
        "            probably the same in the end):\n",
        "              i) Test Class Accuracy:\n",
        "                  - Define the predicted class (I call it selected class) by comparing the logits for our two\n",
        "                    'evaluating tokens' (like 'good', 'bad'). if the logit for (in this example) 'good' is higher,\n",
        "                    then the predicted class is the positive one, etc.\n",
        "              ii)  Token Prediction Accuracy:\n",
        "                  - see how often the correct 'evaluation token' is predicted. I.e., here we do not compare\n",
        "                    whether the model believes that 'good' is a more likely next token than 'bad', but was\n",
        "                    'good' the actual next token prediction (and vise versa).\n",
        "              - get these numbers for each batch and add to the totals\n",
        "\n",
        "    - add the loss to the epoch_loss\n",
        "\n",
        "    Reporting:\n",
        "\n",
        "    - report on the average token accuracy, average class accuracy and average test loss every 'reporting_interval' batches\n",
        "    - report on the same at the end (done for you)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # let's get the proper class ids for the 'evaluating next tokens' (like 'good', 'bad')\n",
        "\n",
        "    if contrast_pair is not None:\n",
        "      class_1, class_2 = contrast_pair\n",
        "      class_1_id, class_2_id = gpt_2_tokenizer.encode(' ' + class_1 + ' ' + class_2)\n",
        "\n",
        "    # now the loop starts:\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    print(correct_label_class)\n",
        "    print(f\"Test Results: \\n\\t Test Token Accuracy: {(100*correct):>0.1f}% \\n\\t Test Class Accuracy: {(100*correct_label_class):>0.1f}%  \\n\\t Avg test loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfYYfoCK1uOX"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(imdb_train_loader, token_prediction_network_base_model, loss_fn, adam_optimizer_base_model, steps=2000)\n",
        "    test_loop(imdb_test_loader, token_prediction_network_base_model, loss_fn, contrast_pair=('good', 'bad'),\n",
        "              steps=500\n",
        "              ) # no optimizer use here!\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkfPjUBE8JWu"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.b. What was your test accuracy after fine-tuning, when starting with the base model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKxtT88eiVtJ"
      },
      "outputs": [],
      "source": [
        "### Q2-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxWvZSg4WAzj"
      },
      "source": [
        "Now we redo this for the model that saw the additional pre-training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O20xrf0gmscR"
      },
      "outputs": [],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(imdb_train_loader, token_prediction_network_addnl_pretrain_model, loss_fn, adam_optimizer_addtl_pretrain_model, steps=2000)\n",
        "    test_loop(imdb_test_loader, token_prediction_network_addnl_pretrain_model, loss_fn, contrast_pair=('good', 'bad'),\n",
        "              steps=500\n",
        "              ) # no optimizer use here!\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "221hDSFSaiee"
      },
      "source": [
        "This looks good! So we had better movie review sentiment classification using the model had had seen the additional pretraining.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGt5EUN2iV1i"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.c. What was your test accuracy after fine-tuning, when starting with the model that had additional pre-training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4urQlZKiV8v"
      },
      "outputs": [],
      "source": [
        "### Q2-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onqyZaW1iWC_"
      },
      "source": [
        "2.d. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data **inside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBQdmmNfiWLZ"
      },
      "outputs": [],
      "source": [
        "### Q2-d Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrpNZT03iWS2"
      },
      "source": [
        "2.e. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data  **outside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58K0RCyqiWdL"
      },
      "outputs": [],
      "source": [
        "### Q2-e Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyUulgoDuf0_"
      },
      "source": [
        "## 3. Sentiment Classification with BERT\n",
        "\n",
        "Now we will see how well the classification with BERT works in comparison. We will get the model tokenizer for that model, then - as discussed in class - use the output of the initial [CLS] token to classify the sentiment. Will it be better? Or worse?\n",
        "\n",
        "See https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel for more details around the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU5Aa6nY1vrM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from transformers import AutoTokenizer, BertModel\n",
        "\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-cased\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrILbM7Tuf0_"
      },
      "source": [
        "Let us look at a simple bert tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d63Uoyx_gbiP"
      },
      "outputs": [],
      "source": [
        "bert_toy_inputs = bert_tokenizer(\"This is new\", return_tensors=\"pt\").to(device)\n",
        "bert_toy_outputs = bert_model(**bert_toy_inputs)\n",
        "\n",
        "last_hidden_states = bert_toy_outputs.last_hidden_state\n",
        "\n",
        "last_hidden_states.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g-wccv3STm0"
      },
      "source": [
        "Play with decode method of the tokenizer to see why the shape is ... x 5 x ... . Then identify the first value of the output of the [CLS] token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W0c0wJ1Rwjf"
      },
      "outputs": [],
      "source": [
        "# Decode the tokenization. Call it bert_toy_tokens.\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END YOUR CODE\n",
        "print('The tokens after tokenization: ', bert_toy_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8QxOlH7XtoT"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "3.a Why is the shape .. x 5 x ... and not .. x 3 x ... ? Explain. (You may need to to look up what the purpose is of one of the extra tokens. Don't write more than 2-3 lines.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Lc1ysqi9_n"
      },
      "outputs": [],
      "source": [
        "### Q3-a Grading Tag: Please put your answer in this cell. Don't edit this line. (THIS IS NOT AN AUTOGRADER QUESTION)\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIjgqCXdRwvL"
      },
      "outputs": [],
      "source": [
        "# Get the output for the [CLS] token. Call it cls_first_out.\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END YOUR CODE\n",
        "print('First output of [CLS] token: ', cls_first_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMiyr5izQPaC"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "3.b What is the first value of the output of the [CLS] token?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFtukAZgjZjr"
      },
      "outputs": [],
      "source": [
        "### Q3-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABg5ttdeWfo4"
      },
      "source": [
        "Now we construct the dataset and the dataloader. The BERT dataset class is defined at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9USxNoQ_WgIg"
      },
      "outputs": [],
      "source": [
        "bert_train_data = BERTClassificationData(imdb_train_set,\n",
        "                                tokenizer=bert_tokenizer,\n",
        "                                max_len=100,\n",
        "                                num_examples=-1\n",
        "                                )\n",
        "\n",
        "bert_test_data = BERTClassificationData(imdb_test_set,\n",
        "                                tokenizer=bert_tokenizer,\n",
        "                                max_len=100,\n",
        "                                num_examples=-1\n",
        "                                )\n",
        "\n",
        "batch_size = 4\n",
        "bert_imdb_train_loader = DataLoader(bert_train_data, batch_size=batch_size, shuffle=True)\n",
        "bert_imdb_test_loader = DataLoader(bert_test_data, batch_size=batch_size, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeghP0kJTqpB"
      },
      "source": [
        "Now build the classification network that uses the output of the [CLS] token for the classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr-vOerxuf0_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "class BERTClassificationNetworkClass(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Write the class for the classification network using\n",
        "    the Masked Language Model BERT.\n",
        "    Specificaly, you will need to extract the output of the [CLS] token\n",
        "    from the BERT model (i.e., the very first token), apply a suitable linear layer,\n",
        "    and apply the sigmoid function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get the forward pass. Apply the BERT model, then the linear layer, and\n",
        "        # then apply the sigmoid\n",
        "        ### YOUR CODE HERE\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return torch.squeeze(sigmoid_output) # removing 'x 1 x ' dimensions\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "bert_classification_model = BERTClassificationNetworkClass().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jja3T50uYlde"
      },
      "source": [
        "Let's test it. Is the structure correct?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K1PVgiVYT23"
      },
      "outputs": [],
      "source": [
        "test = next(iter(bert_imdb_train_loader))\n",
        "\n",
        "out = bert_classification_model({'input_ids': test['input_ids']})\n",
        "\n",
        "loss = loss_fn(out.float(), test['label'].float())\n",
        "\n",
        "print('Output: ', out)\n",
        "print('Loss: ', loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPcaxhduYzxK"
      },
      "source": [
        "Good. Finally, we need train and test loops:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OydE2YjmYM6p"
      },
      "outputs": [],
      "source": [
        "def bert_train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n",
        "    \"\"\"\n",
        "    Following the same logic as above, write the training loop to use the\n",
        "    Masked Language Model BERT for the sentiment classification task. You\n",
        "    only need to report the average loss after the reporting interval\n",
        "    and end of each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n",
        "\n",
        "\n",
        "def bert_test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n",
        "    \"\"\"\n",
        "    Following the same logic as above, write the test loop to use the\n",
        "    Masked Language Model BERT for the sentiment classification task.\n",
        "    Please report on the accuracy after the reporting interval and end of each epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    correct = float(correct.cpu().detach().numpy())\n",
        "\n",
        "    test_loss /= batch\n",
        "    correct /= total\n",
        "    print(correct)\n",
        "\n",
        "    print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7L9eebiuf0_"
      },
      "source": [
        "Now let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weVynJqHuf0_"
      },
      "outputs": [],
      "source": [
        "adam_optimizer_bert = torch.optim.AdamW(bert_classification_model.parameters(), lr=0.00001)\n",
        "\n",
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    bert_train_loop(bert_imdb_train_loader, bert_classification_model, loss_fn, adam_optimizer_bert, steps=2000)\n",
        "    bert_test_loop(bert_imdb_test_loader, bert_classification_model, loss_fn,\n",
        "              steps=500\n",
        "              ) # no optimizer use here!\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U8i12wAuf1A"
      },
      "source": [
        "\n",
        "**QUESTION:**\n",
        "\n",
        "3.c What was the test accuracy you got for the BERT model?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asyvVWSBkeOi"
      },
      "outputs": [],
      "source": [
        "### Q3-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HByEKLxuS8R1"
      },
      "source": [
        "\n",
        "And that is it for assignment 2.  Congratulations!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}