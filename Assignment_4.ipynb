{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hZNSDOBJcBm"
      },
      "source": [
        "# Assignment IV: Using Prompts to Generate Pictures and Words\n",
        "\n",
        "In this fourth assignment we will continue to work with PyTorch and Hugging Face. They provide both code libraries. Hugging Face also provides model weights that allow us to perform the tasks we want to explore.\n",
        "\n",
        "The structure of the Assignment is as follows:\n",
        "\n",
        "1. **Generating images with [Stable Diffusion 1](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) using text prompts**  \n",
        "\n",
        "   Here we will explore how we can use text to instruct a model to generate an image.  We'll see how well the model can follow our instructions by asking it to produce specific numbers of objects.  We'll then use a different model to see if the numbers of objects produced is correct.\n",
        "\n",
        "2. **Examining the capabilities of Mistral**  \n",
        "   We will then use a recent large language model -- [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) from Mistral -- to illustrate the benefits of an increase in the number of parameters and how it affects the performance of the model.  This model doubles the number or parameters but has also undergone a better pre-training and post-training regime and we would expect that to be reflected in the performance of the model and its ability to follow instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0IwczMZFtrF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "\n",
        "* This notebook needs to be run using a GPU. If you use Google Colab, a T4 chip is the recommendation. You can run this in free Colab.\n",
        "  \n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the answers cells as you did in a1. Please do not remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n",
        "\n",
        "* \\### YOUR CODE HERE indicates that you are supposed to write code. All the way up to \\### END YOUR CODE  \n",
        "\n",
        "* \\### YOUR ANSWER HERE indicates that you are supposed to copy and paste an answer as a string into that cell.\n",
        "\n",
        "\n",
        "\n",
        "**AUTOGRADER:**\n",
        "\n",
        "- In each code block, do NOT delete the ### comment at the top of a cell (it's needed for the auto-grading!)\n",
        "  - **This assignment will be manually graded.**\n",
        "  - The assignment needs to be named Assignment_4.ipynb.\n",
        "  - The examples given are samples of how we will test/grade your code.\n",
        "    - Please ensure your code outputs the exact same information / format!\n",
        "    - In addition to the given example, the autograder will test other examples\n",
        "- Please format your input and output strings to be user friendly\n",
        "- Adding comments in your code is strongly suggested but won't be graded.\n",
        "- If you are stuck on a problem or do not understand a question - please come to office hours or ask questions (please don't post your code though). If it is a coding problem send a private email to your instructor.\n",
        "- We also have a couple of TA tutors for extra help and 1 on 1 sessions!\n",
        "- You may use any libraries from the Python Standard Library for this assignment: https://docs.python.org/3/library/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY7Q6CAWTtIj"
      },
      "source": [
        "## 1: Multimodality with Image Generation & Captioning\n",
        "\n",
        "In this fourth assignment we will continue to work with PyTorch and Hugging Face to get more experience with its abstract classes as well as some new models. We will look at the simple task of image generation and use a number of tools to see how we could programatically evaluate the accuracy of the image generation process.\n",
        "\n",
        "The structure of the Assignment is as follows:\n",
        "\n",
        "1. **Image Generation**  \n",
        "\n",
        "   Here we will explore how the stable diffusion model generates images conditioned on a text prompt.\n",
        "\n",
        "\n",
        "2. **Image Classification**\n",
        "\n",
        "   Here we will use CLIP to evaluate captions that describe our images to see which labels most accurately describes our generated images.\n",
        "\n",
        "3. **Image Evaluation**\n",
        "\n",
        "   We will also use a visual question answering system to ask questions about our generated image.  In our prompt we asked for certain items in the image.  In the question answering system we can ask if those items are present in the image.\n",
        "\n",
        "\n",
        "**Models we'll use in this exercise**\n",
        "\n",
        "**Stable Diffusion** - generate image from prompt\n",
        "\n",
        "**CLIP** - compare \"text\" labels with image\n",
        "\n",
        "**VQA** - answer questions about image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POL1zLnHIZyu"
      },
      "source": [
        "For reference, please consider the Lecture material for week 6 as well as the two notebooks:\n",
        "\n",
        "*Week_6_Lesson_Notebook_I_CLIP*\n",
        "\n",
        "*Week_6_Lesson_Notebook_III_Stable_Diffusion_Pipeline*\n",
        "\n",
        "### INSTRUCTIONS:\n",
        "\n",
        "* This notebook requires a T4 GPU.\n",
        "\n",
        "* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the answers file as you did in a1. Please do not remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n",
        "\n",
        "* \\### YOUR CODE HERE indicates that you are supposed to write code or a prompt.\n",
        "\n",
        "*\\### YOUR ANSWER HERE indicates that you are to copy and paste some content from an output cell of your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWnrgn6mwxjL"
      },
      "source": [
        "We'll need to use Google Drive to store the images we generate because we want to reuse them. You will need to edit the file path for the output folder variable below.  I'm using Colab Pro on my personal account and so have created a '267' folder in MyDrive where I can store the images I want to keep in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKbwvxLeyQlh"
      },
      "outputs": [],
      "source": [
        "#mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your directory\n",
        "output_folder = \"/content/drive/MyDrive/267/2025-summer/TestNotebooks/\"\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tceASp32w8fA"
      },
      "source": [
        "### 1.0 Environment Setup\n",
        "\n",
        "Let us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNLhhTlCyQik"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lHMLXu3Pew7"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q -U bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ue3g67RyQd-"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U invisible_watermark safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg0tGYLSKVAN"
      },
      "outputs": [],
      "source": [
        "#You can generate multiple images but not if you are using\n",
        "# the T4 GPU.  This assignment is designed to run with a single image\n",
        "\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz24tv8m10_D"
      },
      "source": [
        "### Stable Diffusion - Generate Images with one object\n",
        "\n",
        "We're going to generate an image using stable diffusion ([Model Card](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)). You will specify a prompt that names one object.  Be sure to specify that you want just one. I used ``\"portrait of a dog in a chair\"``.  How can we programatically tell if the generated image follows our prompt?  We can use some other tools that can examine the photo and tell if it's contents are what we asked for or if the generator fell short. First, you need to load the model and then generate and image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_imDUDkyQbm"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "sd_pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "sd_pipe.to(\"cuda\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2F3tfIWdMtE"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE/PROMPT HERE\n",
        "\n",
        "prompt = \"\"\n",
        "### END YOUR CODE/PROMPT\n",
        "images = sd_pipe(prompt=prompt).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6L_KeFDz_KH"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.a. What is the enhanced prompt you created to generate the image with the single object? Enter the prompt as a triple quote string in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyEzDDsk0-nC"
      },
      "outputs": [],
      "source": [
        "### Q1-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGjKQbfvZQRx"
      },
      "source": [
        "Now save the image in your google drive so that when you find one you like you can re-use it in subsequent steps.  We may ask you to show us your `test_single.png` image if it isn't visible in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrUjLFWcyQaB"
      },
      "outputs": [],
      "source": [
        "filename = output_folder + \"/test_single.png\"\n",
        "images.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTmlN_2gyQYH"
      },
      "outputs": [],
      "source": [
        "images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe13C4wMTn-m"
      },
      "source": [
        "### CLIP - evaluate image with one object\n",
        "\n",
        "Now we'll use the CLIP model ([Model Card](https://huggingface.co/openai/clip-vit-base-patch32)) to see if our image resembles the object in the prompt.  You'll need to edit the list of captions below.  One label in the list should be the object you requested in the prompt.  Other labels can be similiar objects and one should be orthogonal (very different from your chosen object).  Your labels should each be one or two words long (e.g. cat, dog, blue whale). This is similar to building a classifier that \"recognizes\" images by predicting one of many possible labels. You should have at least 4 labels.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLTX0CbsTn-n"
      },
      "outputs": [],
      "source": [
        "!pip install -q Pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1CmR5wMTn-n"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmNY_iJpTn-n"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "cl_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "cl_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0OsHJKITn-o"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img_url = '/content/drive/MyDrive/267/2025-spring/TestNotebooks/test_single.png'\n",
        "raw_image = Image.open(img_url, mode='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can apply your four labels here."
      ],
      "metadata": {
        "id": "PAT_JpJJFRB-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0-2_hPITn-o"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "captions = [\"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\"]\n",
        "\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "inputs = cl_processor(\n",
        "        text=captions, images=raw_image, return_tensors=\"pt\", padding=True\n",
        ")\n",
        "\n",
        "outputs = cl_model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image            # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)                # we can take the softmax to get the label probabilities\n",
        "\n",
        "\n",
        "print()\n",
        "for i, caption in enumerate(captions):\n",
        "   print('%40s - %.4f' % (caption, probs[0, i]))\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpr36z5j1HCb"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.b. What are all of the 1 or 2 word labels you gave to evaluate the generated image? Please list them in a quoted string e.g. \"label, label, label, label\" in the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pM_VAwx1Hga"
      },
      "outputs": [],
      "source": [
        "### Q1-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08VyJ9xq1fRB"
      },
      "source": [
        "\n",
        "**QUESTION:**\n",
        "\n",
        "1.c. What is the correct 1 or 2 word label and the score assigned to it by the CLIP model? Denote this as \"(label, number)\" in the cell below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPy_ffwp1lqj"
      },
      "outputs": [],
      "source": [
        "### Q1-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVDUgktTD_V"
      },
      "source": [
        "\n",
        "Great.  We know the CLIP model can handle more description of the content of the image.  Let's see how well that works.  Instead of your 1 or 2 word labels, create more descriptive captions of roughly 5 to 10 words each.  As with your labels, one caption in your list should describe the object you requested in the prompt. Other captions can be similiar objects and one should be orthogonal (very different from your chosen object).  You must have 4 captions for this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq-YYIK90P0X"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "captions = [\"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\"]\n",
        "\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "inputs = cl_processor(\n",
        "        text=captions, images=raw_image, return_tensors=\"pt\", padding=True\n",
        ")\n",
        "\n",
        "outputs = cl_model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image            # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)                # we can take the softmax to get the label probabilities\n",
        "\n",
        "\n",
        "print()\n",
        "for i, caption in enumerate(captions):\n",
        "   print('%40s - %.4f' % (caption, probs[0, i]))\n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5IT82Tm1yWy"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.d. What are the four 5 to 10 word captions you gave to evaluate the image? Enter your multiword captions as a single string \"caption one, caption two, caption three, caption four\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk_aCjkr14_u"
      },
      "outputs": [],
      "source": [
        "### Q1-d Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1D3MngV0P0X"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.e. What is the correct 5 or 10 word caption and the score assigned to it by the CLIP model?  Denote this as \"(caption, number)\" in the cell below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQBZ_xGA1-kd"
      },
      "outputs": [],
      "source": [
        "### Q1-e Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4x3HIEOVPY2"
      },
      "source": [
        "### Stable Diffusion -  Object Counts\n",
        "\n",
        "We're going to generate another image using stable diffusion.  For this new image you will specify a prompt with 3 of the same object *(Type 1)*, 2 of a different kind of object *(Type 2)* \"in the background\", and two other different individual objects *(Type 3)* and *(Type 4)* in the scene.  For example `3 cats in the garden with 2 snakes in the background with a flowering tree and a rose bush`. Make up your own prompt with your own set of type 1, type 2, type 3, and type 4 objects.\n",
        "\n",
        "How can we programatically tell if the generated image follows our prompt?  We can use some other tools that can examine the photo and tell if it's contents are what we asked for or if the generator fell short.\n",
        "\n",
        "Note that each time you run this cell you generate a new image. You may want to try several images before you select one for future processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGr65X_WUkm9"
      },
      "outputs": [],
      "source": [
        "#Your prompt here\n",
        "\n",
        "### YOUR CODE/PROMPT HERE\n",
        "\n",
        "prompt = \"\"\n",
        "\n",
        "### END YOUR CODE/PROMPT\n",
        "images = sd_pipe(prompt=prompt).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T20wtBrg2Vd8"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.f. What is the prompt you gave to generate your image with the four different types of objects by following our instructions?  Enter the prompt as a triple quoted string in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yODAkBaq2Isf"
      },
      "outputs": [],
      "source": [
        "### Q1-f Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kj8Z3dxz_KL"
      },
      "source": [
        "Now make sure you save the image you generate and want to work with as `test_counts.png`.  You will need to show us the image if it isn't visible in your notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zfNKNQNUkkF"
      },
      "outputs": [],
      "source": [
        "\n",
        "filename = output_folder + \"/test_counts.png\"\n",
        "images.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8l_EOxgUkeW"
      },
      "outputs": [],
      "source": [
        "images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiWYqfstY3aA"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "img_url = '/content/drive/MyDrive/267/2025-spring/TestNotebooks/test_counts.png'\n",
        "raw_image = Image.open(img_url, mode='r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvgklOV9DGz9"
      },
      "source": [
        "### BLIP for Visual Question Answering\n",
        "\n",
        "How can we measure how well our stable diffusion model is generating images?  We can look at our one image and assess, but what if we want to test at scale?  To do that we need some kind of automation.  We can leverage other models ot assist. There's a variation of BLIP ([Model Card](https://huggingface.co/Salesforce/blip-vqa-base)) that has been designed to answer questions about the contents of an image.  We'll use that functionality to ask questions to see if the generated image corresponds to the prompt we provided.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__QNIryG8gXJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
        "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\", torch_dtype=torch.float16).to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYT_wzzPz_KR"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.g. How many <type 1> objects does the VQA say are present in your generated image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnNhoBL7Y33D"
      },
      "outputs": [],
      "source": [
        "#question = \"how many <name of your type 1 objects> are in the picture?\"\n",
        "### YOUR CODE HERE\n",
        "question = \"\"\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=100)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl1IxmXO2wqg"
      },
      "outputs": [],
      "source": [
        "### Q1-g Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiiEXLSVz_KS"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.h. How many <type 2> objects does the VQA say are present in your generated image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvpJcmlpY87G"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "question = \"\"\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=100)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfMN3ATm3Dzx"
      },
      "outputs": [],
      "source": [
        "### Q1-h Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUcEGizEz_KS"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.i. How many <type 2> objects does the VQA say are present in the background of your generated image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wAoXNk6ZPJx"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "question = \"\"\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=100)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woLPFrU13KPl"
      },
      "outputs": [],
      "source": [
        "### Q1-i Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yvkIG3JyQC2"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.j. How many <type 3> objects does the VQA say are present in your generated image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWe-e8Zbz_KT"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "question = \"\"\n",
        "\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=100)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8o_pZw73NiG"
      },
      "outputs": [],
      "source": [
        "### Q1-j Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK0vOQ64z_KT"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "1.k. How many <type 4> objects does the VQA say are present in your generated image?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhODK48Iz_KT"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "question = \"\"\n",
        "### END YOUR CODE\n",
        "\n",
        "\n",
        "inputs = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=100)\n",
        "print(processor.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDVQPmno3RbZ"
      },
      "outputs": [],
      "source": [
        "### Q1-k Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhma1dtG3em_"
      },
      "source": [
        "**QUESTION: 1.l**\n",
        "\n",
        "How well does the BLIP VQA system do at accurately counting the number of objects in the image you generated?\n",
        "- a. It is perfect\n",
        "- b. It works pretty well but needs improvement\n",
        "- c. It sees things I don't see\n",
        "- d. It is less than 50% accurate\n",
        "\n",
        "Enter the letter of your answer below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w63PYf1wTgwl"
      },
      "outputs": [],
      "source": [
        "### Q1-l Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq4p4ojxT5xj"
      },
      "source": [
        "## 2: Prompt Engineering\n",
        "\n",
        "In this fourth assignment we will continue to work with PyTorch and Hugging Face to get more experience. We will work with the [Mistral 7B instruction fine tuned model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) to practice creating effective prompts.\n",
        "\n",
        "<a id = 'returnToTop'></a>\n",
        "\n",
        "The structure of the assignment is as follows:\n",
        "\n",
        "1. [**Synthetic Data Generation**](#synth-gen)  \n",
        "\n",
        "   Here we will explore how to generate synthetic review data.\n",
        "\n",
        "\n",
        "2. [**Synthetic Data Evaluation**](#synth-eval)\n",
        "\n",
        "   Let's evaluate the synthetic data we just generated and see how accurate it is.\n",
        "\n",
        "\n",
        "\n",
        "3. [**JSON Record Generation**](#synth-json)\n",
        "\n",
        "   Let's have the model generate some structured JSON records that incorporate our reviews as well as some other data we specify.\n",
        "\n",
        "\n",
        "\n",
        "4. [**Chain of Thought Generation**](#cot-gen)\n",
        "\n",
        "   Let's create a prompt to reason through a set of arithmetic problems and see if it can give the correct answer and \"show its work.\"\n",
        "\n",
        "\n",
        "\n",
        "5. [**Prompt Templates and Output Improvements**](#prompt-temp)\n",
        "\n",
        "  Let's leverage prompt templates and Ethan Mollick's \"incantations\" to write a description of your start up and then generate an elevator pitch for our company based on the description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8FpOqM8T5xk"
      },
      "source": [
        "For reference, please consider the Lecture material for week 7 as well as the notebook:\n",
        "\n",
        "`Week_7_Lesson_Notebook_Simple_Prompt_Examples.ipynb`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM9vu50WT5xk"
      },
      "source": [
        "### 2.0 Setup\n",
        "\n",
        "**You MUST disconnect and delete the previous runtime and then reconnect to work on section 2**\n",
        "\n",
        "First, some utility functions we'll use later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOklPgdty0k5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_text_between_tags(text, start_tag, end_tag):\n",
        "  pattern = fr'{start_tag}(.|\\n)*?{end_tag}'\n",
        "  cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "def remove_final_tag(text, end_tag):\n",
        "  pattern = fr'\\s?{end_tag}'\n",
        "  cleaned_text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "def ret_post_final_tag(text, end_tag):\n",
        "  cleaned_text = text.split(end_tag)[-1]\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "def remove_after_last_curlybrace(string):\n",
        "  last_brace_index = string.rfind('}')\n",
        "  if last_brace_index != -1 and last_brace_index != len(string) - 1:\n",
        "    string = string[:last_brace_index + 1]\n",
        "  return string\n",
        "\n",
        "\n",
        "start = \"<s>\\s?\\[INST\\]\"\n",
        "fin = \"\\[/INST\\]\"\n",
        "fin2 = \"</s>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA-_zRWKHUxq"
      },
      "source": [
        "\n",
        "\n",
        "[Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) is a small but highly performant model. It is also possible to use it commercially. The model has been instruction fine-tuned by Mistral so it should be able to follow your prompts and return good on point output.  We'll also use a quantized version (down to 4 bits) so we know it can load in our small GPU.  \n",
        "\n",
        "**Hugging Face and Mistral require you to register to use the model.**  Please go to the [model page](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) and either log in to your Hugging Face account or follow their directions to create one.  It is free.  Once you have an account you can use it to get permission to use the model.\n",
        "\n",
        "First let's load the libraries necessary for it to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fG9SdStIJn9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip uninstall -y transformers\n",
        "!pip install -q -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQfFGDXdPDet"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -q accelerate\n",
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoVy0mg4UZGu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pprint\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGE6IQusOrz"
      },
      "source": [
        "This is the bits and bytes config file where we specify our quantization arguments.  You can read about it [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XzWl0QiTmE1"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "### YOUR CODE HERE\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "\n",
        ")\n",
        "### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3r6r2534uZZ"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.0.a. Did you enter the config arguments? Yes or No.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esKOn3Ab434S"
      },
      "outputs": [],
      "source": [
        "### Q2-0-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg0MWHE3tKrE"
      },
      "source": [
        "\n",
        "This model has been trained to work with dialog, meaning instances here have multiple utterance and response pairs to create the context so the model can reply. We'll populate the context with only our prompt and not have any back and forth.\n",
        "\n",
        "First we'll ask the model to generate a blurb based on the title of the draft of the 3rd Edition of the pre-emininet textbook in computational linguistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPO9VFCByCi"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'synth-gen'></a>\n",
        "\n",
        "\n",
        "### 2.1 Synthetic Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Y4mTGQHVf3"
      },
      "outputs": [],
      "source": [
        "#Note: It can take up to 8 minutes to download this model\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" # the device to load the model onto\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVBbnI2h9Ni9"
      },
      "source": [
        "Let's set up a prompt to generate one blurb about the book.  This prompt is very simple.  Prompts can be significantly more complex.\n",
        "Do not modify this prompt.  Just run it as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q98XLIXmFOLh"
      },
      "outputs": [],
      "source": [
        "myprompt = (\n",
        "    \"write a three sentence description for the following text book: Jurafsky and Martin Speech and Language Processing (3rd ed. draft)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9oxpqiXsPYL"
      },
      "outputs": [],
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": myprompt}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "encodeds = encodeds.to(device)\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)\n",
        "\n",
        "\n",
        "generated_ids = model.generate(encodeds, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "blurb = decoded[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVQ9uoBuKe7v"
      },
      "outputs": [],
      "source": [
        "#let's look at the blurb we've generated\n",
        "blurb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbCEU63ENKC9"
      },
      "outputs": [],
      "source": [
        "#Let's clean up the old instruction and the closing tags\n",
        "cleaned1 = remove_text_between_tags(blurb, start, fin)\n",
        "cleaned_blurb = remove_final_tag(cleaned1, fin2)\n",
        "cleaned_blurb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYyhxQ0lxFWX"
      },
      "source": [
        "You can also try one of the generated blurbs below if you like.  The contents of your blurb can have an effect on downstram performance. Just uncomment the line you want to try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_qohfhX5pzP"
      },
      "outputs": [],
      "source": [
        "#cleaned_blurb = \"This comprehensive textbook, \\\"Speech and Language Processing\\\" by Daniel Jurafsky and James H. Martin, provides a thorough introduction to the fundamental principles and techniques of speech and language processing. Covering topics from basic concepts to advanced applications, the book offers a unified approach to the field, bridging the gap between linguistics, computer science, and engineering. With its clear explanations, real-world examples, and extensive exercises, this textbook is an essential resource for students and professionals in the fields of natural language processing, speech recognition, and human-computer interaction.\"\n",
        "#cleaned_blurb = \"This comprehensive textbook, \"Jurafsky and Martin Speech and Language Processing (3rd ed. draft)\", is a leading resource in the field of natural language processing and speech recognition, offering a thorough introduction to the fundamental concepts, theories, and techniques of speech and language processing. Written by renowned experts Daniel Jurafsky and James H. Martin, the book provides a clear and concise overview of the subject, covering topics such as phonetics, phonology, morphology, syntax, semantics, and pragmatics, as well as machine learning and statistical methods for speech and language processing. By combining theoretical foundations with practical applications, this textbook is an essential resource for students, researchers, and practitioners in the fields of computer science, linguistics, and cognitive science.\"\n",
        "#cleaned_blurb = \"Speech and Language Processing by Jurafsky and Martin is a comprehensive and authoritative textbook that provides a thorough introduction to the fundamental concepts and techniques of speech and language processing. This 3rd edition draft covers the latest advancements in the field, including natural language processing, speech recognition, and machine translation, making it an essential resource for students and researchers in computer science, linguistics, and related fields. By combining theoretical foundations with practical applications, the book offers a unique blend of technical rigor and real-world relevance, making it an indispensable guide for anyone interested in the rapidly evolving field of speech and language processing.\"\n",
        "#cleaned_blurb = \"Speech and Language Processing by Jurafsky and Martin is a comprehensive textbook that provides an in-depth exploration of the fundamental concepts, theories, and techniques in speech and language processing. This 3rd edition draft offers a detailed and up-to-date treatment of the field, covering topics such as speech recognition, natural language processing, and machine learning. By presenting the subject matter in a clear and accessible manner, the authors equip students and researchers with the knowledge and skills necessary to tackle the complex challenges in speech and language processing.\"\n",
        "#cleaned_blurb = \"Jurafsky and Martin\\'s Speech and Language Processing (3rd ed. draft) is a comprehensive textbook introducing students to the field of speech and language processing. It covers both spoken and written languages, exploring methods for analyzing and generating human language. The authors, renowned researchers in the field, provide practical insights and real-world applications, making the concepts accessible and engaging for beginner and advanced students alike.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppZMQedrt6hf"
      },
      "source": [
        "Now we'll ask the model to generate a set of reviews of the book using the blurb we just generated.  We'll also ask the model to create reviews that match a particular sentiment.  We have a list of those sentiments below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfQrr-jnkcSh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "# A list of data labels for 15 records using five distinct labels\n",
        "labels = [\"positive\", \"negative\", \"very negative\", \"neutral\", \"positive\", \"very positive\", \"neutral\", \"negative\", \"positive\", \"very positive\", \"very negative\", \"negative\", \"neutral\", \"very positive\", \"very negative\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op2yWprgCV0V"
      },
      "source": [
        "Let's generate some reviews of this book.  Specifically, we'll generate 15 reviews and each one will use one of the labels from the labels list associated with it e.g. label[5] will indicate the sentiment of review[5].\n",
        "\n",
        "Run the cell once to generate the 15 reviews to see how the loop works and see how well it performs.  Then you can go back and modify the prompt to improve the accuracy of the reviews.  Ideally, at this stage you should end up with at least over half of the reviews you generate reflecting the sentiment of the label associated with the review.  This is subjective as you just need to read them to see.  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNmcWK9pPrfb"
      },
      "outputs": [],
      "source": [
        "rev_rec_list = []\n",
        "\n",
        "for label in labels:\n",
        "  #This is the default prompt that incorporates the label.  Improve the prompt to get better reviews\n",
        "  # that more accurately reflect the label in the iterated label list.\n",
        "\n",
        "  ### YOUR CODE/PROMPT HERE\n",
        "\n",
        "  myprompt = f\"Write a {label} three sentence review. Provide only the review as output. Do not mention the {label}. Base the review on the following blurb: {cleaned_blurb}\"\n",
        "\n",
        "  ### END YOUR CODE\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": myprompt}\n",
        "  ]\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "  #model.to(device)\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "  print(\".\")\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  cleaned = decoded[0]\n",
        "\n",
        "  cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "  cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "  cleaned3 = ret_post_final_tag(cleaned2, fin)\n",
        "  rev_rec_list.append(cleaned3.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COIl2r_NPrfc"
      },
      "outputs": [],
      "source": [
        "#Let's see the reviews it generated\n",
        "for record in rev_rec_list:\n",
        "        print(record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY-1eKOg5X2d"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.1.a. What is the final improved prompt you are using to generate the reviews with the correct sentiment?  Enter it below as a string in triple quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAcY502H5Zqp"
      },
      "outputs": [],
      "source": [
        "### Q2-1-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhggXCjoCEG3"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'synth-eval'></a>\n",
        "\n",
        "### 2.2 Synthetic Data Evaluation\n",
        "\n",
        "The overall goal of this exercise is to have the model generate a non-verbose review that closely matches the labels in the labels list, e.g. \"the review is very negative\".  In the previous step you were looking at this yourself to determine if the review was matching the label.  We need to be more programtic about it if we want to scale.  You should write a new prompt that reads the review and indicates which label the model thinks applies to the review.  We want just the label and not a bunch of explanation. We can then compare the model's new assessment label with the label we used to generate the review. Given this evaluation, your goal is to emit reviews so that you get at least 7 of the 15 reviews reflect the given (\"correct\") sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yeoX5BsfRhx"
      },
      "outputs": [],
      "source": [
        "#Utility function to check predicted label against actual\n",
        "#\n",
        "def check_label(sentence, label_list):\n",
        "    lower_sentence = sentence.lower()\n",
        "    #lower_list = [label.lower() for label in label_list]\n",
        "    lower_list = label.lower()\n",
        "    #print(f\"list: {label} AND label_list: {lower_list} AND sentence: {lower_sentence}\")\n",
        "\n",
        "\n",
        "    # First, handle cases where 'very' is in the label\n",
        "    if ('very' in label and label in lower_sentence):\n",
        "        #print(f\"Match found1: {label} in {lower_sentence}\")\n",
        "        return True\n",
        "\n",
        "    # Check for the bad pattern match\n",
        "    pattern = r'\\b((overwhelmingly|strongly|extremely|wonderfully|super|overly|highly|solidly|forcefully|exceedingly|inordinately|unduly|predominantly|not|too)\\s+(positive|negative|neutral)\\s*)'\n",
        "    if re.search(pattern, lower_sentence):\n",
        "        #print(f\"Rejected at2: {re.search(pattern, lower_sentence).group(0)}\")\n",
        "        return False\n",
        "\n",
        "    # check is very label is in sentence but label doesn't include very\n",
        "    if (f\"very {label}\" in lower_sentence and 'very' not in label):\n",
        "        #print(f\"Rejected at3: {label} not in {lower_sentence}\")\n",
        "        return False\n",
        "\n",
        "    # check if label even occurs in the sentence\n",
        "    if (label not in lower_sentence):\n",
        "        #print(f\"Rejected at4: {label} not in {lower_sentence}\")\n",
        "        return False\n",
        "\n",
        "    # Finally, check for direct matches for other labels\n",
        "    if any(label in lower_sentence for label in label_list):\n",
        "        #print(f\"Match found5: {next(label for label in label_list if label in lower_sentence)}\")\n",
        "        return True\n",
        "\n",
        "    # If no match found\n",
        "    #print(f\"Rejected at6: {label} not in {lower_sentence}\")\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCH0Nms9Q6n9"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "answers = []\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  label = labels[i]                # correct answer\n",
        "  review = rev_rec_list[i]         #review\n",
        "\n",
        "  ### YOUR CODE/PROMPT HERE\n",
        "  myprompt = f\"\"\n",
        "\n",
        "  ### END YOUR CODE\n",
        "\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": myprompt}\n",
        "  ]\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  print(\".\")\n",
        "\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  cleaned = decoded[0]\n",
        "\n",
        "\n",
        "  cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "  cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "  result = check_label(cleaned2, label)\n",
        "  print(f\"Result: {result}\")\n",
        "  if result:\n",
        "    print(f\"{label} match returned\")\n",
        "    correct+=1\n",
        "  else:\n",
        "    print(f\"{label} not matched\")\n",
        "\n",
        "  answers.append(cleaned2)\n",
        "\n",
        "print(f\"There are {correct} correct labels out of {i+1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQOoZy815w9v"
      },
      "source": [
        "You can toggle between generating reviews and evaluating reviews.  You should modify the prompts to emit labels so that you get at least 7 of the 15 reviews correct.\n",
        "\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.2.a. What is the final prompt that you used to accurately assess the sentiment expressed in each review?  Enter your final improved promt as a triple quote string below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOGyTnG85xx0"
      },
      "outputs": [],
      "source": [
        "### Q2-2-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k70MoopP4Wka"
      },
      "source": [
        "\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.2.b. How many of your predicted sentiment labels are correct? Enter a number below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlRrFmlP5_pJ"
      },
      "outputs": [],
      "source": [
        "### Q2-2-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFsb8CP-Q6dc"
      },
      "outputs": [],
      "source": [
        "# As a reminder labels = [\"positive\", \"negative\", \"very negative\", \"neutral\", \"positive\", \"very positive\", \"neutral\", \"negative\", \"positive\", \"very positive\", \"very negative\", \"negative\", \"neutral\", \"very positive\", \"very negative\"]\n",
        "#See what is in the individual answers and why you might not get a correct label\n",
        "answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emOU_HroQ7fD"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'synth-json'></a>\n",
        "\n",
        "### 2.3 JSON Record Generation\n",
        "\n",
        "Now let's build on the prompt that generates reviews that correspond to the sentiment in the label.  You should change the prompt below to generate a JSON record that contains fields for author, title, review, and stars based on the blurb we used earlier (in the variable `cleaned_blurb`).  You can let the model fill in the values for these fields. We want the JSON records to be well formed.  At least 12 of the 15 records should be well formed JSON and contain *ALL* of the fields we request.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VTOzR9OHXJ2"
      },
      "outputs": [],
      "source": [
        "json_rec_list = []\n",
        "\n",
        "for label in labels:\n",
        "\n",
        "  ### YOUR CODE/PROMPT HERE\n",
        "\n",
        "  myprompt = f\"\"\n",
        "\n",
        "  ### END YOUR CODE\n",
        "\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": myprompt}\n",
        "  ]\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "  print(\"x\")\n",
        "\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  cleaned = decoded[0]\n",
        "\n",
        "  #Let's clean out the prompt text\n",
        "  cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "  cleaned2 = ret_post_final_tag(cleaned1, fin)\n",
        "  cleaned3 = remove_after_last_curlybrace(cleaned2)\n",
        "  json_rec_list.append(cleaned3.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwWVlrmU_m4X"
      },
      "source": [
        "You can eyeball the records below to see if they seem compliant with the JSON standard.  Again, we're looking for at least 12 of the 15 records to be compliant and include the correct fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxLUVRkv1ni"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "for record in json_rec_list:\n",
        "    try:\n",
        "        jrec = json.loads(record)\n",
        "        #print(json.dumps(record, indent=4)) #this alone will just print the record but won't check compliance\n",
        "        print(json.dumps(jrec, indent=4))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e} for record: {record}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvJjjqo46V4V"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.3.a.  What is the final prompt you created to generate a set of JSON records with fields containing the reviews?  Enter the contents of the prompt as a triple quote string below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03l1OoX76XF4"
      },
      "outputs": [],
      "source": [
        "### Q2-3-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWekPvt5dXW"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'cot-prompt'></a>\n",
        "\n",
        "### 2.4 Chain of Thought Prompting\n",
        "\n",
        "Can we get the model to go through the steps it takes to solve the problem and appear to display reasoning?\n",
        "\n",
        "What happens if you use a simple prompt and the problem text with the model?  Different models behave differently so you should get to know its idiosyncracies.\n",
        "\n",
        "Answer key:\n",
        "- 1. 9;\n",
        "- 2. 3;\n",
        "- 3. Rashid 9 cats, Maya 8 dogs;\n",
        "- 4. 36;\n",
        "- 5. $379.50\n",
        "- 6. 26.47% increase;\n",
        "- 7. take 40 students on the trip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2_aJzh2MGLw"
      },
      "outputs": [],
      "source": [
        "word_problems = (\n",
        "   \"1. Leo has 5 apples and 3 pears.  Mary has 3 apples and 3 pears.  Marwan has 7 apples and 5 oranges.  If they each give two apples to their teacher what is the total number of apples they have left?\",\n",
        "   \"2. How many R's in strawberry?\",\n",
        "   \"3. Rashid has 5 cats and three dogs.  Maya has four cats and 5 dogs.  If they exchange and Rashid gets the cats while Maya gets the dogs, how many cats and dogs will each one of them have?\",\n",
        "   \"4. Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas, 1 medium pizza, and 1 small pizza. A large pizza has 16 slices, a medium pizza has one quarter of the slices in a large pizza times 3, and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day?\",\n",
        "   \"5. Derek has $1060 to buy his books for the semester. He spends half of that on his textbooks, and he spends a quarter of what is left on his school supplies, and $18 on a nice dinner. What is the amount of money Derek has left?\",\n",
        "   \"6. In Banff National Park, Alberta, a conservation effort has been underway to increase the populations of both grizzly bears and wolverines. In the past year, the grizzly bear population has grown from 120 to 150, while the wolverine population has grown from 50 to 65. What is the percentage increase in the total number of these two species combined?\",\n",
        "   \"7. The students of the École Polytechnique in Paris are planning a school trip to Rome. They have a budget of €15,000 for transportation and accommodation. The transportation company charges €200 per student for a round-trip ticket, and the hotel charges €50 per student per night for a 3-night stay. If there are 50 students going on the trip, and the school wants to spend no more than €8,000 on transportation and no more than €7,000 on accommodation, how many students can they afford to take on the trip if they want to stay within their budget?\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRPxE9jqMGJF"
      },
      "outputs": [],
      "source": [
        "answers = []\n",
        "\n",
        "do_sample_val = True\n",
        "do_top_p_val = 90\n",
        "do_temp_val = 1.25\n",
        "\n",
        "\n",
        "for problem in word_problems:\n",
        "\n",
        "  ### YOUR CODE/PROMPT HERE\n",
        "  myprompt = f\"You are a master teacher. Answer the following Q: {problem} A: Let's take a deep breath and think this through step by step.\"\n",
        "  ### END YOUR CODE\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": myprompt}\n",
        "  ]\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=do_sample_val, top_p=do_top_p_val, temperature=do_temp_val, pad_token_id=tokenizer.eos_token_id)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  cleaned = decoded[0]\n",
        "\n",
        "  cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "  cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "  answers.append(cleaned2.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgqiFMQf88hR"
      },
      "source": [
        "Answer key:\n",
        "- 1. 9;\n",
        "- 2. 3;\n",
        "- 3. Rashid 9 cats, Maya 8 dogs;\n",
        "- 4. 36;\n",
        "- 5. $379.50\n",
        "- 6. 26.47% increase;\n",
        "- 7. take 40 students on the trip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "476XEUjEMGEK"
      },
      "outputs": [],
      "source": [
        "#see your answers\n",
        "answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgsdly0D7gLF"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.4.a.  What are the final answers the LLM gives when you give it the prompt above as input?  Enter the final answers it provides in a semi-colon delimited string in the space below. e.g. \"10 apples; 5 Rs; Rashid 5 cats, 4 dogs, Maya 4 cats, 5 dogs; 10 pieces; $576.42; increase 10%; 20 students\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMko04q17gLF"
      },
      "outputs": [],
      "source": [
        "### Q2-4-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOtYzuyuVMu3"
      },
      "source": [
        "Now change the prompt and hyperparameters to get the model to both indicate the steps it took to reach a solution AND to give the correct answer for at least 5 of the 7 questions **every time you run it**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwZd9PXMMGBp"
      },
      "outputs": [],
      "source": [
        "prompt_answers = []\n",
        "\n",
        "\n",
        "for problem in word_problems:\n",
        "\n",
        "  do_sample_val =\n",
        "  do_top_p_val =\n",
        "  do_temp_val =\n",
        "\n",
        "  ### YOUR CODE/PROMPT HERE\n",
        "\n",
        "  myprompt = f\"\"\n",
        "\n",
        "  ### END YOUR CODE\n",
        "  messages = [\n",
        "        {\"role\": \"user\", \"content\": myprompt}\n",
        "  ]\n",
        "\n",
        "  encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "  model_inputs = encodeds.to(device)\n",
        "  #model.to(device)\n",
        "\n",
        "  generated_ids = model.generate(model_inputs, max_new_tokens=1200, do_sample=do_sample_val, top_p=do_top_p_val, temperature=do_temp_val,\n",
        "                                 pad_token_id=tokenizer.eos_token_id)\n",
        "  decoded = tokenizer.batch_decode(generated_ids)\n",
        "  cleaned = decoded[0]\n",
        "\n",
        "  cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "  cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "  prompt_answers.append(cleaned2.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vrJAIIEC-Q5"
      },
      "source": [
        "Let's examine the answers and see if the model is now answering at least five of the seven problems correctly every time while including steps to solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pglyYsYiMF8e"
      },
      "outputs": [],
      "source": [
        "prompt_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE1Vekx6QJDu"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.4.b. What is the final enhanced prompt you created to generate at least 5 out of 7 correct answers while showing the steps to reach the answer? Please enter it in a triple quote sring below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k-ANdqx74xQ"
      },
      "outputs": [],
      "source": [
        "### Q2-4-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrrtQo5fRocd"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.4.c.  What are the final numeric answers the LLM gives when use the special prompt you created? Enter the final answers it provides in a semi-colon delimited string in the space below. e.g. \"10 apples; 5 Rs; Rashid 5 cats, 4 dogs, Maya 4 cats, 5 dogs; 10 pieces; increase 10%; 33%; 20 students\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSZm59D9Rocf"
      },
      "outputs": [],
      "source": [
        "### Q2-4-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUtIe0etL7PU"
      },
      "source": [
        "[Return to Top](#returnToTop)  \n",
        "<a id = 'prompt-temp'></a>\n",
        "\n",
        "### 2.5 Prompt Templates and Output Improvements\n",
        "\n",
        "Professor Ethan Mollick from Wharton has an excellent substack where he provides very practical advice on dealing with generative AI.  He has an excellent and practical [guide to writing prompts](https://www.oneusefulthing.org/p/a-guide-to-prompting-ai-for-what) that you should read.  He has another excllent piece on [How To Think Like an AI](https://www.oneusefulthing.org/p/thinking-like-an-ai) which you should read.\n",
        "\n",
        "In December of 2023, he sent out [a tweet](https://twitter.com/emollick/status/1734283119295898089) that included a set of these \"incantations\" that people anecdotally insist help with output from their LLM.  His list included the following:\n",
        "  * It is May.\n",
        "  * You are very capable.\n",
        "  * Many people will die if this is not done well.\n",
        "  * You really can do this and are awesome.\n",
        "  * Take a deep breath and think this through.\n",
        "  * My career depends on it.\n",
        "  * Think step by step.\n",
        "\n",
        "You can see if any one of them helps improve your output!\n",
        "\n",
        "LLMs can be helpful with marketing.  Let's test that. Write a prompt that will generate three ideas for a capstone project.  You can augment the prompt with areas of interest. Each idea should be roughly five sentences long and describe how your project is leveraging this new Gen AI technology to differentiate itself from the competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKCAuHLlv1Py"
      },
      "outputs": [],
      "source": [
        "#Prompt Template and Improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuXby9KxouL6"
      },
      "source": [
        "Think about our discussions of prompt templates and how they can be used to help you to construct better prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fYraMJiUFgr"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE/PROMPT HERE\n",
        "\n",
        "### END YOUR CODE\n",
        "prompt_text = f\"{prompt_role} {prompt_task} {prompt_audience} {prompt_output} {prompt_nots} {prompt_question} {prompt_mollick}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXwwrwCvBChu"
      },
      "outputs": [],
      "source": [
        "prompt_answer = \"\"\n",
        "messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt_text}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "cleaned = decoded[0]\n",
        "\n",
        "cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "prompt_answer = cleaned2.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwV35F66jjkK"
      },
      "outputs": [],
      "source": [
        "#These are your three candidate ideas\n",
        "prompt_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y3KHqMMUef2"
      },
      "source": [
        "Select one of your three generated ideas and copy the string into the cell below so that it can be accessed as the variable `capstone_description`. Make sure you run the cell so it is loaded in notebook memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNMLw8azX2fM"
      },
      "outputs": [],
      "source": [
        "\n",
        "capstone_description = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8auIkKWt8IjS"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.5.a. How many of the seven prompt components did you end up filling in for your capstone idea prompt? Enter the number below. You should be using at least 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUGFsukX8IjS"
      },
      "outputs": [],
      "source": [
        "### Q2-5-a Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Tyg7Au8Ite"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.5.b. How many of Ethan Mollick's incantations did you use in your capstone idea prompt? Enter a number below. You should test several and be using at least 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se2iGCnB8Itf"
      },
      "outputs": [],
      "source": [
        "### Q2-5-b Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzyTYxvPiTkv"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.5.c. What is the final prompt you used to generate your three capstone ideas?  Enter the prompt as a triple quote string below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXXi_5vz8I25"
      },
      "outputs": [],
      "source": [
        "### Q2-5-c Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOyLSRPqgdZ4"
      },
      "source": [
        "Now let's generate another key part of presenting your idea.  An elevator pitch is a one or two sentence very compelling description of a your idea that you can use while riding in an elevator with a funder in order to intrigue them to investment in your idea.  You must write a prompt that generates a one (1) sentence compelling elevator pitch for your chosen idea. The pitch should NOT be generic and should be specific to the idea you're pitching to your group.  The pitch should also use at least two of the following buzzwords: 'Deep Fake', 'Intelligent Virtual Agent', 'AI Assistant', 'Blackbox', 'AGI', 'Agentic', 'LLM', 'AI', 'No code', 'Hallucination', 'Explainable', 'Smart', or 'Singularity'.  You must pass the 3 sentence description you generated into the prompt as one basis of the elevator pitch generation that follows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8SDV2WS27r-"
      },
      "outputs": [],
      "source": [
        "prompt_description = f\"Here is the description of the capstone ```{capstone_description}```.\"\n",
        "### YOUR CODE/PROMPT HERE\n",
        "\n",
        "### END YOUR CODE\n",
        "prompt_text = f\"{prompt_description} {prompt_role} {prompt_task} {prompt_audience} {prompt_output} {prompt_nots} {prompt_question} {prompt_mollick}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3QUmq9C27sM"
      },
      "outputs": [],
      "source": [
        "prompt_elevator = \"\"\n",
        "messages = [\n",
        "      {\"role\": \"user\", \"content\": prompt_text}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "\n",
        "model_inputs = encodeds.to(device)\n",
        "#model.to(device)\n",
        "\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "cleaned = decoded[0]\n",
        "\n",
        "cleaned1 = remove_text_between_tags(cleaned, start, fin)\n",
        "cleaned2 = remove_final_tag(cleaned1, fin2)\n",
        "prompt_elevator = cleaned2.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYh3QiR527sN"
      },
      "outputs": [],
      "source": [
        "prompt_elevator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYV9obFP8OZK"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.5.d. How many of the eight prompt components did you end up filling in for your final elevator pitch prompt? You should be using at least 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E7pI8eS8OZL"
      },
      "outputs": [],
      "source": [
        "### Q2-5-d Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-NQe-Bd8Ohd"
      },
      "source": [
        "**QUESTION:**\n",
        "\n",
        "2.5.e. How many of Ethan Mollick's anecdotal incantations did you use in your final elevator pitch prompt? You should be using at least 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cARSAOv8Ohd"
      },
      "outputs": [],
      "source": [
        "### Q2-5-e Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dErE4Uq-cWGq"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION:**\n",
        "\n",
        "2.5.f. What is the final prompt you used to generate your elevator pitch with at least two of the buzzwords?  Enter it as a triple quoted string in the cell below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJRYkUq98OqN"
      },
      "outputs": [],
      "source": [
        "### Q2-5-f Grading Tag: Please put your answer in this cell. Don't edit this line.\n",
        "\n",
        "### YOUR ANSWER HERE\n",
        "\n",
        "### END YOUR ANSWER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Mkm1QYXxUv"
      },
      "source": [
        "###**Congratulations! You have completed the image generation and prompt engineering assignment.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}