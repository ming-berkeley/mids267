{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### PyTorch Intro I: The Basics\n","\n","This notebook supports the Slides \"A Practical Intro to PyTorch I - Basics\".  It is structured as follows:\n","\n","1. Building a Neural Net with PyTorch    \n","  a. Defining and working Tensors     \n","  b. Basic Layer Construction  \n","  c. Basic Network Construction: Sub-classing of nn.Module  \n","\n","2. Dealing with data  \n","  a. Data sets  \n","  b. Data loader\n","\n","3. Neural Net Training    \n","  a. Loss/Cost Functions    \n","  b. Gradients     \n","  c. Optimizers    \n","  d. Training & Eval loops  \n","\n","4. Practical Considerations    \n","  a. Saving a model  \n","  b. Loading a model  \n","  c. CPU and GPU usage   \n","\n","\n","A lot of the content originates in the [PyTorch Tutorial](https:/https://pytorch.org/tutorials/beginner/basics/intro.html/)\n","\n","Let's start with a few installs and imports:\n"],"metadata":{"id":"9hZNSDOBJcBm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuN074G-JK_T"},"outputs":[],"source":["%%capture\n","\n","#!pip install torch\n","#!pip install transformers   # for our application example in the end\n","#!pip install numpy"]},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"9piY_5XnuVa9","executionInfo":{"status":"ok","timestamp":1747106519248,"user_tz":420,"elapsed":4345,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["### 1. Building a Neural Network with PyTorch\n","\n","#### a. Defining and working Tensors\n","\n","A tensor is simply a multi-dimensional generalization of a matrix and it is a central element of neural nets. Let's define a tensor in PyTorch and familiarize ourselves with these objects.\n","\n","You can create a tensor from a list (of lists), from a numpy array, or define it directly through PyTorch:"],"metadata":{"id":"L6KA0CFntCX0"}},{"cell_type":"code","source":["data = [[1, 2, 3],[4, 5, 6]]\n","x_data = torch.tensor(data)\n","\n","x_data"],"metadata":{"id":"Wl9Qf1aoJUw_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106520961,"user_tz":420,"elapsed":37,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"524ce8ee-5775-46e6-c944-f90cdfbdffd0"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["np_array = np.array(data)\n","x_np = torch.tensor(np_array)\n","x_np"],"metadata":{"id":"rlyJi7pRtBo3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106523104,"user_tz":420,"elapsed":21,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"66435582-1564-4920-d139-e8dd6816ffaa"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3],\n","        [4, 5, 6]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x_rand = torch.rand_like(x_data, dtype=torch.float)\n","x_rand"],"metadata":{"id":"mtsfi06ctBrz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106527571,"user_tz":420,"elapsed":51,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"24fcc669-40d5-45a5-c6a8-41a91753dc6c"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4436, 0.6623, 0.6207],\n","        [0.8975, 0.8524, 0.0107]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["How do we get the shape? 'Think numpy':  "],"metadata":{"id":"bFe7Vec9urCK"}},{"cell_type":"code","source":["x_rand.shape"],"metadata":{"id":"gsFR71-LuqZa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106532480,"user_tz":420,"elapsed":34,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ed729048-907d-42eb-a907-6a905e6ad5b8"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["Let's slice and dice tensors to get other tensors. Very straightforward:"],"metadata":{"id":"fQrI6y2vu_Sy"}},{"cell_type":"code","source":["x_rand[:, -1]"],"metadata":{"id":"6-gJDlcgu6Pw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106540528,"user_tz":420,"elapsed":26,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d24f4710-e97f-4eb3-9b20-ef5b6ad54442"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.6207, 0.0107])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["x_rand[1:]"],"metadata":{"id":"VfjEeXpGu6S3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106553999,"user_tz":420,"elapsed":11,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"f490b1a0-597f-4423-95b7-de328315fba4"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8975, 0.8524, 0.0107]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["x_rand[:, 1:]"],"metadata":{"id":"8kXqPyrLu6V-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106556521,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"7a6f21d3-afa0-455a-8571-1a1c3aed9f40"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.6623, 0.6207],\n","        [0.8524, 0.0107]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["All as expected...\n","\n","How do we transpose and then multiply tensors?"],"metadata":{"id":"XcEF1wRLvQQg"}},{"cell_type":"code","source":["x_rand"],"metadata":{"id":"vODzIQoq4D3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106601917,"user_tz":420,"elapsed":43,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"16fd9708-6d92-46bc-b613-529614fb6b12"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4436, 0.6623, 0.6207],\n","        [0.8975, 0.8524, 0.0107]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["x_rand.T"],"metadata":{"id":"GgVtFqNimMO-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106604265,"user_tz":420,"elapsed":13,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"30010e69-3448-47c3-b245-c54287f32c4a"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4436, 0.8975],\n","        [0.6623, 0.8524],\n","        [0.6207, 0.0107]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["prod = x_rand.matmul(x_rand.T)\n","prod"],"metadata":{"id":"A_Iz3B3ktBui","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106605853,"user_tz":420,"elapsed":25,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"5a7cbf7e-b6b0-4034-f1ae-db8b4ab602c6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0207, 0.9694],\n","        [0.9694, 1.5322]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["prod.numpy()"],"metadata":{"id":"6FGKFugG58Z-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106610822,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"6dfd1595-f583-418a-cab3-633e7e9a94cb"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.0206786 , 0.96938396],\n","       [0.96938396, 1.5322416 ]], dtype=float32)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["'.T' does the transposition. I get a 2 x 2 result from multiplying a 2 x 3 matrix with its transpose (from the left).\n","\n","All good. How do we move the other way around, from torch to numpy values?"],"metadata":{"id":"tjRoQUmSv6AS"}},{"cell_type":"code","source":["type(prod.numpy())"],"metadata":{"id":"E3ZZsDckv4WU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106613725,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"87c68373-7920-4a78-cbef-2b29de7b102b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#### b. Working with Layers"],"metadata":{"id":"3dr2JJBl2zhi"}},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"_GMGGAqW2yq5","executionInfo":{"status":"ok","timestamp":1747106615253,"user_tz":420,"elapsed":2,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Let's start by defining a simple linear layer, corresponding to a 100 x 200 matrix and corresponding weights:"],"metadata":{"id":"BQGimpje3Ezi"}},{"cell_type":"code","source":["linear_layer = nn.Linear(100, 200)\n","linear_layer"],"metadata":{"id":"eS3c3-GN2ytT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106619326,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0a7037f7-d023-4734-d638-de0be7bbdf16"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=100, out_features=200, bias=True)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["linear_layer.weight"],"metadata":{"id":"490OyOv52ywB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106642679,"user_tz":420,"elapsed":22,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"3e25752e-5771-4938-d008-d3a1b0c725c6"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0696,  0.0107,  0.0198,  ..., -0.0204,  0.0280,  0.0452],\n","        [-0.0782, -0.0481,  0.0937,  ...,  0.0618,  0.0122, -0.0270],\n","        [ 0.0050, -0.0202, -0.0063,  ...,  0.0529,  0.0065,  0.0514],\n","        ...,\n","        [-0.0637, -0.0537,  0.0565,  ...,  0.0667,  0.0159, -0.0655],\n","        [-0.0532,  0.0481, -0.0932,  ...,  0.0819,  0.0780, -0.0659],\n","        [-0.0963, -0.0398,  0.0096,  ...,  0.0613, -0.0471,  0.0310]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["linear_layer.weight.shape"],"metadata":{"id":"9MdfdtW32yyf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106645822,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"2e734d49-a12d-4ebe-eec0-250ea2b5d17e"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([200, 100])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["linear_layer.weight"],"metadata":{"id":"o133uuuJ2y1o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106647978,"user_tz":420,"elapsed":21,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"e0ed8979-54e9-40ae-cccd-17a77452a332"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0696,  0.0107,  0.0198,  ..., -0.0204,  0.0280,  0.0452],\n","        [-0.0782, -0.0481,  0.0937,  ...,  0.0618,  0.0122, -0.0270],\n","        [ 0.0050, -0.0202, -0.0063,  ...,  0.0529,  0.0065,  0.0514],\n","        ...,\n","        [-0.0637, -0.0537,  0.0565,  ...,  0.0667,  0.0159, -0.0655],\n","        [-0.0532,  0.0481, -0.0932,  ...,  0.0819,  0.0780, -0.0659],\n","        [-0.0963, -0.0398,  0.0096,  ...,  0.0613, -0.0471,  0.0310]],\n","       requires_grad=True)"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["Great... let's try to convert these into a numpy array:"],"metadata":{"id":"iSGO5wC638lF"}},{"cell_type":"code","source":["linear_layer.weight.numpy()"],"metadata":{"id":"CoC3eYzF37oK","colab":{"base_uri":"https://localhost:8080/","height":144},"executionInfo":{"status":"error","timestamp":1747106656040,"user_tz":420,"elapsed":64,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d41fe173-5b42-4aec-84b7-965c26c2bed3"},"execution_count":19,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-965f1b7cb7c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinear_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."]}]},{"cell_type":"markdown","source":["Ouch, why the error? You need to 'detach' the object first. A layer is more than just the action of weights and biases."],"metadata":{"id":"1HXogtR-4GhS"}},{"cell_type":"code","source":["linear_layer.weight.detach().numpy()"],"metadata":{"id":"trTzDAY_37xZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106659535,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"216a6663-a321-4275-9f6f-a1d9697b4fdf"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.06957863,  0.01073621,  0.01979731, ..., -0.02038162,\n","         0.02800957,  0.04524515],\n","       [-0.0782189 , -0.0481025 ,  0.0937292 , ...,  0.06180744,\n","         0.01222303, -0.02695934],\n","       [ 0.00502089, -0.02019792, -0.00626776, ...,  0.0529028 ,\n","         0.00648832,  0.05139188],\n","       ...,\n","       [-0.06374276, -0.05367049,  0.05653309, ...,  0.06667212,\n","         0.01586897, -0.06550749],\n","       [-0.05324009,  0.04813758, -0.09315715, ...,  0.0819044 ,\n","         0.07799258, -0.06592729],\n","       [-0.09626263, -0.03983566,  0.00958504, ...,  0.06132403,\n","        -0.04707544,  0.03098365]], dtype=float32)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["What about activation functions? Simple! They are also in torch.nn:"],"metadata":{"id":"QJZUqJho4pUw"}},{"cell_type":"code","source":["nn.ReLU()"],"metadata":{"id":"JJguE6IKweet","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106664207,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"30d291e9-8194-42ff-b4cb-a7c7149055b0"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ReLU()"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Great. So the nn package has a lot of what we need. Now we are ready to build our first simple neural net using nn.module!\n","\n","#### c. Building a neural net\n","\n","We need to subclass nn.Module, the base class for all neural network modules in PyTorch.\n","\n","We need to first implement the \\_\\_init\\_\\_ and forward methods:\n","\n","\n","\n","*   \\_\\_init\\_\\_:  ~ define the layers in the network\n","*   forward: ~ define how the layers act on the input to generate the output(s)\n","\n","Here is the first simple example:\n","\n"],"metadata":{"collapsed":false,"id":"KPl3wQKZAc37"}},{"cell_type":"code","execution_count":22,"outputs":[],"source":["class MyBasicNetworkClass(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):                             # x represents the input that the network will use/act on later\n","        out_linear = self.linear(x)\n","        output = self.relu(out_linear)\n","        return output"],"metadata":{"id":"QgsstAZNAc38","executionInfo":{"status":"ok","timestamp":1747106667340,"user_tz":420,"elapsed":20,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}}},{"cell_type":"markdown","source":["Let's test it.\n","\n","We first define a test input (What shape does it need to have?), and then act with the network on it.\n","\n","Here is the test input:"],"metadata":{"id":"mMNw0po-6_EW"}},{"cell_type":"code","source":["input_dim = 3\n","output_dim = 5\n","\n","test_input = torch.tensor(np.random.random(input_dim), dtype=torch.float) # the types will need to match with the types in the network\n","test_input"],"metadata":{"id":"cnTUMCsf69vW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106672403,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"59a41070-3cab-46c6-b034-00a2a36165a4"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.8602, 0.9328, 0.9199])"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["And now let's try to act with the network 'on it', i.e., use this test _inout as the input to our network. For this, we first need to create an instance of the network and then apply it to the input:"],"metadata":{"id":"FKv_19rO7f9F"}},{"cell_type":"code","source":["my_basic_network = MyBasicNetworkClass(input_dim=input_dim, output_dim=output_dim)\n","my_basic_network"],"metadata":{"id":"jLa3j2PD7GHI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106683645,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"506675ac-23ce-41f4-d00f-d97824a7ce12"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MyBasicNetworkClass(\n","  (linear): Linear(in_features=3, out_features=5, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["Ok, that makes sense.\n","\n","Let's look at the action of the network on this test input:"],"metadata":{"id":"8HQ3kS2m8MBz"}},{"cell_type":"code","source":["test_output = my_basic_network(test_input)\n","test_output"],"metadata":{"id":"sY4j46WO7QG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106687327,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"498c5bd5-ed67-4788-a17f-537fb0a7e4ae"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.3740, 0.0000, 0.0000, 0.0000, 0.6927], grad_fn=<ReluBackward0>)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["Perfect! That looks right. (Why all the zeros?)\n","\n","Our first basic neural net using sub-classing! This is the base to work off from!\n","\n","What happens if we have 10 examples in a batch?"],"metadata":{"id":"nX-ttHD-97is"}},{"cell_type":"code","source":["batched_test_input = torch.tensor(np.random.random(30).reshape(10, 3), dtype=torch.float)\n","batched_test_input.shape"],"metadata":{"id":"xdP5mVdN8RMP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106693272,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"cb0dea4c-5455-4668-c47d-0cd60afcc0df"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 3])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["batched_test_output = my_basic_network(batched_test_input)\n","batched_test_output.shape"],"metadata":{"id":"cS37bhYQ-g3m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106695263,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ece2f947-3e0b-467d-df67-8c331c40fc52"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 5])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["batched_test_output.shape"],"metadata":{"id":"ww1Dz6W3oQUi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106698672,"user_tz":420,"elapsed":12,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ef76eb9c-57b0-4132-c619-d9c876c45210"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 5])"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["That looks right again! Implicitly __broadcasting__ was used, where the network, which technically expected a dim=3 input, received a dim=10x3, and it applied the network actions independently to each of the 10 examples in the batch.\n","\n","What if we do not want to hard-code the dimensions into the network class? Again, the approach is very pythonic: we define the parameters in the init step.\n","\n","Let's try that and also extend the network a bit to act as a simple classification network. We could do that be adding another layer with __one__ output neuron and use a sigmoid activation function:   \n"],"metadata":{"id":"9mRkcnCX-19T"}},{"cell_type":"code","source":["class MySimpleClassificationNetworkClass(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.linear_2 = nn.Linear(hidden_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","\n","    def forward(self, x):                             # x stands for the input that the network will use/act on later\n","        hidden = self.relu(self.linear_1(x))\n","        output = self.sigmoid(self.linear_2(hidden))\n","        return output"],"metadata":{"id":"AROxXlSW-08z","executionInfo":{"status":"ok","timestamp":1747106742960,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["Let's test this out:"],"metadata":{"id":"nC7Hr6n9Bsiq"}},{"cell_type":"code","source":["my_simple_classification_network = MySimpleClassificationNetworkClass(input_dim=2, hidden_dim=4)\n","my_simple_classification_network"],"metadata":{"id":"7y_UieL6-oyL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106746271,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"7a8f7f46-3ce6-4df2-97c4-5ca1697227d2"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MySimpleClassificationNetworkClass(\n","  (linear_1): Linear(in_features=2, out_features=4, bias=True)\n","  (relu): ReLU()\n","  (linear_2): Linear(in_features=4, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["my_simple_classification_network.linear_1"],"metadata":{"id":"fdYXTs9RGV-F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106750377,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"8cad5ba3-57f4-4a46-d609-8b31f6175de2"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=2, out_features=4, bias=True)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["my_simple_classification_network.linear_1.bias"],"metadata":{"id":"3VMKT_G9GeUv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106751627,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"977c46a8-f61b-4e89-f712-d246719b91b6"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([-0.1663,  0.3813, -0.4476,  0.1922], requires_grad=True)"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["Let's look at the model in more comprehensively:"],"metadata":{"id":"bKNeNBTzvjCF"}},{"cell_type":"code","source":["print(f\"Model structure: {my_simple_classification_network}\\n\\n\")\n","\n","for name, param in my_simple_classification_network.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"],"metadata":{"id":"AurhrHWivJRC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106754113,"user_tz":420,"elapsed":14,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"84557947-75db-49a9-bc91-9fc20b691d51"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: MySimpleClassificationNetworkClass(\n","  (linear_1): Linear(in_features=2, out_features=4, bias=True)\n","  (relu): ReLU()\n","  (linear_2): Linear(in_features=4, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n","\n","\n","Layer: linear_1.weight | Size: torch.Size([4, 2]) | Values : tensor([[-0.6177, -0.5957],\n","        [ 0.2173,  0.2986]], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_1.bias | Size: torch.Size([4]) | Values : tensor([-0.1663,  0.3813], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_2.weight | Size: torch.Size([1, 4]) | Values : tensor([[-0.3204, -0.2926, -0.4486,  0.0074]], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_2.bias | Size: torch.Size([1]) | Values : tensor([-0.2615], grad_fn=<SliceBackward0>) \n","\n"]}]},{"cell_type":"markdown","source":["\n","Looks right!\n","\n","Now what about the actions on a fake batch of dim=2 inputs?\n"],"metadata":{"id":"yd_N68ZV_t3j"}},{"cell_type":"code","source":["batched_test_classification_input = torch.tensor(np.random.random(20).reshape(10, 2), dtype=torch.float)\n","batched_test_classification_input"],"metadata":{"id":"k-k38yipCJlM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106760558,"user_tz":420,"elapsed":18,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"e9d8baa5-62e8-4185-c170-59a8d4504644"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3427, 0.1684],\n","        [0.4139, 0.6248],\n","        [0.7938, 0.1998],\n","        [0.8596, 0.7983],\n","        [0.0128, 0.0577],\n","        [0.0165, 0.6082],\n","        [0.8933, 0.5487],\n","        [0.3353, 0.3896],\n","        [0.7779, 0.7650],\n","        [0.6982, 0.8574]])"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["batched_test_classification_output = my_simple_classification_network(batched_test_classification_input)\n","batched_test_classification_output"],"metadata":{"id":"jKrIz1kzCQ7V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106774507,"user_tz":420,"elapsed":52,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0abc673b-31a0-4e27-b5f8-6516ef673b11"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3990],\n","        [0.3885],\n","        [0.3915],\n","        [0.3781],\n","        [0.4067],\n","        [0.3953],\n","        [0.3828],\n","        [0.3946],\n","        [0.3800],\n","        [0.3793]], grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["Looks correct again! Since the input was random and the initialization of the network layers was random we expect essentially a set of 50/50s.\n","\n","Before we continue with training of the network, let's talk about datasets and data loaders\n","\n","### 2. Dealing with data\n","\n","#### a. Datasets\n","\n","First off, PyTorch comes with a lot of datasets across many domains (text, vision, etc.). Here is a vision example (see: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#):\n","\n","\n"],"metadata":{"id":"SONbaUbJChU7"}},{"cell_type":"code","source":["%%capture\n","#import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"sLFe_6B_CZGd","executionInfo":{"status":"ok","timestamp":1747106791933,"user_tz":420,"elapsed":4604,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")"],"metadata":{"id":"K0y0aOaVk_xF","executionInfo":{"status":"ok","timestamp":1747106799637,"user_tz":420,"elapsed":7702,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["training_data"],"metadata":{"id":"Hd4iPT_vlIwk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106799662,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ad449199-c671-4be1-c47d-708b3927791b"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset FashionMNIST\n","    Number of datapoints: 60000\n","    Root location: data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["How does the data 'look' like?"],"metadata":{"id":"aPBbAS5olxoq"}},{"cell_type":"code","source":["training_data[1]"],"metadata":{"id":"06QV1A9XlmBU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106799709,"user_tz":420,"elapsed":46,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"99666310-e763-4e73-9eda-a72e39fffb53"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.1608, 0.7373, 0.4039, 0.2118, 0.1882, 0.1686,\n","           0.3412, 0.6588, 0.5216, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.1922,\n","           0.5333, 0.8588, 0.8471, 0.8941, 0.9255, 1.0000, 1.0000, 1.0000,\n","           1.0000, 0.8510, 0.8431, 0.9961, 0.9059, 0.6275, 0.1765, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.6902, 0.8706,\n","           0.8784, 0.8314, 0.7961, 0.7765, 0.7686, 0.7843, 0.8431, 0.8000,\n","           0.7922, 0.7882, 0.7882, 0.7882, 0.8196, 0.8549, 0.8784, 0.6431,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7373, 0.8588, 0.7843,\n","           0.7765, 0.7922, 0.7765, 0.7804, 0.7804, 0.7882, 0.7686, 0.7765,\n","           0.7765, 0.7843, 0.7843, 0.7843, 0.7843, 0.7882, 0.7843, 0.8824,\n","           0.1608, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.8588, 0.7804, 0.7961,\n","           0.7961, 0.8314, 0.9333, 0.9725, 0.9804, 0.9608, 0.9765, 0.9647,\n","           0.9686, 0.9882, 0.9725, 0.9216, 0.8118, 0.7961, 0.7961, 0.8706,\n","           0.5490, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.4549, 0.8863, 0.8078, 0.8000,\n","           0.8118, 0.8000, 0.3961, 0.2941, 0.1843, 0.2863, 0.1882, 0.1961,\n","           0.1765, 0.2000, 0.2471, 0.4431, 0.8706, 0.7922, 0.8078, 0.8627,\n","           0.8784, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.8706, 0.8196, 0.7961,\n","           0.8431, 0.7843, 0.0000, 0.2745, 0.3843, 0.0000, 0.4039, 0.2314,\n","           0.2667, 0.2784, 0.1922, 0.0000, 0.8588, 0.8078, 0.8392, 0.8235,\n","           0.9804, 0.1490, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.9686, 0.8549, 0.8314, 0.8235,\n","           0.8431, 0.8392, 0.0000, 0.9961, 0.9529, 0.5451, 1.0000, 0.6824,\n","           0.9843, 1.0000, 0.8039, 0.0000, 0.8431, 0.8510, 0.8392, 0.8157,\n","           0.8627, 0.3725, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.1765, 0.8863, 0.8392, 0.8392, 0.8431,\n","           0.8784, 0.8039, 0.0000, 0.1647, 0.1373, 0.2353, 0.0627, 0.0667,\n","           0.0471, 0.0510, 0.2745, 0.0000, 0.7412, 0.8471, 0.8314, 0.8078,\n","           0.8314, 0.6118, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.6431, 0.9216, 0.8392, 0.8275, 0.8627,\n","           0.8471, 0.7882, 0.2039, 0.2784, 0.3490, 0.3686, 0.3255, 0.3059,\n","           0.2745, 0.2980, 0.3608, 0.3412, 0.8078, 0.8118, 0.8706, 0.8353,\n","           0.8588, 0.8157, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.4157, 0.7333, 0.8745, 0.9294, 0.9725,\n","           0.8275, 0.7765, 0.9882, 0.9804, 0.9725, 0.9608, 0.9725, 0.9882,\n","           0.9922, 0.9804, 0.9882, 0.9373, 0.7882, 0.8314, 0.8824, 0.8431,\n","           0.7569, 0.4431, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.2118, 0.6235,\n","           0.8706, 0.7569, 0.8157, 0.7529, 0.7725, 0.7843, 0.7843, 0.7843,\n","           0.7843, 0.7882, 0.7961, 0.7647, 0.8235, 0.6471, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n","           0.8824, 0.7529, 0.8392, 0.7961, 0.8078, 0.8000, 0.8000, 0.8039,\n","           0.8078, 0.8000, 0.8314, 0.7725, 0.8549, 0.4196, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0235, 0.0000, 0.1804,\n","           0.8314, 0.7647, 0.8314, 0.7922, 0.8078, 0.8039, 0.8000, 0.8039,\n","           0.8078, 0.8000, 0.8314, 0.7843, 0.8549, 0.3569, 0.0000, 0.0118,\n","           0.0039, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0431,\n","           0.7725, 0.7804, 0.8039, 0.7922, 0.8039, 0.8078, 0.8000, 0.8039,\n","           0.8118, 0.8000, 0.8039, 0.8039, 0.8549, 0.3020, 0.0000, 0.0196,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.0078,\n","           0.7490, 0.7765, 0.7882, 0.8039, 0.8078, 0.8039, 0.8039, 0.8078,\n","           0.8196, 0.8078, 0.7804, 0.8196, 0.8588, 0.2902, 0.0000, 0.0196,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n","           0.7373, 0.7725, 0.7843, 0.8118, 0.8118, 0.8000, 0.8118, 0.8118,\n","           0.8235, 0.8157, 0.7765, 0.8118, 0.8667, 0.2824, 0.0000, 0.0157,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000,\n","           0.8431, 0.7765, 0.7961, 0.8078, 0.8157, 0.8039, 0.8118, 0.8118,\n","           0.8235, 0.8157, 0.7843, 0.7922, 0.8706, 0.2941, 0.0000, 0.0157,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.8314, 0.7765, 0.8196, 0.8078, 0.8196, 0.8078, 0.8157, 0.8118,\n","           0.8275, 0.8078, 0.8039, 0.7765, 0.8667, 0.3137, 0.0000, 0.0118,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.8000, 0.7882, 0.8039, 0.8157, 0.8118, 0.8039, 0.8275, 0.8039,\n","           0.8235, 0.8235, 0.8196, 0.7647, 0.8667, 0.3765, 0.0000, 0.0118,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.7922, 0.7882, 0.8039, 0.8196, 0.8118, 0.8039, 0.8353, 0.8078,\n","           0.8235, 0.8196, 0.8235, 0.7608, 0.8510, 0.4118, 0.0000, 0.0078,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.8000, 0.8000, 0.8039, 0.8157, 0.8118, 0.8039, 0.8431, 0.8118,\n","           0.8235, 0.8157, 0.8275, 0.7569, 0.8353, 0.4510, 0.0000, 0.0078,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","           0.8000, 0.8118, 0.8118, 0.8157, 0.8078, 0.8078, 0.8431, 0.8235,\n","           0.8235, 0.8118, 0.8314, 0.7647, 0.8235, 0.4627, 0.0000, 0.0078,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.7765, 0.8157, 0.8157, 0.8157, 0.8000, 0.8118, 0.8314, 0.8314,\n","           0.8235, 0.8118, 0.8275, 0.7686, 0.8118, 0.4745, 0.0000, 0.0039,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.7765, 0.8235, 0.8118, 0.8157, 0.8078, 0.8196, 0.8353, 0.8314,\n","           0.8275, 0.8118, 0.8235, 0.7725, 0.8118, 0.4863, 0.0000, 0.0039,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","           0.6745, 0.8235, 0.7961, 0.7882, 0.7804, 0.8000, 0.8118, 0.8039,\n","           0.8000, 0.7882, 0.8039, 0.7725, 0.8078, 0.4980, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n","           0.7373, 0.8667, 0.8392, 0.9176, 0.9255, 0.9333, 0.9569, 0.9569,\n","           0.9569, 0.9412, 0.9529, 0.8392, 0.8784, 0.6353, 0.0000, 0.0078,\n","           0.0000, 0.0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n","           0.5451, 0.5725, 0.5098, 0.5294, 0.5294, 0.5373, 0.4902, 0.4863,\n","           0.4902, 0.4745, 0.4667, 0.4471, 0.5098, 0.2980, 0.0000, 0.0000,\n","           0.0000, 0.0000, 0.0000, 0.0000]]]),\n"," 0)"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["Ok. Looks like the digitized image and a label. Here is a better way to look at the data in this case:"],"metadata":{"id":"YwBwC66OlrPb"}},{"cell_type":"code","source":["labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    img, label = training_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()\n"],"metadata":{"id":"0ZFlCEyHlNrn","colab":{"base_uri":"https://localhost:8080/","height":675},"executionInfo":{"status":"ok","timestamp":1747106804447,"user_tz":420,"elapsed":472,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"9b612fcd-a888-43f7-8753-53af35cd7ad6"},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x800 with 9 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["More importantly though Dataset and Dataloader should be used to manage your training/validation/test data. It abstracts the data management and ensures that the data works well with your PyTorch networks.\n","\n","_Dataset_: stores the data and labels (as you saw above)\n","_Dataoader_: creates an iterable of the data for easy and consistent access to the data samples.\n","\n","#### a) Dataset\n","\n","You should create a Dataset from your data for much easier use later on. To do that, you need to subclass _Dataset_ (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).\n","\n","For concreteness, let's generate a fake dataset with 1000 random 5-d vectors and a random label which can be zero or 1:    "],"metadata":{"id":"uFXja3Z2mA-t"}},{"cell_type":"code","source":["X_fake = np.random.random(500).reshape(100, 5)\n","y_fake = [np.random.randint(2) for _ in range(100)]"],"metadata":{"id":"l_zF09ZulYa9","executionInfo":{"status":"ok","timestamp":1747106812125,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["data_fake = [(x, y) for x, y in zip(list(X_fake), list(y_fake))]\n","data_fake[:5]\n","\n"],"metadata":{"id":"NKsE3RC3p2Xi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106822534,"user_tz":420,"elapsed":11,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"5adbd25e-0f5c-4ea6-9300-265f334dc239"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(array([0.52488372, 0.84951109, 0.63859506, 0.88679697, 0.07023013]), 0),\n"," (array([0.62124263, 0.87820287, 0.94875157, 0.16829122, 0.88998649]), 1),\n"," (array([0.59754729, 0.50293544, 0.58112469, 0.59558779, 0.82218274]), 0),\n"," (array([0.5793454 , 0.71574223, 0.79182865, 0.79401015, 0.77257269]), 0),\n"," (array([0.29291162, 0.37138515, 0.53685856, 0.36600734, 0.70865917]), 1)]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["Now let's set up the dataset. We need to implement  \\_\\_init\\_\\_, \\_\\_len\\_\\_, and \\_\\_getitem\\_\\_:"],"metadata":{"id":"kh7D1nmKq2_W"}},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x = torch.tensor(self.data[idx][0])    #not a bad idea to create torch tensors from the data\n","        y = self.data[idx][1]\n","        return x, y"],"metadata":{"id":"euKIH14Ip4m2","executionInfo":{"status":"ok","timestamp":1747106826194,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["Really simple. Let's look at this for our fake data:"],"metadata":{"id":"q5udAwk8sGxz"}},{"cell_type":"code","source":["myFakeData = MyDataset(data_fake)"],"metadata":{"id":"P68FtBYysDOQ","executionInfo":{"status":"ok","timestamp":1747106829289,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["myFakeData"],"metadata":{"id":"v9TCFQ0aApId","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106830228,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"8475122d-7851-4f88-a04e-4dc50ad7c196"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.MyDataset at 0x7ea57cb49890>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["len(myFakeData)"],"metadata":{"id":"zugwXMbQHs4j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106832191,"user_tz":420,"elapsed":52,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"9a464545-f9d7-4563-d9e6-649548fa0c3d"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["myFakeData[16]"],"metadata":{"id":"s_xboqaesS95","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106834523,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"dc371aca-6a17-45b5-827f-85072cf6ce02"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.9618, 0.1832, 0.6593, 0.4437, 0.2307], dtype=torch.float64), 1)"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","source":["Looks good. Now we use the Dataloader to manage the samples.\n","\n","#### b) Dataloader"],"metadata":{"id":"GrnTqi_MtFOZ"}},{"cell_type":"code","source":["data_fake_dataloader = DataLoader(myFakeData, batch_size=16, shuffle=True)\n"],"metadata":{"id":"LSeA5zgtsUCO","executionInfo":{"status":"ok","timestamp":1747106861397,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":["So what can we do with this? For example, iterate through batches:"],"metadata":{"id":"u1BvMXH7trZx"}},{"cell_type":"code","source":["iter_fake_data_loader = iter(data_fake_dataloader)\n","iter_fake_data_loader"],"metadata":{"id":"dGjOAmN0tnLe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106864928,"user_tz":420,"elapsed":11,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"b26465db-f69e-43e7-ea96-40b989b017ed"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7ea5860f3590>"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["fake_features, fake_labels = next(iter_fake_data_loader)\n","fake_features"],"metadata":{"id":"BpANf-KDukHY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106866141,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"c60d8e17-f0b8-4707-8249-57802b2f36bb"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4961, 0.4312, 0.1472, 0.8265, 0.8526],\n","        [0.8471, 0.1239, 0.7129, 0.4285, 0.0925],\n","        [0.6886, 0.2092, 0.4898, 0.0864, 0.3000],\n","        [0.5687, 0.3181, 0.8602, 0.6857, 0.8826],\n","        [0.7198, 0.3124, 0.0323, 0.6118, 0.7719],\n","        [0.1790, 0.0632, 0.9235, 0.4248, 0.9285],\n","        [0.9598, 0.0321, 0.2272, 0.4758, 0.3392],\n","        [0.7226, 0.5040, 0.7619, 0.6818, 0.8677],\n","        [0.9939, 0.5409, 0.2406, 0.4121, 0.3956],\n","        [0.0234, 0.2374, 0.5223, 0.2856, 0.8053],\n","        [0.4912, 0.1136, 0.3533, 0.8890, 0.1934],\n","        [0.3961, 0.0329, 0.9735, 0.9893, 0.4447],\n","        [0.1506, 0.6210, 0.7532, 0.4933, 0.2410],\n","        [0.5118, 0.2463, 0.3557, 0.2457, 0.5762],\n","        [0.2571, 0.6212, 0.1585, 0.2748, 0.9878],\n","        [0.5868, 0.4909, 0.9655, 0.6435, 0.8030]], dtype=torch.float64)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["next(iter_fake_data_loader)"],"metadata":{"id":"sJF5EfM_tol1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106869376,"user_tz":420,"elapsed":15,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"4f446d1c-d093-4eb8-c32e-d21ab4ac3730"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([[0.2915, 0.7283, 0.6577, 0.7370, 0.9224],\n","         [0.8383, 0.1330, 0.1960, 0.5447, 0.8223],\n","         [0.3037, 0.2248, 0.1180, 0.6367, 0.7678],\n","         [0.7290, 0.3545, 0.6402, 0.4510, 0.0444],\n","         [0.3525, 0.4142, 0.9150, 0.0407, 0.7123],\n","         [0.8192, 0.0849, 0.4965, 0.4276, 0.4939],\n","         [0.7297, 0.3748, 0.8288, 0.5806, 0.9542],\n","         [0.1712, 0.2535, 0.5353, 0.0729, 0.7122],\n","         [0.0253, 0.1101, 0.7461, 0.9029, 0.4163],\n","         [0.7313, 0.3338, 0.4670, 0.0328, 0.2064],\n","         [0.4990, 0.5612, 0.9129, 0.2663, 0.6398],\n","         [0.5793, 0.7157, 0.7918, 0.7940, 0.7726],\n","         [0.6212, 0.8782, 0.9488, 0.1683, 0.8900],\n","         [0.8457, 0.6535, 0.8854, 0.5889, 0.1183],\n","         [0.6100, 0.1721, 0.0487, 0.1639, 0.2830],\n","         [0.2444, 0.4944, 0.6813, 0.2242, 0.9558]], dtype=torch.float64),\n"," tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1])]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["Great. Now we can 'train' a model for our fake data!\n","\n","### 3) Train the Network\n","\n","But before we are ready to write a training loop we need to start with the loss and then look at the gradients needed for backprop.\n","\n","#### a) Loss Function\n","\n","These are also in the nn module (https://pytorch.org/docs/stable/nn.html#loss-functions). For example, here is the binary cross entropy loss:"],"metadata":{"id":"0GaRQLy8ur0v"}},{"cell_type":"code","source":["loss_fn = nn.BCELoss()\n","loss_fn"],"metadata":{"id":"8gXT2NZPuBdU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106873401,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d83ef8f4-ac36-454b-9852-b0743130e480"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BCELoss()"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["loss_fn(torch.tensor([0.8, 0.4], dtype=torch.float64), torch.tensor([1, 0], dtype=torch.float64))"],"metadata":{"id":"3j7ETXjduCzJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106874445,"user_tz":420,"elapsed":15,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"1890fee9-8b8e-4a0d-80b1-85ef66d2c5a5"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.3670, dtype=torch.float64)"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["Is this correct?"],"metadata":{"id":"gt5TQun1x_QQ"}},{"cell_type":"code","source":["1/2 * (-np.log(0.8) -np.log(0.6) )"],"metadata":{"id":"pVtqB5ymuHfM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106877144,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"b16b6139-bbd9-4e58-ca84-ff53dac84151"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(0.3669845875401002)"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["Cool! Correct!\n","\n","Now on to the gradients.\n","\n","#### b) Gradients\n","\n","Gradients are computed through torch.autograd (see: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#automatic-differentiation-with-torch-autograd)"],"metadata":{"id":"RAmTQpqSySZL"}},{"cell_type":"code","source":["\n","x = torch.tensor(np.array([[2.,]]), requires_grad=False, dtype=torch.float) # one sample, one feature with value 1\n","y = torch.tensor(np.array([4.,]), requires_grad=False, dtype=torch.float)  # expected output, one sample with one output neuron with value 2\n","\n","w = torch.tensor(np.array([2.,]), requires_grad=True, dtype=torch.float)\n","b = torch.tensor(np.array([1,]), requires_grad=True, dtype=torch.float)\n","\n","z = torch.matmul(x, w) + b\n","\n","loss = nn.MSELoss()(z,y)\n","\n","\n","# y_calc = x * w + b"],"metadata":{"id":"TqD8BeoUyQWT","executionInfo":{"status":"ok","timestamp":1747106880494,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["print('Output: ', z)"],"metadata":{"id":"L9Hz4aygqqKI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106884693,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"3d95aa78-dfac-4cef-a80d-d74c1ed6afb1"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:  tensor([5.], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["So this is the PyTorch way of calculating $2x + 1$ with $x=2$, and calculating the error for target value $y=4$.\n","\n","What should the loss be? $(2*2 + 1 - 4)^2 = 1$"],"metadata":{"id":"YiDBqzWjJkzU"}},{"cell_type":"code","source":["loss"],"metadata":{"id":"YuMuRSpKDSJQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106886741,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"e811861f-b4d5-4518-e562-7082effd320b"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1., grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["Correct. So what about the gradients?"],"metadata":{"id":"FF-40_1MKwBi"}},{"cell_type":"code","source":["loss.backward()   # compute derivatives\n","print(w.grad)    # dloss/dw\n","print(b.grad)   # dloss/db"],"metadata":{"id":"2RtWl00xDS48","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106890739,"user_tz":420,"elapsed":51,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"635dd20a-5236-44ec-ab0e-3c661a95c320"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([4.])\n","tensor([2.])\n"]}]},{"cell_type":"markdown","source":["Correct?\n","\n","Loss:\n","\n","$\\rm{loss} = (wx + b - y)^2 = (2 * w + b - 4)^2$\n","\n","Derivatives with respect to __model parameters__ at __$x = 1$__:\n","\n","$\\frac{d\\rm{loss}}{dw} = 2 (2 * w + b - 4) * 2 = 4$\n","\n","$\\frac{d\\rm{loss}}{db} = 2 (2 * w + b - 4) = 2$\n","\n"],"metadata":{"id":"u-ZbRV__LrdJ"}},{"cell_type":"markdown","source":["So w.grad/b.grad are the indeed the gradients.\n","\n","We are now ready to move on to the optimizers.\n","\n","#### c) Optimizers\n","\n","The optimizers can be found in torch.optim. For example, here is the Adam optimizer, that will be in charge of optimizing the parameters of our earlier model. (Adam and AdamW are usually effective optimizers, generally outperforming SGD significantly.)\n"],"metadata":{"id":"puFtb8jqOKAC"}},{"cell_type":"code","source":["adam_opt = torch.optim.Adam(params=my_simple_classification_network.parameters(), lr=0.001)\n","adam_opt"],"metadata":{"id":"DMld9QAnGdMV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106910250,"user_tz":420,"elapsed":46,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"c9878cda-9ad2-4117-cca1-270c314c8176"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    lr: 0.001\n","    maximize: False\n","    weight_decay: 0\n",")"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["Now we are ready to put it all together.\n","\n","#### d) Training the Network\n","\n","In addition to what we did above, we need to define a training loop (and optionally also an eval loop.). But let's put it all back together, now also adding a bit of a signal to our training/test data.\n","\n","Data:"],"metadata":{"id":"HJHCbYz3XAgK"}},{"cell_type":"code","source":["n_train = 600\n","n_test = 100\n","n_features = 5\n","eps = 0.3\n","\n","y_train = torch.tensor([1.0] * int(n_train/2) + [0.] * int(n_train/2), dtype=torch.float32).unsqueeze(1)\n","X_train = (torch.tensor(np.random.random(n_train * n_features).reshape([n_train, n_features]), dtype=torch.float32) +\n","           eps * y_train.matmul(torch.tensor([[1.] * n_features], dtype=torch.float32)) )                    # add a bit of a signal so that we can train a bit\n","\n","y_test = torch.tensor([1.0] * int(n_test/2) + [0.] * int(n_test/2), dtype=torch.float32).unsqueeze(1)\n","X_test = (torch.tensor(np.random.random(n_test * n_features).reshape([n_test, n_features]), dtype=torch.float32) +\n","           eps * y_test.matmul(torch.tensor([[1.] * n_features], dtype=torch.float32)) )                    # ditto for test\n"],"metadata":{"id":"DmN61cyAY4M6","executionInfo":{"status":"ok","timestamp":1747106913985,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["y_test[:3]"],"metadata":{"id":"LHgIYFV1DXuz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106917141,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"98821f82-e1ce-4eda-dd1b-e2ef3ddb349e"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.],\n","        [1.],\n","        [1.]])"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["Datasets:"],"metadata":{"id":"tkae_uURdqK-"}},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, X, y):\n","        self.data = [(a, b) for a,b in zip(list(X), list(y))]    # first dimension in X and y is the barch dimension\n","\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x = self.data[idx][0]\n","        y = self.data[idx][1]\n","        return x, y\n","\n","training_data = MyDataset(X_train, y_train)\n","test_data = MyDataset(X_test, y_test)"],"metadata":{"id":"shRo_gciG_Fm","executionInfo":{"status":"ok","timestamp":1747106918871,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["test_data[2]"],"metadata":{"id":"y0gEO4RMDivE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106921943,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"dd640163-ae36-4e36-8c7d-6eed15e5fcca"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.6445, 1.0221, 0.6138, 0.8083, 0.7090]), tensor([1.]))"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["Dataloaders:"],"metadata":{"id":"JekDkaG6ecSs"}},{"cell_type":"code","source":["batch_size = 16\n","\n","training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"wYtgF-uGHBAa","executionInfo":{"status":"ok","timestamp":1747106924103,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["Network:"],"metadata":{"id":"DBDGwouee5tt"}},{"cell_type":"code","source":["class MySimpleClassificationNetworkClass(nn.Module):\n","    def __init__(self, input_dim, hidden_dim):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.linear_2 = nn.Linear(hidden_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","\n","    def forward(self, x):                             # x stands for the input that the network will use/act on later\n","        hidden = self.relu(self.linear_1(x))\n","        output = self.sigmoid(self.linear_2(hidden))\n","        return output\n","\n","my_classification_network = MySimpleClassificationNetworkClass(n_features, hidden_dim=8)"],"metadata":{"id":"awwnaVtrHMGq","executionInfo":{"status":"ok","timestamp":1747106928524,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["Let's do a quick test:"],"metadata":{"id":"DbnnqHZtfJvb"}},{"cell_type":"code","source":["my_classification_network(X_train)[:5]"],"metadata":{"id":"py4RtJOFHNeT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106931395,"user_tz":420,"elapsed":8,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"1ba8b21d-1cd4-4c6b-a55e-ed1bbb665dde"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.5463],\n","        [0.5662],\n","        [0.5569],\n","        [0.5710],\n","        [0.5695]], grad_fn=<SliceBackward0>)"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["Looks right, one output neuron per example. Classification changes somewhere in 50/50 area, as expected.\n","\n","Loss Function & Optimizer:"],"metadata":{"id":"RzLuoQQRiI_W"}},{"cell_type":"code","source":["loss_fn = nn.BCELoss()\n","adam_optimizer = torch.optim.Adam(my_classification_network.parameters(), lr=0.001)"],"metadata":{"id":"6LCkCX04iFhK","executionInfo":{"status":"ok","timestamp":1747106934245,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready for the training and eval loops, following and modifying https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation.\n","\n","Training & Eval loops:"],"metadata":{"id":"DT2kgjX3g9pM"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    # Set the model to training mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    epoch_loss = 0\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Compute prediction and loss\n","\n","        pred = model(X)\n","\n","        loss = loss_fn(pred, y)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()                      # the gradients need to be zeroed out after the gradients are applied by the optimizer\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n","\n","\n","\n","def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for batch, (X, y) in enumerate(dataloader):\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            #print((pred.argmax(1) == y))\n","            predictions = [int(x[0] > 0.5) for x in pred]\n","            labels = [int(x[0] > 0.5) for x in y]\n","            correct += np.sum([x == y for (x, y) in zip(predictions, labels)])\n","\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"Je_zSonEHPYG","executionInfo":{"status":"ok","timestamp":1747106936702,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":["Now... let's go! Let's also allow to place the model and the data on cpu or gpu via setting the device."],"metadata":{"id":"25_LlaagimBI"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # try cuda if gpu is available\n","\n","my_classification_network = my_classification_network.to('cuda')\n","X_train = X_train.to(device)\n","X_test = X_test.to(device)\n","y_train = y_train.to(device)\n","y_test = y_test.to(device)\n","batch_size = 16\n","\n","training_data = MyDataset(X_train, y_train)\n","test_data = MyDataset(X_test, y_test)\n","\n","training_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n","\n"],"metadata":{"id":"alk_zwqugVZ5","executionInfo":{"status":"ok","timestamp":1747106941799,"user_tz":420,"elapsed":177,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"id":"7-1nL_QpsUvn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106945807,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ee9b6c9b-2067-48ff-e61e-3fb8c196e0e8"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["%%time\n","epochs = 20\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(training_dataloader, my_classification_network, loss_fn, adam_optimizer)\n","    test_loop(test_dataloader, my_classification_network, loss_fn) # no optimizer use here!\n","print(\"Done!\")"],"metadata":{"id":"cQt0ZHi4HSNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106949084,"user_tz":420,"elapsed":1822,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"6c99ea51-5f34-4b7d-c79c-0fe0f9ab7226"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.699061 \n","\n","Test Results: \n"," Test Accuracy: 50.0%, Avg test loss: 0.677203 \n","\n","Epoch 2\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.693292 \n","\n","Test Results: \n"," Test Accuracy: 50.0%, Avg test loss: 0.664336 \n","\n","Epoch 3\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.686206 \n","\n","Test Results: \n"," Test Accuracy: 50.0%, Avg test loss: 0.668953 \n","\n","Epoch 4\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.681830 \n","\n","Test Results: \n"," Test Accuracy: 51.0%, Avg test loss: 0.670154 \n","\n","Epoch 5\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.677042 \n","\n","Test Results: \n"," Test Accuracy: 53.0%, Avg test loss: 0.661207 \n","\n","Epoch 6\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.669740 \n","\n","Test Results: \n"," Test Accuracy: 57.0%, Avg test loss: 0.658529 \n","\n","Epoch 7\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.665649 \n","\n","Test Results: \n"," Test Accuracy: 59.0%, Avg test loss: 0.632902 \n","\n","Epoch 8\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.657852 \n","\n","Test Results: \n"," Test Accuracy: 60.0%, Avg test loss: 0.636347 \n","\n","Epoch 9\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.647929 \n","\n","Test Results: \n"," Test Accuracy: 63.0%, Avg test loss: 0.632781 \n","\n","Epoch 10\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.638075 \n","\n","Test Results: \n"," Test Accuracy: 67.0%, Avg test loss: 0.628982 \n","\n","Epoch 11\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.626848 \n","\n","Test Results: \n"," Test Accuracy: 69.0%, Avg test loss: 0.612028 \n","\n","Epoch 12\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.615448 \n","\n","Test Results: \n"," Test Accuracy: 72.0%, Avg test loss: 0.606758 \n","\n","Epoch 13\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.604296 \n","\n","Test Results: \n"," Test Accuracy: 73.0%, Avg test loss: 0.597096 \n","\n","Epoch 14\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.591367 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.589021 \n","\n","Epoch 15\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.579530 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.566345 \n","\n","Epoch 16\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.564866 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.556767 \n","\n","Epoch 17\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.553355 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.578680 \n","\n","Epoch 18\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.540024 \n","\n","Test Results: \n"," Test Accuracy: 78.0%, Avg test loss: 0.542883 \n","\n","Epoch 19\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.527114 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.518620 \n","\n","Epoch 20\n","-------------------------------\n","Training Results: \n","  Avg train loss: 0.514960 \n","\n","Test Results: \n"," Test Accuracy: 77.0%, Avg test loss: 0.551670 \n","\n","Done!\n","CPU times: user 1.22 s, sys: 123 ms, total: 1.34 s\n","Wall time: 1.82 s\n"]}]},{"cell_type":"code","source":["np.exp(0.69)"],"metadata":{"id":"O6O28v4nZApJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747106955923,"user_tz":420,"elapsed":39,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"af9ccb7c-0760-4a98-b783-9baeb6db491f"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float64(1.9937155332430823)"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","source":["Looks like a decent enough model.\n"],"metadata":{"id":"s4gSB9NhjPK5"}},{"cell_type":"markdown","source":["\n","How could we save and later restore?\n","\n","### 5. Saving and Restoring\n","\n","Easy!\n","\n","If you want to save the full model with structure and parameters, use:\n","\n"],"metadata":{"id":"_NyOL84bs8uq"}},{"cell_type":"code","source":["torch.save(my_classification_network, 'my_new_classification_network.pth')"],"metadata":{"id":"sbgK9LSYjriu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can then restore it into a new network as:"],"metadata":{"id":"mcOFfDR0yKoM"}},{"cell_type":"code","source":["my_restored_classification_network = torch.load('my_new_classification_network.pth')"],"metadata":{"id":"crfBQnJ1yG3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loop(test_dataloader, my_restored_classification_network, loss_fn)"],"metadata":{"id":"LE7DnM_QykY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next notebook, let's apply everything we have learned to a real problem: sentiment classification for texts!"],"metadata":{"id":"giThQpnkyyyp"}}]}