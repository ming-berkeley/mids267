{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"102ccd2dfcf34a879477140eeb513546":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38f94869ddb7457192249ecb0413f9f1","IPY_MODEL_4a986886df174e229a1cce191b846661","IPY_MODEL_21c7d8a8c04842b9bd9abc7818abf629"],"layout":"IPY_MODEL_e0a115113bf34e849ba387d3d1c2626c"}},"38f94869ddb7457192249ecb0413f9f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c3b3b1faeaf4fb9ad44fc32b84677b1","placeholder":"​","style":"IPY_MODEL_5767cc2ad6194b3e9f180816b239b0f3","value":"README.md: 100%"}},"4a986886df174e229a1cce191b846661":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6329d4101a6d46efbbfa9c68bbafbbfa","max":7809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0f6f6b88f1042daa74e7dd830b01acc","value":7809}},"21c7d8a8c04842b9bd9abc7818abf629":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac868c27173b4e2698d495cad9dda8ba","placeholder":"​","style":"IPY_MODEL_27cef823e1b2466e8ff5c452b62cea40","value":" 7.81k/7.81k [00:00&lt;00:00, 726kB/s]"}},"e0a115113bf34e849ba387d3d1c2626c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c3b3b1faeaf4fb9ad44fc32b84677b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5767cc2ad6194b3e9f180816b239b0f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6329d4101a6d46efbbfa9c68bbafbbfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f6f6b88f1042daa74e7dd830b01acc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac868c27173b4e2698d495cad9dda8ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27cef823e1b2466e8ff5c452b62cea40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eef5bb31fef4538a902d90680007145":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_371d301f29cf4eb193d6cec6a4d038ac","IPY_MODEL_e159a3c0ff164168aeb93f8d5797967d","IPY_MODEL_3fffbd4c0bcd4831989335930401456e"],"layout":"IPY_MODEL_08e379242220430b8b6709a0e70c478a"}},"371d301f29cf4eb193d6cec6a4d038ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d639018fb76c4acab7ff36e02528368a","placeholder":"​","style":"IPY_MODEL_cede834c479e4938af6305fd13877362","value":"train-00000-of-00001.parquet: 100%"}},"e159a3c0ff164168aeb93f8d5797967d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbb218b1dcf544c3b2ed9151b09fff6f","max":20979968,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2418a368c4264a84bec22fc2a95cf38f","value":20979968}},"3fffbd4c0bcd4831989335930401456e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c15311ee4a84471b09c368998d98be9","placeholder":"​","style":"IPY_MODEL_ecfbb1cf1a964c35ba667bf77130cfc2","value":" 21.0M/21.0M [00:00&lt;00:00, 150MB/s]"}},"08e379242220430b8b6709a0e70c478a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d639018fb76c4acab7ff36e02528368a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cede834c479e4938af6305fd13877362":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbb218b1dcf544c3b2ed9151b09fff6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2418a368c4264a84bec22fc2a95cf38f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c15311ee4a84471b09c368998d98be9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecfbb1cf1a964c35ba667bf77130cfc2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e4a6863758c4f8a8e0c8aaf7b1fa2d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d91337dd2a14fa8b68ce4c2dd1da215","IPY_MODEL_1f7b41afa8ff4b278f2eb66d47bcb5ca","IPY_MODEL_bbba70e96a42432886c4198c7b2d683d"],"layout":"IPY_MODEL_5e101c9995ce4ca48e4d5fd932bef012"}},"2d91337dd2a14fa8b68ce4c2dd1da215":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0f8329c056745fea5df8511111a3fb0","placeholder":"​","style":"IPY_MODEL_fd4b868dcf254850817e0b09eab2ea96","value":"test-00000-of-00001.parquet: 100%"}},"1f7b41afa8ff4b278f2eb66d47bcb5ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9517be60394a26863c77816d3aa0c3","max":20470363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e98ad424c5e149e38bb0d359ad288a6b","value":20470363}},"bbba70e96a42432886c4198c7b2d683d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d69bd5bad714249aaa86e6270d797ed","placeholder":"​","style":"IPY_MODEL_1aa9385b326b47d08c6242cfce1075bc","value":" 20.5M/20.5M [00:00&lt;00:00, 248MB/s]"}},"5e101c9995ce4ca48e4d5fd932bef012":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f8329c056745fea5df8511111a3fb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd4b868dcf254850817e0b09eab2ea96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa9517be60394a26863c77816d3aa0c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e98ad424c5e149e38bb0d359ad288a6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d69bd5bad714249aaa86e6270d797ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aa9385b326b47d08c6242cfce1075bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"658fc44b6ab44bdd88ab1500c8b7d819":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f91ce56bdf404352b4852126aa6144c6","IPY_MODEL_ea273d1f18864e139c920c1f0e15562c","IPY_MODEL_42dbafbf2e734ecb97411d30c42ffc89"],"layout":"IPY_MODEL_7fd2bde0d9cf4c73920f4e2ee722b66a"}},"f91ce56bdf404352b4852126aa6144c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea7e22c29a3f4920a4eaf24a488ea38b","placeholder":"​","style":"IPY_MODEL_54e4b7f2244e45208581d2046543efc5","value":"unsupervised-00000-of-00001.parquet: 100%"}},"ea273d1f18864e139c920c1f0e15562c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bad02e073016426d9cd524c6f9d1822d","max":41996509,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f517b5bc5734cc8889db2b5c033b8d4","value":41996509}},"42dbafbf2e734ecb97411d30c42ffc89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7c49f1240204b6b9b31e11933880b04","placeholder":"​","style":"IPY_MODEL_6fffdb1a53214f4b903cb27937b7ed2a","value":" 42.0M/42.0M [00:00&lt;00:00, 177MB/s]"}},"7fd2bde0d9cf4c73920f4e2ee722b66a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea7e22c29a3f4920a4eaf24a488ea38b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54e4b7f2244e45208581d2046543efc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bad02e073016426d9cd524c6f9d1822d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f517b5bc5734cc8889db2b5c033b8d4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7c49f1240204b6b9b31e11933880b04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fffdb1a53214f4b903cb27937b7ed2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"310f7180918b40589aff9b9f3d853c6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be2e4ae9f63d41cf8294b8c252fee53d","IPY_MODEL_c84decea85954942b9978308d3cda3a1","IPY_MODEL_21b50842f4d9496d92d382aab058858d"],"layout":"IPY_MODEL_08f64b61312d4129b6ddf115be582cfd"}},"be2e4ae9f63d41cf8294b8c252fee53d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a96ae480e8b434ea077ca9b9647b0c4","placeholder":"​","style":"IPY_MODEL_57cb53b9dae64186b2c71e750eec6f78","value":"Generating train split: 100%"}},"c84decea85954942b9978308d3cda3a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf9785903905409ca8d960d701289c0c","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33b2ab320cf54526b5a4942568ff62cb","value":25000}},"21b50842f4d9496d92d382aab058858d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_918325ecccae49a68f83accca0b91638","placeholder":"​","style":"IPY_MODEL_4221d560c82144f48ed650bdde26db62","value":" 25000/25000 [00:00&lt;00:00, 66659.80 examples/s]"}},"08f64b61312d4129b6ddf115be582cfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a96ae480e8b434ea077ca9b9647b0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57cb53b9dae64186b2c71e750eec6f78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf9785903905409ca8d960d701289c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33b2ab320cf54526b5a4942568ff62cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"918325ecccae49a68f83accca0b91638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4221d560c82144f48ed650bdde26db62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbcaa994f7e349dd8a74595081ece692":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab74e82523aa4f4e9a45127ede87aeaa","IPY_MODEL_8aed79598f3f48c98b2c14454633e6ec","IPY_MODEL_80949f4148be44c99013137ab9c065f0"],"layout":"IPY_MODEL_2af77cf390304a68912c1c24af5bd4b8"}},"ab74e82523aa4f4e9a45127ede87aeaa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aec4f861bc454aa4bf8d36958393a137","placeholder":"​","style":"IPY_MODEL_e24ea5944e0540cc8775358aa3046eba","value":"Generating test split: 100%"}},"8aed79598f3f48c98b2c14454633e6ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c7e825a2a3f474f8f40942fd241ff48","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee54a0d178e144f3a5949cbfaea11686","value":25000}},"80949f4148be44c99013137ab9c065f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6003cc22fd884a6bbd55c62d8ca99f38","placeholder":"​","style":"IPY_MODEL_ad57fc9f87d946ffb854cb77620cce0f","value":" 25000/25000 [00:00&lt;00:00, 95082.52 examples/s]"}},"2af77cf390304a68912c1c24af5bd4b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aec4f861bc454aa4bf8d36958393a137":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e24ea5944e0540cc8775358aa3046eba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c7e825a2a3f474f8f40942fd241ff48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee54a0d178e144f3a5949cbfaea11686":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6003cc22fd884a6bbd55c62d8ca99f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad57fc9f87d946ffb854cb77620cce0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9542a4bcf5a24859a8d5019ae3a5691e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5df98d258e4e416a8aadb5b73416c206","IPY_MODEL_ff5377cd9e5b4743bd43f7fd9f107f34","IPY_MODEL_72a375b56f434815814ebf69902d9a19"],"layout":"IPY_MODEL_cf24bd8dff2c44de85aa306e882483c5"}},"5df98d258e4e416a8aadb5b73416c206":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16b08d90894e492a9592b2913fe4de94","placeholder":"​","style":"IPY_MODEL_37f65e6c68e74ffda8412a11a239ced8","value":"Generating unsupervised split: 100%"}},"ff5377cd9e5b4743bd43f7fd9f107f34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bef766ae5c3f47a59b27ed7711cd833e","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e849ebf7304841b29b1075c19d1de820","value":50000}},"72a375b56f434815814ebf69902d9a19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6ce8e5f1d574aeeae26af4e43cd7207","placeholder":"​","style":"IPY_MODEL_00ae4b986b1540008d5e4b4cec30ce1f","value":" 50000/50000 [00:00&lt;00:00, 103019.13 examples/s]"}},"cf24bd8dff2c44de85aa306e882483c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16b08d90894e492a9592b2913fe4de94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f65e6c68e74ffda8412a11a239ced8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bef766ae5c3f47a59b27ed7711cd833e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e849ebf7304841b29b1075c19d1de820":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6ce8e5f1d574aeeae26af4e43cd7207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ae4b986b1540008d5e4b4cec30ce1f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### PyTorch Intro II: Applying the Basics to Sentiment Classification\n","\n","This notebook supports the Slides \"PyTorch Intro II: Sentiment Classification\".  It assumes full understanding of the first part (\"Basics\") and will introduce a new concept: Text Embeddings.\n","\n","1. Review: What Needs to be Done?\n","\n","2. A Warm-Up: Sentence Embeddings with the Sentence Transformer\n","\n","3. Sentiment Classification with GPT-2 - Option A: Straight Last Token Prediction Classification\n","    1. Downloading and working with GPT-2: Tokenizer & Model\n","    2. Data Preparation\n","    3. Network Setup\n","    4. Training\n","\n","\n","This notebook has been run on a T4 processor using Google Colab.\n","\n","Let's again start with a few installs and imports.\n","\n","The following installs are not required if you use Google Colab (at least not for 'Pro' version):\n"],"metadata":{"id":"9hZNSDOBJcBm"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"GuN074G-JK_T","executionInfo":{"status":"ok","timestamp":1747161503003,"user_tz":420,"elapsed":103825,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["%%capture\n","\n","!pip install torch\n","!pip install torchtext\n","!pip install transformers   # for our application example in the end\n","!pip install numpy"]},{"cell_type":"markdown","source":["The following installs are still required when you use Google Colab:"],"metadata":{"id":"DQtOFDHkLQZw"}},{"cell_type":"code","source":["%%capture\n","#!pip install torchdata\n","!pip install -U datasets fsspec huggingface_hub\n","#!pip install portalocker"],"metadata":{"id":"DfG3rtt821On","executionInfo":{"status":"ok","timestamp":1747161510569,"user_tz":420,"elapsed":7559,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Let's make sure we land on the proper device:"],"metadata":{"collapsed":false,"id":"LZmkkCXxuf0o"}},{"cell_type":"code","source":["import torch\n","#torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available() #workaround\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","\n","import numpy as np\n","import random\n","\n","from datasets import load_dataset\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device\n","\n"],"metadata":{"id":"9piY_5XnuVa9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161519721,"user_tz":420,"elapsed":9148,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"5cd4cfdf-8454-4c87-f634-7bd2fc59921c"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### 1. Review: What Needs to be Done?\n","\n","At the core, we will have a list/set/iterable of text inputs, and we will try to predict the sentiment, positiver or negative in this case. We will assign the value 0 to the negative class, and 1 to the positive class. So what do we need to do?\n","\n","1. Data:     \n","    \n","    a. We need to get the data. We will use the IMDB dataset, included in tochtext.datasets.    \n","\n","    b. We need to define the Dataset. This step would do all relevant pre-processing. In particular, we will\n","    1. For simplicity, only include examples that have at least our target length (max_tokens) and cut the input text off there.\n","    1. Tokenize the text, i.e., split the text into tokens, which are then represented as integers, corresponding to the number of the token in the vocabulary. (Example: vocabulary: ('I', 'you', 'animal', '_s', '_r'); 'you' -> 1, 'animals' -> 2, 3). We will use a model from HuggingFace to do this.\n","    1. We will convert the labels in the dataset (1: negative, 2: positive) to our classes 0 and 1 (we need that for the binary cross-entropy calculation)\n","    c. We need to define the Dataloader for batching and shuffling.\n","\n","2. Network:\n","    a. We need a way to convert text (token ids) into vectors. We will use a pre-trained HuggingFace GPT-2 model for this, which we will further train.\n","    b. We will add a hidden layer\n","    c. We will create a classification layer\n","\n","2. Training\n","    a. We will define our cost function as the binary cross-entropy\n","    b. We will use the Adam Optimizer\n","    c. We will write training and eval loops\n","      1.The training loop will update the parameters\n","      2. The eval loop will capture test accuracy and test loss.\n","\n","And that's it!\n","\n","But we will start with a warm-up: Sentence Embeddings with a pre-trained Sentence Transformer."],"metadata":{"id":"L6KA0CFntCX0"}},{"cell_type":"markdown","source":["### 2. Warm-Up: Sentence Embeddings with the Sentence Transformer\n","\n","The Sentence Transformer (see: https://huggingface.co/sentence-transformers) is a very handy (set of) pre-trained model(s) that converts sentence/document into a meaningful vector. This can serve as the base for Semantic Search, Text Clustering, etc."],"metadata":{"collapsed":false,"id":"l9VdYiNgw_jm"}},{"cell_type":"code","execution_count":4,"outputs":[],"source":["%%capture\n","!pip install -U sentence-transformers\n","!pip install portalocker"],"metadata":{"id":"sKa7yptuw_jm","executionInfo":{"status":"ok","timestamp":1747161530299,"user_tz":420,"elapsed":10583,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}}},{"cell_type":"code","execution_count":5,"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","import numpy as np"],"metadata":{"id":"HcAKBaRBw_jm","executionInfo":{"status":"ok","timestamp":1747161555038,"user_tz":420,"elapsed":24735,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}}},{"cell_type":"code","source":["%%capture\n","sentence_embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"],"metadata":{"id":"_cfq6LkI1gF4","executionInfo":{"status":"ok","timestamp":1747161570335,"user_tz":420,"elapsed":15273,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Let's look at a few layers in this model. You'll see the individual layers of the underlying Transformer!"],"metadata":{"id":"3-Ij8A6R3z1v"}},{"cell_type":"code","source":["print(f\"Model structure: {sentence_embedding_model}\\n\\n\")\n","\n","layer_count = 0\n","\n","\n","# Let's see the first 25 layers\n","num_layer_names_shown = 25\n","\n","\n","for layer_num, (name, param) in enumerate(sentence_embedding_model.named_parameters()):\n","    if layer_num < num_layer_names_shown:\n","      print(f\"Layer: {name} | Size: {param.size()}\")\n","\n","print(\"\\nNumber of layers in model: \", str(layer_num))\n","\n"],"metadata":{"id":"Bt9xzVQr3H6v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161570344,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"f7c78455-4926-4524-c0c4-87894214f4fa"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: SentenceTransformer(\n","  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n","  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",")\n","\n","\n","Layer: 0.auto_model.embeddings.word_embeddings.weight | Size: torch.Size([30522, 384])\n","Layer: 0.auto_model.embeddings.position_embeddings.weight | Size: torch.Size([512, 384])\n","Layer: 0.auto_model.embeddings.token_type_embeddings.weight | Size: torch.Size([2, 384])\n","Layer: 0.auto_model.embeddings.LayerNorm.weight | Size: torch.Size([384])\n","Layer: 0.auto_model.embeddings.LayerNorm.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.query.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.query.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.key.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.key.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.value.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.0.attention.self.value.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.output.dense.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.0.attention.output.dense.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.intermediate.dense.weight | Size: torch.Size([1536, 384])\n","Layer: 0.auto_model.encoder.layer.0.intermediate.dense.bias | Size: torch.Size([1536])\n","Layer: 0.auto_model.encoder.layer.0.output.dense.weight | Size: torch.Size([384, 1536])\n","Layer: 0.auto_model.encoder.layer.0.output.dense.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.output.LayerNorm.weight | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.0.output.LayerNorm.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.1.attention.self.query.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.1.attention.self.query.bias | Size: torch.Size([384])\n","Layer: 0.auto_model.encoder.layer.1.attention.self.key.weight | Size: torch.Size([384, 384])\n","Layer: 0.auto_model.encoder.layer.1.attention.self.key.bias | Size: torch.Size([384])\n","\n","Number of layers in model:  102\n"]}]},{"cell_type":"markdown","source":["Lot's going on. Let's ignore that for now, and instead look at a couple of embeddings for three sample sentences. Note that sentences 0 and 1 have **similar meanings**, and that sentences 1 and 2 have **similar words**. So a word(token)-matching approach would probably suggest that sentences 1 and two are most similar... clearly not what we want. How do the sentence embeddings do?"],"metadata":{"id":"f2VB-lXp4ydO"}},{"cell_type":"code","source":["sentences = ['What a nice start of the day', 'A great morning', 'A great problem']\n","\n","encoded_sentences = sentence_embedding_model.encode(sentences, normalize_embeddings=True)\n","encoded_sentences.shape"],"metadata":{"id":"1SfErif_1rrD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161571116,"user_tz":420,"elapsed":771,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"a84a4197-9757-455a-8354-d3d31e2c962f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 384)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["First off, this seems like what we need: three 384-dim vectors.\n","\n","Let's verify that the sentence embedding vectors are normalized:"],"metadata":{"id":"sBV6J-wlLx9f"}},{"cell_type":"code","source":["np.dot(encoded_sentences[0], encoded_sentences[0])"],"metadata":{"id":"Fn9jNM1R6hw7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161571117,"user_tz":420,"elapsed":13,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"748b6eb8-af9e-4c96-df72-6533e77dfbb6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float32(1.0)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Good. Now, what are the mutual dot products (equivalent to cosine similarities as the vectors are normalized)?"],"metadata":{"id":"bSFguyCy5OuS"}},{"cell_type":"code","source":["np.matmul(encoded_sentences, np.transpose(encoded_sentences))"],"metadata":{"id":"d0galB6D2PS4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161571117,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d00d56cf-c77b-49c5-8f4d-2122c53e2375"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.99999946, 0.70665336, 0.08578878],\n","       [0.70665336, 0.9999995 , 0.2648253 ],\n","       [0.08578878, 0.2648253 , 1.0000002 ]], dtype=float32)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["Great, the first two sentences (sentences 0 and 1) have a much larger similarity with each other than with the third sentence. That is what we want in embeddings!\n","\n","We could actually use these embeddings in a network and add a classification head (and other layer in between if desired) to perform sentiment classification. But we will follow a different approach for the sentiment classification.\n","\n","### 3.Sentiment Classification with GPT-2 - Option A: Straight Last Token Prediction Classification\n","\n","We will now use the idea of Sentence Embeddings for our Sentiment Classification problem. We will however use the GPT-2 model from HuggingFace (provided by OpenAI). This gives us another opportunity to also look at a generative AI model explicitly.\n","\n","The architectural picture can be found in the slides for this notebook. The idea is to use the **last output vector** to represent the full sentence, as its calculation is based on the entire input.\n","\n","#### a. Downloading and working with GPT-2: Tokenizer & Model\n","\n","We will first get the model, see here: https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Model\n"],"metadata":{"id":"6qglF4yw2lBP"}},{"cell_type":"code","execution_count":11,"outputs":[],"source":["%%capture\n","from transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification\n","\n","gpt_2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","gpt_2_model = GPT2Model.from_pretrained(\"gpt2\").to(device)"],"metadata":{"id":"aRHvtWbtw_jp","executionInfo":{"status":"ok","timestamp":1747161581971,"user_tz":420,"elapsed":10856,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}}},{"cell_type":"code","source":["tokenized_input = gpt_2_tokenizer(['Great. This is fun'], return_tensors=\"pt\").to(device)\n","tokenized_input"],"metadata":{"id":"GkNpNLOgsDG5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161581983,"user_tz":420,"elapsed":11,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ab9aa744-d91a-42e2-d0a5-777775f363cd"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[13681,    13,   770,   318,  1257]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["gpt_2_tokenizer.encode('Great. This is fun')"],"metadata":{"id":"ZwEWeNHLt5ly","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161581987,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"8015cd9d-888c-4f86-ff75-ee089bb09b41"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[13681, 13, 770, 318, 1257]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["gpt_2_tokenizer.decode(13681)"],"metadata":{"id":"Kg9KaNzeuCcC","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1747161581992,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"aae563c7-a457-4f8d-d72b-8ea1ccc5bf37"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Great'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["['Great. This is fun', 'I want to go and see a movie']"],"metadata":{"id":"GN14UGQ1ulJn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582016,"user_tz":420,"elapsed":23,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d5bc8995-530c-46c7-9b13-5db53c6f1fd8"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Great. This is fun', 'I want to go and see a movie']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["What do we see here?\n","\n","* input_ids: the positions in the vocabulary for the words(tokens)\n","* attention_mask: Is this position relevant, or 'filled in'? (Like if sentences are 'padded' with padding tokens to make sure that all sentences have the same. But that is for a deep dive discussion.)\n","\n","Here is a slightly more complex example:"],"metadata":{"collapsed":false,"id":"uJuYrkhKw_jp"}},{"cell_type":"code","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[ 1212,   318,  1049, 50257, 50257, 50257, 50257],\n","        [38864,  3481,  1760,   416,   345,     0, 50257]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 0]], device='cuda:0')}"]},"metadata":{},"execution_count":16}],"source":["gpt_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","tokenized_input_2 = gpt_2_tokenizer(['This is great', 'Marvelously done by you!'],\n","                              return_tensors=\"pt\",\n","                              max_length=7,\n","                              truncation=True,\n","                              padding='max_length').to(device)\n","tokenized_input_2"],"metadata":{"id":"XwHMzRyBw_jq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582027,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"807daaff-23f5-440a-dff0-ff61d34c4212"}},{"cell_type":"code","source":["device"],"metadata":{"id":"Y7YjT_-SvtqW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582028,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"cccd8033-3739-4ed0-996c-8e364cb62dc1"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 1, 1, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 0]], device='cuda:0')"]},"metadata":{},"execution_count":18}],"source":["tokenized_input_2['attention_mask']"],"metadata":{"id":"RvlaQbpwuf0r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582028,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"9db7c27c-e30f-421c-d7b4-403364ecf7b3"}},{"cell_type":"markdown","source":["Two examples in batch, length 7. Seems right.\n","\n","For future comparison, let's extract some individual weights and biases. (We'll later see that they were changed during the upcoming Fine-Tuning process). We'll pick a random one: 'h.11.ln_2.bias' and look at the first 10 values:"],"metadata":{"id":"AIEHwkU2dpOw"}},{"cell_type":"code","source":["pre_train_param_values = []\n","\n","print(f\"Model structure: {gpt_2_model}\\n\\n\")\n","\n","layer_count = 0\n","\n","for param in gpt_2_model.parameters():\n","    param.requires_grad = True   # make sure that the gpt2 model layers can be retrained (try also without...)\n","\n","num_layer_names_shown = 100000\n","\n","\n","for layer_num, (name, param) in enumerate(gpt_2_model.named_parameters()):\n","  pre_train_param_values.append(param.cpu().detach().numpy())\n","\n","pre_train_param_values[15][:15]"],"metadata":{"id":"m1XYsxrXRKLj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582413,"user_tz":420,"elapsed":386,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"da1de6b4-57d3-4099-f3d0-5feeee9ae8a6"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: GPT2Model(\n","  (wte): Embedding(50257, 768)\n","  (wpe): Embedding(1024, 768)\n","  (drop): Dropout(p=0.1, inplace=False)\n","  (h): ModuleList(\n","    (0-11): 12 x GPT2Block(\n","      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (attn): GPT2Attention(\n","        (c_attn): Conv1D(nf=2304, nx=768)\n","        (c_proj): Conv1D(nf=768, nx=768)\n","        (attn_dropout): Dropout(p=0.1, inplace=False)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (mlp): GPT2MLP(\n","        (c_fc): Conv1D(nf=3072, nx=768)\n","        (c_proj): Conv1D(nf=768, nx=3072)\n","        (act): NewGELUActivation()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-0.00362932, -0.00622698, -0.00362774, -0.0011091 , -0.01039783,\n","       -0.00634792, -0.02703804,  0.04477038, -0.01744087, -0.00305082,\n","       -0.00317421, -0.00448913, -0.00828817, -0.01425266, -0.01139623],\n","      dtype=float32)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["gpt_2_model"],"metadata":{"id":"7AmJwkDYwVqP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582451,"user_tz":420,"elapsed":37,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"07237402-6608-4e8e-e0d5-2b40cd6cbe17"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2Model(\n","  (wte): Embedding(50257, 768)\n","  (wpe): Embedding(1024, 768)\n","  (drop): Dropout(p=0.1, inplace=False)\n","  (h): ModuleList(\n","    (0-11): 12 x GPT2Block(\n","      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (attn): GPT2Attention(\n","        (c_attn): Conv1D(nf=2304, nx=768)\n","        (c_proj): Conv1D(nf=768, nx=768)\n","        (attn_dropout): Dropout(p=0.1, inplace=False)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (mlp): GPT2MLP(\n","        (c_fc): Conv1D(nf=3072, nx=768)\n","        (c_proj): Conv1D(nf=768, nx=3072)\n","        (act): NewGELUActivation()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["pre_train_param_values[15][:15]"],"metadata":{"id":"HZCwGtClwRgW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582457,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"a6f09012-6ac3-4afb-aba2-5617edd6f985"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.00362932, -0.00622698, -0.00362774, -0.0011091 , -0.01039783,\n","       -0.00634792, -0.02703804,  0.04477038, -0.01744087, -0.00305082,\n","       -0.00317421, -0.00448913, -0.00828817, -0.01425266, -0.01139623],\n","      dtype=float32)"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Ok. So overall, what happened here?\n","\n","* We asked for truncation and padding to length 7. This made sure that we do get a tensor; all length are identical.\n","* The 'attention_mask' now has 1s and 0s. The 0s correspond to filled-in 'padding tokens'\n","* Historically, GPT-2 did not have a padding token, and we needed to add it. (Moving forward, we can ignore the padding aspect as we will only consider examples that fit. But know that this is generally an important aspect.)\n","* The first sentence with 3 words has 3 non-adding tokens (1212, 318, 1049). Makes sense. But the second one with 4 words and one exclamation mark has 6 non-padding tokens. Why? 'Marvelously' was split into 2 tokens, and the '!' has its own token\n"],"metadata":{"collapsed":false,"id":"n8sXiKXHw_jq"}},{"cell_type":"markdown","source":["What about the action of the model on our inputs? First off, a lot of things are returned (and more can be returned! See the HuggungFace specifications)."],"metadata":{"collapsed":false,"id":"T3_g6X2yw_jq"}},{"cell_type":"code","source":["tokenized_input"],"metadata":{"id":"55uY_LFswfoN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582460,"user_tz":420,"elapsed":2,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"627ceafc-3491-4ad7-e7b8-7b7412768425"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[13681,    13,   770,   318,  1257]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'past_key_values'])"]},"metadata":{},"execution_count":23}],"source":["gpt_2_model_output = gpt_2_model(**tokenized_input)\n","gpt_2_model_output.keys()"],"metadata":{"id":"sHYhPuBaw_jr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582562,"user_tz":420,"elapsed":101,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"91a7ec2c-9b90-4791-9996-4af88c49c9f2"}},{"cell_type":"markdown","source":["What is in the last hidden state? Is that something we want?"],"metadata":{"collapsed":false,"id":"ff7DpzpOw_jr"}},{"cell_type":"code","execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":24}],"source":["gpt_2_model_output['last_hidden_state'].shape"],"metadata":{"id":"04jcp3uIw_jr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582589,"user_tz":420,"elapsed":25,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"19a1652a-3939-4817-ae87-3300aa7d92c4"}},{"cell_type":"markdown","source":["That makes sense. 2 sentences, 3 words each, and a 768-dim output for standard GPT-2. So this corresponds to the output vector for sentence _i_ and position _j_.\n","\n","We will need the __last__ output vector for each sentence, as that will have seen the entire sentence (see class discussion). How do we get that?\n","\n"],"metadata":{"collapsed":false,"id":"c2kwIIo7w_jr"}},{"cell_type":"code","execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 768])"]},"metadata":{},"execution_count":25}],"source":["last_position_logits = gpt_2_model(**tokenized_input)['last_hidden_state'][:, -1, :]\n","last_position_logits.shape"],"metadata":{"id":"VPzlGF0aw_jr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161582617,"user_tz":420,"elapsed":28,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"425ff054-b588-403d-9127-085cb1ae7cde"}},{"cell_type":"markdown","source":["This will be our document embedding later."],"metadata":{"collapsed":false,"id":"ZpiBRGChw_jr"}},{"cell_type":"markdown","source":["#### b. Data Preparation\n","\n","Now we need the data and create the Dataset with all pre-processing and the Dataloader. We will use the IMDB dataset sourced from Hugging Face using the dataset library (https://huggingface.co/datasets/stanfordnlp/imdb) provided by the Stanford NLP team."],"metadata":{"collapsed":false,"id":"z89fkrunw_js"}},{"cell_type":"code","source":["imdb_dataset = load_dataset(\"IMDB\")"],"metadata":{"id":"KPUYkG4Cnjbh","colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["102ccd2dfcf34a879477140eeb513546","38f94869ddb7457192249ecb0413f9f1","4a986886df174e229a1cce191b846661","21c7d8a8c04842b9bd9abc7818abf629","e0a115113bf34e849ba387d3d1c2626c","9c3b3b1faeaf4fb9ad44fc32b84677b1","5767cc2ad6194b3e9f180816b239b0f3","6329d4101a6d46efbbfa9c68bbafbbfa","a0f6f6b88f1042daa74e7dd830b01acc","ac868c27173b4e2698d495cad9dda8ba","27cef823e1b2466e8ff5c452b62cea40","1eef5bb31fef4538a902d90680007145","371d301f29cf4eb193d6cec6a4d038ac","e159a3c0ff164168aeb93f8d5797967d","3fffbd4c0bcd4831989335930401456e","08e379242220430b8b6709a0e70c478a","d639018fb76c4acab7ff36e02528368a","cede834c479e4938af6305fd13877362","bbb218b1dcf544c3b2ed9151b09fff6f","2418a368c4264a84bec22fc2a95cf38f","3c15311ee4a84471b09c368998d98be9","ecfbb1cf1a964c35ba667bf77130cfc2","2e4a6863758c4f8a8e0c8aaf7b1fa2d3","2d91337dd2a14fa8b68ce4c2dd1da215","1f7b41afa8ff4b278f2eb66d47bcb5ca","bbba70e96a42432886c4198c7b2d683d","5e101c9995ce4ca48e4d5fd932bef012","a0f8329c056745fea5df8511111a3fb0","fd4b868dcf254850817e0b09eab2ea96","aa9517be60394a26863c77816d3aa0c3","e98ad424c5e149e38bb0d359ad288a6b","8d69bd5bad714249aaa86e6270d797ed","1aa9385b326b47d08c6242cfce1075bc","658fc44b6ab44bdd88ab1500c8b7d819","f91ce56bdf404352b4852126aa6144c6","ea273d1f18864e139c920c1f0e15562c","42dbafbf2e734ecb97411d30c42ffc89","7fd2bde0d9cf4c73920f4e2ee722b66a","ea7e22c29a3f4920a4eaf24a488ea38b","54e4b7f2244e45208581d2046543efc5","bad02e073016426d9cd524c6f9d1822d","6f517b5bc5734cc8889db2b5c033b8d4","b7c49f1240204b6b9b31e11933880b04","6fffdb1a53214f4b903cb27937b7ed2a","310f7180918b40589aff9b9f3d853c6a","be2e4ae9f63d41cf8294b8c252fee53d","c84decea85954942b9978308d3cda3a1","21b50842f4d9496d92d382aab058858d","08f64b61312d4129b6ddf115be582cfd","5a96ae480e8b434ea077ca9b9647b0c4","57cb53b9dae64186b2c71e750eec6f78","cf9785903905409ca8d960d701289c0c","33b2ab320cf54526b5a4942568ff62cb","918325ecccae49a68f83accca0b91638","4221d560c82144f48ed650bdde26db62","fbcaa994f7e349dd8a74595081ece692","ab74e82523aa4f4e9a45127ede87aeaa","8aed79598f3f48c98b2c14454633e6ec","80949f4148be44c99013137ab9c065f0","2af77cf390304a68912c1c24af5bd4b8","aec4f861bc454aa4bf8d36958393a137","e24ea5944e0540cc8775358aa3046eba","5c7e825a2a3f474f8f40942fd241ff48","ee54a0d178e144f3a5949cbfaea11686","6003cc22fd884a6bbd55c62d8ca99f38","ad57fc9f87d946ffb854cb77620cce0f","9542a4bcf5a24859a8d5019ae3a5691e","5df98d258e4e416a8aadb5b73416c206","ff5377cd9e5b4743bd43f7fd9f107f34","72a375b56f434815814ebf69902d9a19","cf24bd8dff2c44de85aa306e882483c5","16b08d90894e492a9592b2913fe4de94","37f65e6c68e74ffda8412a11a239ced8","bef766ae5c3f47a59b27ed7711cd833e","e849ebf7304841b29b1075c19d1de820","f6ce8e5f1d574aeeae26af4e43cd7207","00ae4b986b1540008d5e4b4cec30ce1f"]},"executionInfo":{"status":"ok","timestamp":1747161589257,"user_tz":420,"elapsed":6639,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"8e6bff29-37a4-4b6e-a536-488112606355"},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102ccd2dfcf34a879477140eeb513546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eef5bb31fef4538a902d90680007145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e4a6863758c4f8a8e0c8aaf7b1fa2d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658fc44b6ab44bdd88ab1500c8b7d819"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"310f7180918b40589aff9b9f3d853c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbcaa994f7e349dd8a74595081ece692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9542a4bcf5a24859a8d5019ae3a5691e"}},"metadata":{}}]},{"cell_type":"markdown","source":["You always want to look at the data right away to see the structure:"],"metadata":{"id":"KLQqcVw1aXdg"}},{"cell_type":"code","source":["imdb_dataset['train'][6]"],"metadata":{"id":"gLY61Z1RoWr9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161589293,"user_tz":420,"elapsed":31,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"c54857ea-d60a-4f34-9c8b-ddaa0b544422"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'text': \"Whoever wrote the screenplay for this movie obviously never consulted any books about Lucille Ball, especially her autobiography. I've never seen so many mistakes in a biopic, ranging from her early years in Celoron and Jamestown to her later years with Desi. I could write a whole list of factual errors, but it would go on for pages. In all, I believe that Lucille Ball is one of those inimitable people who simply cannot be portrayed by anyone other than themselves. If I were Lucie Arnaz and Desi, Jr., I would be irate at how many mistakes were made in this film. The filmmakers tried hard, but the movie seems awfully sloppy to me.\",\n"," 'label': 0}"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["So '0' seems to be 'negative'. (And we know separately that '1' is positive, but feel free to check yourself.) Let's count the numbers of positive and negative examples."],"metadata":{"collapsed":false,"id":"Jgk05Yxow_js"}},{"cell_type":"code","source":["def create_temp_set(base_data, num_examples=1000000000):\n","    num_positive = 0\n","    num_negative = 0\n","    num_other = 0\n","\n","    temp_data = []\n","    out_data = []\n","\n","    for example_num, example in enumerate(base_data):\n","\n","      temp_data.append(example)\n","\n","    random.shuffle(temp_data)\n","\n","    for example_num, example in enumerate(temp_data):\n","\n","      if num_examples != -1 and example_num > num_examples:\n","        break\n","\n","      if example['label'] == 0:\n","        num_negative += 1\n","      elif example['label'] == 1:\n","        num_positive += 1\n","      else:\n","        num_other += 1\n","\n","      out_data.append(example)\n","\n","\n","    print('positive: ', num_positive)\n","    print('negative: ', num_negative)\n","    print('other: ', num_other)\n","\n","    return out_data\n","\n","# There appears to be an issue with the IMDB data. Let's which of the two sets has the full data and pick that one.\n","my_imdb_data_train = create_temp_set(imdb_dataset['train'], 10000)\n","my_imdb_data_test = create_temp_set(imdb_dataset['test'], 2000)\n","\n","[x['label'] for x in my_imdb_data_train[:20]]"],"metadata":{"id":"-GaQyM_jsRzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161590692,"user_tz":420,"elapsed":1401,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"fa12cbee-df47-4b41-bb62-47e64fa49d61"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["positive:  5037\n","negative:  4964\n","other:  0\n","positive:  950\n","negative:  1051\n","other:  0\n"]},{"output_type":"execute_result","data":{"text/plain":["[1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["This looks reasonable."],"metadata":{"collapsed":false,"id":"cGZrbHnVw_jt"}},{"cell_type":"markdown","source":["We will now create the actual Dataset and the Dataloader.\n","\n","We will use the Dataset definition to properly format the data:\n","\n","* we will ignore examples that do not have our desired minimum length\n","* we may (see Option B) 'add a prompt', i.e., augment the text so that _the natural next token prediction would actually be the sentiment (example: 'This did not go well.' -> 'Here is a statement: This did not go well. Is the sentiment positive or negative?' The classification would be done from the output of the last '?')\n","* we need to convert the input text into input ids etc.\n","\n","\n","A good reference is also [here (Sentiment Analysis using Roberta)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb#scrollTo=3vWRDemOGxJD), where the Encoder model RoBertA is used.\n"],"metadata":{"id":"2TnHlOC30FHj"}},{"cell_type":"code","source":["class ClassificationData(Dataset):\n","    def __init__(self, base_data, tokenizer, max_len, use_prompt=False):\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","\n","        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n","        for example_num, example in enumerate(base_data):\n","\n","\n","            try:\n","              token_encoder = tokenizer(example['text'])['input_ids']\n","            except:\n","              print(example_num)\n","              break\n","\n","            try:\n","              if len(token_encoder) <= self.max_len:\n","                continue    # avoids complications with short sentences. No padding is needed then.\n","            except:\n","              print(example_num)\n","              print(token_encoder)\n","              print(len(token_encoder))\n","              break\n","\n","            truncated_encoding = token_encoder[:self.max_len]\n","            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n","\n","            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n","\n","            if use_prompt:\n","                cutoff = self.max_len + 13\n","                prompted_text_line = 'Here is a movie review: ' + truncated_example + ' Is this review positive or negative?'\n","\n","            else:\n","                cutoff = self.max_len\n","                prompted_text_line = truncated_example\n","\n","            if len(gpt_2_tokenizer(prompted_text_line)['input_ids']) != cutoff:\n","                    continue\n","\n","            tokenized_example = self.tokenizer(prompted_text_line,\n","                                               return_tensors=\"pt\",\n","                                               max_length=cutoff,\n","                                               truncation=True,\n","                                               padding='max_length').to(device)\n","\n","            self.data.append({'label': (float(example['label'])),\n","                              'tokenized_text':\n","                                  {'input_ids': torch.tensor(torch.squeeze(tokenized_example['input_ids']),\n","                                      device=device),\n","                                   'attention_mask': torch.tensor(torch.squeeze(tokenized_example['attention_mask']),\n","                                      device=device)\n","                                   }\n","                              })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input': self.data[index]['tokenized_text'],\n","            'label': torch.tensor(self.data[index]['label'],\n","                                  dtype=torch.float,\n","                                  device=device)\n","        }"],"metadata":{"id":"F14utSM1OkY2","executionInfo":{"status":"ok","timestamp":1747161590737,"user_tz":420,"elapsed":38,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-29-c18cad85e06e>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  {'input_ids': torch.tensor(torch.squeeze(tokenized_example['input_ids']),\n","<ipython-input-29-c18cad85e06e>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'attention_mask': torch.tensor(torch.squeeze(tokenized_example['attention_mask']),\n","Token indices sequence length is longer than the specified maximum sequence length for this model (1257 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["train_data = ClassificationData(my_imdb_data_train, tokenizer=gpt_2_tokenizer, max_len=100)\n","test_data = ClassificationData(my_imdb_data_test, tokenizer=gpt_2_tokenizer, max_len=100)"],"metadata":{"id":"z3-86YHfw_ju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161616964,"user_tz":420,"elapsed":26230,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"4a0e134a-3498-45af-a6a2-c032f600aa13"}},{"cell_type":"markdown","source":["Quick peek at what we have created:"],"metadata":{"collapsed":false,"id":"RKcmB3O0w_jv"}},{"cell_type":"markdown","source":["Perfect. This looks like the input_ids and attention masks (not critical here) for the input plus the labels.  And all data is the GPU, if available.\n","\n","And now to the Dataloaders to create batches and deal with shuffling (we don't need to do this here anymore):"],"metadata":{"collapsed":false,"id":"jna9pxDww_jv"}},{"cell_type":"code","source":["batch_size = 16\n","train_texts = DataLoader(train_data, batch_size=batch_size, shuffle=True) # usually we shuffle, but we shuffled already above\n","test_texts = DataLoader(test_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"HN5V3AXv_gJU","executionInfo":{"status":"ok","timestamp":1747161616967,"user_tz":420,"elapsed":18,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Another peek, now at the Dataloader. Do we see the right shapes?"],"metadata":{"id":"1nEEm6tCGFWF"}},{"cell_type":"code","source":["next(iter(test_texts)).keys()"],"metadata":{"id":"cM4I-tfaGJdk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161616977,"user_tz":420,"elapsed":10,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"448933b5-b4d4-4e0e-cf41-3225afd74da1"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input', 'label'])"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["\n","Yup!\n","\n","#### c. Network Setup\n","\n","\n","How would we use this type of model for our classification goal? We could use it as the first layer in our network, where we use the (tokenized) review as input, then use the last output vector as input to the classification layer."],"metadata":{"id":"dLr90zwfHGk1"}},{"cell_type":"code","source":["class MyTextClassificationNetworkClass(torch.nn.Module):\n","    def __init__(self, embedding_model, embedding_model_dim):\n","        super().__init__()\n","        self.lm = embedding_model\n","        self.dropout = torch.nn.Dropout(0.2)\n","        self.linear_2 = torch.nn.Linear(embedding_model_dim, 1)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","\n","    def forward(self, x):                             # x stands for the input that the network will use/act on later\n","        embeddings = self.lm(**x)['last_hidden_state'][:, -1]\n","        dropout_embeddings = self.dropout(embeddings)\n","        output = self.linear_2(dropout_embeddings)[:, 0]  # flatten the last dimension, there is only 1 neuron. And this is what the cost function will expect\n","        output = self.sigmoid(output)\n","        #output = self.sigmoid(self.linear_2(dropout_embeddings))[:, 0]  # flatten the last dimension, there is only 1 neuron\n","        return output\n","\n","my_text_classification_network = MyTextClassificationNetworkClass(embedding_model=gpt_2_model,\n","                                                                  embedding_model_dim=768)\n","\n","my_text_classification_network.to(device)"],"metadata":{"id":"RGXqvAkpBPr6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161616981,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"7a03bc70-c578-400e-b615-3d393d2b7e42"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MyTextClassificationNetworkClass(\n","  (lm): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (linear_2): Linear(in_features=768, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["NOTES:\n","\n","  - we can obviously build the network layer-by-layer and test whether we get the expected output dimensions!   \n","  - The 'last_hidden_state' does **not** refer to the last token in the sequence, but rather the output of the last Transformer layer! So we need to add the [:, -1] slicing.\n","\n","Does this work?"],"metadata":{"collapsed":false,"id":"lSdGywvHuf0-"}},{"cell_type":"code","execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': {'input_ids': tensor([[ 5246, 26510,   373,  ...,    11,   290,  1312],\n","          [ 5779,   612,   338,  ...,   355,   257,  3957],\n","          [ 1212,  2646,  4952,  ...,   326,   673,   561],\n","          ...,\n","          [   40,   550,   262,  ...,  4687,    74,  2427],\n","          [ 5703,   355,   262,  ...,  6735,  7328,   780],\n","          [   40,  2497,   428,  ..., 35589,    13,   843]], device='cuda:0'),\n","  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","          [1, 1, 1,  ..., 1, 1, 1],\n","          [1, 1, 1,  ..., 1, 1, 1],\n","          ...,\n","          [1, 1, 1,  ..., 1, 1, 1],\n","          [1, 1, 1,  ..., 1, 1, 1],\n","          [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')},\n"," 'label': tensor([1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n","        device='cuda:0')}"]},"metadata":{},"execution_count":34}],"source":["example = next(iter(test_texts))\n","example\n","\n"],"metadata":{"id":"dRNzG8zVuf0-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617186,"user_tz":420,"elapsed":204,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"575fb5cd-bd8a-4ebf-a314-538edf96e060"}},{"cell_type":"code","source":["print(my_text_classification_network(example['input']))"],"metadata":{"id":"3PoaUX7az5d_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617399,"user_tz":420,"elapsed":209,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"390aa1c4-64d8-4c50-d400-319b393109ea"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9962, 0.9796, 0.9993, 0.0323, 0.9978, 0.9902, 0.8796, 0.7315, 0.9720,\n","        0.8857, 0.9873, 0.9907, 0.8540, 0.8927, 0.7251, 0.4248],\n","       device='cuda:0', grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"markdown","source":["Good. This looks right, dimension-wise. Obviously no valid predictions because the models hasn't been trained yet.\n","\n","\n","What are the layers? Do we see our added classification layer?"],"metadata":{"collapsed":false,"id":"FkWM_obiuf0-"}},{"cell_type":"code","source":["for name, param in my_text_classification_network.named_parameters():\n","  print(name)\n"],"metadata":{"id":"iTTKDzf04m43","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617410,"user_tz":420,"elapsed":15,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0948f36a-49db-470e-bea9-c2589c581255"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["lm.wte.weight\n","lm.wpe.weight\n","lm.h.0.ln_1.weight\n","lm.h.0.ln_1.bias\n","lm.h.0.attn.c_attn.weight\n","lm.h.0.attn.c_attn.bias\n","lm.h.0.attn.c_proj.weight\n","lm.h.0.attn.c_proj.bias\n","lm.h.0.ln_2.weight\n","lm.h.0.ln_2.bias\n","lm.h.0.mlp.c_fc.weight\n","lm.h.0.mlp.c_fc.bias\n","lm.h.0.mlp.c_proj.weight\n","lm.h.0.mlp.c_proj.bias\n","lm.h.1.ln_1.weight\n","lm.h.1.ln_1.bias\n","lm.h.1.attn.c_attn.weight\n","lm.h.1.attn.c_attn.bias\n","lm.h.1.attn.c_proj.weight\n","lm.h.1.attn.c_proj.bias\n","lm.h.1.ln_2.weight\n","lm.h.1.ln_2.bias\n","lm.h.1.mlp.c_fc.weight\n","lm.h.1.mlp.c_fc.bias\n","lm.h.1.mlp.c_proj.weight\n","lm.h.1.mlp.c_proj.bias\n","lm.h.2.ln_1.weight\n","lm.h.2.ln_1.bias\n","lm.h.2.attn.c_attn.weight\n","lm.h.2.attn.c_attn.bias\n","lm.h.2.attn.c_proj.weight\n","lm.h.2.attn.c_proj.bias\n","lm.h.2.ln_2.weight\n","lm.h.2.ln_2.bias\n","lm.h.2.mlp.c_fc.weight\n","lm.h.2.mlp.c_fc.bias\n","lm.h.2.mlp.c_proj.weight\n","lm.h.2.mlp.c_proj.bias\n","lm.h.3.ln_1.weight\n","lm.h.3.ln_1.bias\n","lm.h.3.attn.c_attn.weight\n","lm.h.3.attn.c_attn.bias\n","lm.h.3.attn.c_proj.weight\n","lm.h.3.attn.c_proj.bias\n","lm.h.3.ln_2.weight\n","lm.h.3.ln_2.bias\n","lm.h.3.mlp.c_fc.weight\n","lm.h.3.mlp.c_fc.bias\n","lm.h.3.mlp.c_proj.weight\n","lm.h.3.mlp.c_proj.bias\n","lm.h.4.ln_1.weight\n","lm.h.4.ln_1.bias\n","lm.h.4.attn.c_attn.weight\n","lm.h.4.attn.c_attn.bias\n","lm.h.4.attn.c_proj.weight\n","lm.h.4.attn.c_proj.bias\n","lm.h.4.ln_2.weight\n","lm.h.4.ln_2.bias\n","lm.h.4.mlp.c_fc.weight\n","lm.h.4.mlp.c_fc.bias\n","lm.h.4.mlp.c_proj.weight\n","lm.h.4.mlp.c_proj.bias\n","lm.h.5.ln_1.weight\n","lm.h.5.ln_1.bias\n","lm.h.5.attn.c_attn.weight\n","lm.h.5.attn.c_attn.bias\n","lm.h.5.attn.c_proj.weight\n","lm.h.5.attn.c_proj.bias\n","lm.h.5.ln_2.weight\n","lm.h.5.ln_2.bias\n","lm.h.5.mlp.c_fc.weight\n","lm.h.5.mlp.c_fc.bias\n","lm.h.5.mlp.c_proj.weight\n","lm.h.5.mlp.c_proj.bias\n","lm.h.6.ln_1.weight\n","lm.h.6.ln_1.bias\n","lm.h.6.attn.c_attn.weight\n","lm.h.6.attn.c_attn.bias\n","lm.h.6.attn.c_proj.weight\n","lm.h.6.attn.c_proj.bias\n","lm.h.6.ln_2.weight\n","lm.h.6.ln_2.bias\n","lm.h.6.mlp.c_fc.weight\n","lm.h.6.mlp.c_fc.bias\n","lm.h.6.mlp.c_proj.weight\n","lm.h.6.mlp.c_proj.bias\n","lm.h.7.ln_1.weight\n","lm.h.7.ln_1.bias\n","lm.h.7.attn.c_attn.weight\n","lm.h.7.attn.c_attn.bias\n","lm.h.7.attn.c_proj.weight\n","lm.h.7.attn.c_proj.bias\n","lm.h.7.ln_2.weight\n","lm.h.7.ln_2.bias\n","lm.h.7.mlp.c_fc.weight\n","lm.h.7.mlp.c_fc.bias\n","lm.h.7.mlp.c_proj.weight\n","lm.h.7.mlp.c_proj.bias\n","lm.h.8.ln_1.weight\n","lm.h.8.ln_1.bias\n","lm.h.8.attn.c_attn.weight\n","lm.h.8.attn.c_attn.bias\n","lm.h.8.attn.c_proj.weight\n","lm.h.8.attn.c_proj.bias\n","lm.h.8.ln_2.weight\n","lm.h.8.ln_2.bias\n","lm.h.8.mlp.c_fc.weight\n","lm.h.8.mlp.c_fc.bias\n","lm.h.8.mlp.c_proj.weight\n","lm.h.8.mlp.c_proj.bias\n","lm.h.9.ln_1.weight\n","lm.h.9.ln_1.bias\n","lm.h.9.attn.c_attn.weight\n","lm.h.9.attn.c_attn.bias\n","lm.h.9.attn.c_proj.weight\n","lm.h.9.attn.c_proj.bias\n","lm.h.9.ln_2.weight\n","lm.h.9.ln_2.bias\n","lm.h.9.mlp.c_fc.weight\n","lm.h.9.mlp.c_fc.bias\n","lm.h.9.mlp.c_proj.weight\n","lm.h.9.mlp.c_proj.bias\n","lm.h.10.ln_1.weight\n","lm.h.10.ln_1.bias\n","lm.h.10.attn.c_attn.weight\n","lm.h.10.attn.c_attn.bias\n","lm.h.10.attn.c_proj.weight\n","lm.h.10.attn.c_proj.bias\n","lm.h.10.ln_2.weight\n","lm.h.10.ln_2.bias\n","lm.h.10.mlp.c_fc.weight\n","lm.h.10.mlp.c_fc.bias\n","lm.h.10.mlp.c_proj.weight\n","lm.h.10.mlp.c_proj.bias\n","lm.h.11.ln_1.weight\n","lm.h.11.ln_1.bias\n","lm.h.11.attn.c_attn.weight\n","lm.h.11.attn.c_attn.bias\n","lm.h.11.attn.c_proj.weight\n","lm.h.11.attn.c_proj.bias\n","lm.h.11.ln_2.weight\n","lm.h.11.ln_2.bias\n","lm.h.11.mlp.c_fc.weight\n","lm.h.11.mlp.c_fc.bias\n","lm.h.11.mlp.c_proj.weight\n","lm.h.11.mlp.c_proj.bias\n","lm.ln_f.weight\n","lm.ln_f.bias\n","linear_2.weight\n","linear_2.bias\n"]}]},{"cell_type":"markdown","source":["Let's set up the loss function and the optimizer as before. We want to use the binary cross-entropy and the Adam optimizer (usually, you want to pick Adam or AdamW)."],"metadata":{"id":"6lMTx6IUMFyv"}},{"cell_type":"code","source":["loss_fn = torch.nn.BCELoss()\n","adam_optimizer = torch.optim.Adam(my_text_classification_network.parameters(), lr=0.0001)"],"metadata":{"id":"Zer3mvWGK89C","executionInfo":{"status":"ok","timestamp":1747161617412,"user_tz":420,"elapsed":1,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["Let's test whether the loss function works for our labels and model outputs. Not that we may have to adjust the format of the labels."],"metadata":{"id":"wA2F_LP-5Wqe"}},{"cell_type":"code","source":["sample_example = next(iter(train_texts))\n","sample_input = sample_example['input']\n","sample_labels = sample_example['label']\n","\n","sample_labels"],"metadata":{"id":"CA7L35fL1E6d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617422,"user_tz":420,"elapsed":9,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0bc6c96a-7e08-4209-b561-24728ffc1db0"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.],\n","       device='cuda:0')"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["sample_output = my_text_classification_network(sample_input)#.to(device)\n","\n","loss_fn(sample_output, sample_labels)"],"metadata":{"id":"9vIPiwVt0bW0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617511,"user_tz":420,"elapsed":88,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"d64952ff-fb3d-44b0-e359-31be2ce316aa"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.6976, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["sample_output"],"metadata":{"id":"rzYOt2-N0nlV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617520,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0952fb5a-47b3-4ea4-d199-99b3035e6bf2"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.8674, 0.5647, 0.9826, 0.9856, 0.9842, 0.9855, 0.9668, 0.9878, 0.8838,\n","        0.0588, 0.9953, 0.9780, 0.9067, 0.9847, 0.9997, 0.3837],\n","       device='cuda:0', grad_fn=<SigmoidBackward0>)"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["Is this correct? Let's see. We need to get the labels and then calculate manually."],"metadata":{"id":"X5_SdeMi5vdl"}},{"cell_type":"code","source":["print('Labels: ', sample_labels)\n","print('Sample Output: ', sample_output)\n"],"metadata":{"id":"2jB2Uqlg5tw0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617557,"user_tz":420,"elapsed":33,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"5dc391f6-253f-4961-c6fe-94a91518f544"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Labels:  tensor([0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.],\n","       device='cuda:0')\n","Sample Output:  tensor([0.8674, 0.5647, 0.9826, 0.9856, 0.9842, 0.9855, 0.9668, 0.9878, 0.8838,\n","        0.0588, 0.9953, 0.9780, 0.9067, 0.9847, 0.9997, 0.3837],\n","       device='cuda:0', grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Manual binary cross entropy discussion:  1.6976345\n"]}],"source":["sample_output_list = list(sample_output.cpu().detach().numpy())\n","sample_labels_list = list(sample_labels.cpu().detach().numpy())\n","\n","loss = 0\n","\n","for (label, positive_class_prob) in zip(sample_labels_list, sample_output_list):\n","  if label == 0:\n","    loss -= np.log(1 - positive_class_prob)\n","  else:\n","    loss -= np.log(positive_class_prob)\n","\n","average_loss = loss/len(sample_labels_list)\n","\n","\n","\n","print('Manual binary cross entropy discussion: ', average_loss)"],"metadata":{"id":"8338y5Wow_jx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161617557,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"f5ea394c-e2e5-4c2b-cf31-c517b290785a"}},{"cell_type":"markdown","source":["Great, the manual calculation agrees!\n","\n","Off to write the loops:"],"metadata":{"collapsed":false,"id":"ipVv7255w_jy"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=25, steps=None):\n","    # size = len(dataloader.dataset)\n","    # Set the model to training mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    epoch_loss = 0\n","    model.train()\n","\n","    for batch, example in enumerate(dataloader):\n","\n","      X =  example['input']\n","      y = example['label']\n","\n","      if steps is not None:\n","        if batch > steps:\n","          break\n","\n","      optimizer.zero_grad()     # the gradients need to be zeroed out after the gradients are applied by the optimizer\n","\n","      # Compute prediction and loss\n","\n","      pred = model(X)\n","\n","      loss = loss_fn(pred, y)\n","\n","      # Backpropagation\n","      loss.backward()\n","      optimizer.step()\n","\n","      epoch_loss += loss.item()\n","\n","      if int(batch + 1) % reporting_interval == 0:\n","        print('\\tFinished batches: ', str(batch + 1))\n","        print('\\tCurrent average loss: ', epoch_loss/batch)\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n","\n","\n","def test_loop(dataloader, model, loss_fn, reporting_interval=100, steps=None):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for batch, example in enumerate(dataloader):\n","\n","          X =  example['input']\n","          y = example['label']\n","\n","          if steps is not None:\n","            if int(batch) > steps:\n","              break\n","\n","          pred = model(X)\n","          test_loss += loss_fn(pred, y).item()\n","          predictions = [int(x > 0.5) for x in list(pred)]\n","          labels = [int(label > 0.5) for label in list(y)]\n","          correct += np.sum([x == y for (x, y) in zip(predictions, labels)])\n","          total += np.sum([1 for _ in predictions])\n","\n","          if int(batch + 1) % reporting_interval == 0:\n","            print('Accuracy after', str(batch + 1), 'batches:', str(correct/total))\n","\n","    test_loss /= batch\n","    correct /= total\n","    print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"gjNAFq5pMtww","executionInfo":{"status":"ok","timestamp":1747161617562,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["#my_text_classification_network = my_text_classification_network.to(device)\n","\n","epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_texts, my_text_classification_network, loss_fn, adam_optimizer, steps=500)\n","    test_loop(test_texts, my_text_classification_network, loss_fn, steps=125) # no optimizer use here!\n","print(\"Done!\")"],"metadata":{"id":"MkwTXQo-NgHc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161787825,"user_tz":420,"elapsed":170266,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ea6be084-ac48-4cdc-f137-341af1572418"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","\tFinished batches:  25\n","\tCurrent average loss:  0.9133863424261411\n","\tFinished batches:  50\n","\tCurrent average loss:  0.8098145139460661\n","\tFinished batches:  75\n","\tCurrent average loss:  0.7534962062900131\n","\tFinished batches:  100\n","\tCurrent average loss:  0.7151649293273387\n","\tFinished batches:  125\n","\tCurrent average loss:  0.6864801807509314\n","\tFinished batches:  150\n","\tCurrent average loss:  0.6516119676748379\n","\tFinished batches:  175\n","\tCurrent average loss:  0.6188156107205084\n","\tFinished batches:  200\n","\tCurrent average loss:  0.5965864139435878\n","\tFinished batches:  225\n","\tCurrent average loss:  0.5819241707213223\n","\tFinished batches:  250\n","\tCurrent average loss:  0.5690490765025816\n","\tFinished batches:  275\n","\tCurrent average loss:  0.5601985939118984\n","\tFinished batches:  300\n","\tCurrent average loss:  0.5446470992571135\n","\tFinished batches:  325\n","\tCurrent average loss:  0.5380626094791993\n","\tFinished batches:  350\n","\tCurrent average loss:  0.5301465981968153\n","\tFinished batches:  375\n","\tCurrent average loss:  0.5202388201446776\n","\tFinished batches:  400\n","\tCurrent average loss:  0.5141414319326106\n","\tFinished batches:  425\n","\tCurrent average loss:  0.5065540819130135\n","\tFinished batches:  450\n","\tCurrent average loss:  0.5017425579356988\n","\tFinished batches:  475\n","\tCurrent average loss:  0.4968184211491784\n","\tFinished batches:  500\n","\tCurrent average loss:  0.49137342682403173\n","Training Results: \n","  Avg train loss: 0.490248 \n","\n","Accuracy after 100 batches: 0.8275\n","Test Results: \n"," Test Accuracy: 83.1%, Avg test loss: 0.379084 \n","\n","Done!\n"]}]},{"cell_type":"markdown","source":["This looks ok. Maybe using a prompt would be better in order to prime the model for the task? Let's revisit in Assignment II.\n","\n","Lastly, did the underlying GPT2 model get updated too? Let us look again at the same values as before:\n"],"metadata":{"collapsed":false,"id":"UyUulgoDuf0_"}},{"cell_type":"code","source":["post_train_param_values = []\n","\n","print(f\"Model structure: {my_text_classification_network}\\n\\n\")\n","\n","layer_count = 0\n","\n","num_layer_names_shown = 100000\n","\n","\n","for layer_num, (name, param) in enumerate(my_text_classification_network.named_parameters()):\n","  post_train_param_values.append(param.cpu().detach().numpy())\n","\n","\n","post_train_param_values[15][:15]"],"metadata":{"id":"EO4iOP3VF72e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161788221,"user_tz":420,"elapsed":394,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"19295b55-64c4-44ce-b8ca-0acea94b5e5f"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: MyTextClassificationNetworkClass(\n","  (lm): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (linear_2): Linear(in_features=768, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-0.00504095, -0.00651887, -0.00320126, -0.00186698, -0.01149514,\n","       -0.00528767, -0.02683423,  0.04433359, -0.01597745, -0.00354672,\n","       -0.0031457 , -0.00185023, -0.00850381, -0.01216451, -0.01116965],\n","      dtype=float32)"]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["This compares to these values before Fine-Tuning:"],"metadata":{"id":"A3iSQGxkhkCG"}},{"cell_type":"code","source":["pre_train_param_values[15][:15]"],"metadata":{"id":"TjJlnq4phjKW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747161788228,"user_tz":420,"elapsed":5,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"acd0c8d0-5475-4a6c-b5d4-f82006e33911"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.00362932, -0.00622698, -0.00362774, -0.0011091 , -0.01039783,\n","       -0.00634792, -0.02703804,  0.04477038, -0.01744087, -0.00305082,\n","       -0.00317421, -0.00448913, -0.00828817, -0.01425266, -0.01139623],\n","      dtype=float32)"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["So clearly, the values have changes, so the underlying GPT2 model weights were further fine-tuned as well, not just the linear layer!"],"metadata":{"id":"zM5-7U7chp5K"}},{"cell_type":"code","source":[],"metadata":{"id":"1xWObGDptqqa","executionInfo":{"status":"ok","timestamp":1747161788230,"user_tz":420,"elapsed":1,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":46,"outputs":[]}]}