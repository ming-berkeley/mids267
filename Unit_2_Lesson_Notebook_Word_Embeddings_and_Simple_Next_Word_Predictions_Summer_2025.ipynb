{"cells":[{"cell_type":"markdown","metadata":{"id":"d6BkSj_fpP2_"},"source":["# Week 2 Lesson Notebook: Word2Vec_Embeddings & GPT-2 Predictions\n","\n","In this notebook, we play with some classic word embeddings (using Word2Vec) and then use an old Language Model, GPT-2, to make a few next-word predictions. The purpose is start building up some intuition for the entities and concepts we are working with.  We use \"embedding\" vectors to represent the words in language as we process them in neural networks.  Embeddings are a fuzzy representation of words.  We use decoder transformers to predict the next word based on the previous sequence of words.  We'll see the mechanics of feeding a sequence of words into a transformer to predict the next word.  We'll use this process through out the rest of the class.<br>\n","\n","**Note:** In this and other lesson notebooks we will also pose questions for you to think about and solve, if you are interested. Look for '**Additional Question**'."]},{"cell_type":"markdown","metadata":{"id":"FfXJPARLpP3C"},"source":["## 1. Setup\n","\n","This notebook requires the tensorflow dataset and other prerequisites that you must download and then store locally."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ywohLRs0pP3D","executionInfo":{"status":"ok","timestamp":1747280174378,"user_tz":420,"elapsed":13393,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["!pip install gensim --quiet\n","!pip install pydot --quiet"]},{"cell_type":"markdown","metadata":{"id":"aWwM6biepP3F"},"source":["Ready to do the imports."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"o6D-WpktpP3F","executionInfo":{"status":"ok","timestamp":1747280182871,"user_tz":420,"elapsed":8489,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["import sklearn as sk\n","import os\n","import nltk\n","from nltk.corpus import reuters\n","from nltk.data import find\n","\n","import matplotlib.pyplot as plt\n","\n","import re\n","\n","import gensim\n","\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"w_0YP3-rpP3F"},"source":["Below is a helper function for similarity evaluation:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Bmk2otpopP3G","executionInfo":{"status":"ok","timestamp":1747280182875,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["# We are using cosine similarity\n","\n","def cos_sim(a, b):\n","\n","    \"\"\"\n","    Computes the cosine similarity\n","    \"\"\"\n","    dot_product = np.dot(a, b)\n","    norm_a = np.linalg.norm(a)\n","    norm_b = np.linalg.norm(b)\n","    return dot_product / (norm_a * norm_b)\n"]},{"cell_type":"markdown","metadata":{"id":"1z9OBwNapP3G"},"source":["## 2. Word Embeddings\n","\n","Next, we get the word2vec model from nltk."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vv4toN1RpP3H","executionInfo":{"status":"aborted","timestamp":1747269148161,"user_tz":420,"elapsed":39693,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["nltk.download('word2vec_sample')\n","\n","word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n","model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"]},{"cell_type":"markdown","metadata":{"id":"5twlOMzcpP3H"},"source":["How many words are in the vocabulary?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Mel3V35pP3H","executionInfo":{"status":"aborted","timestamp":1747269148185,"user_tz":420,"elapsed":39716,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["len(model.key_to_index)"]},{"cell_type":"markdown","metadata":{"id":"T4BohyyLpP3I"},"source":["How do the word vectors look like? As expected:"]},{"cell_type":"code","source":["model['school']"],"metadata":{"id":"TgnfU_zYKP4n","executionInfo":{"status":"aborted","timestamp":1747269148185,"user_tz":420,"elapsed":39715,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's vectorize at a few words and look at the cosine similarities:"],"metadata":{"id":"nBmLxQh_KQjk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFf5ICwCpP3I","executionInfo":{"status":"aborted","timestamp":1747269148186,"user_tz":420,"elapsed":39715,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["vec_car = model['car']\n","vec_vehicle = model['vehicle']\n","vec_school = model['school']"]},{"cell_type":"code","source":["cos_sim(vec_car, vec_school)"],"metadata":{"id":"KxogZtbKqtBK","executionInfo":{"status":"aborted","timestamp":1747269148186,"user_tz":420,"elapsed":39714,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cos_sim(vec_car, vec_vehicle)"],"metadata":{"id":"TBMBuNDkqvrp","executionInfo":{"status":"aborted","timestamp":1747269148186,"user_tz":420,"elapsed":39713,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cos_sim(vec_school, vec_vehicle)"],"metadata":{"id":"67ko6BLvqyAx","executionInfo":{"status":"aborted","timestamp":1747269148187,"user_tz":420,"elapsed":39713,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chUpj0ZfpP3I"},"source":["Let's play with a few more examples..."]},{"cell_type":"code","source":["vec_related = model['automotive']\n","cos_sim(vec_car, vec_related)"],"metadata":{"id":"EJDP-saQ9VyV","executionInfo":{"status":"aborted","timestamp":1747269148187,"user_tz":420,"elapsed":39712,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vec_unrelated = model['aardvark']\n","cos_sim(vec_car, vec_unrelated)"],"metadata":{"id":"gTYB4yZZ9tDj","executionInfo":{"status":"aborted","timestamp":1747269148187,"user_tz":420,"elapsed":39711,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Oops! Out of vocabulary used to be a real issue for classic word embeddings.\n","\n","**Additional Question 1:** Can you verify that the word vectors represent interesting syntactic and semantic relationships well, like '*run* is to *running* as *swim* is *swimming*'. How could you approach that? (Hint: conceptually, 'ing' ~ 'running' - 'run )."],"metadata":{"id":"8TWGmMfcoGN8"}},{"cell_type":"markdown","metadata":{"id":"xkOE7aDxpP3I"},"source":["## 3. Simple Next-Word Predictions with GPT-2\n","\n","We will now download the GPT2 model from Huggingface and use it to get a feeling for these next-word predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVEjB6wxpP3I","executionInfo":{"status":"aborted","timestamp":1747269148188,"user_tz":420,"elapsed":39712,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["#!pip install transformers  --quiet"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, GPT2LMHeadModel"],"metadata":{"id":"8bhsG6JGrpnD","executionInfo":{"status":"aborted","timestamp":1747269148188,"user_tz":420,"elapsed":39711,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","model = GPT2LMHeadModel.from_pretrained('gpt2')"],"metadata":{"id":"prAklH0Mrjrc","executionInfo":{"status":"aborted","timestamp":1747269148188,"user_tz":420,"elapsed":39710,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model requires tokenized input. I.e., each word is split into tokens (one word can be comprised of one or more tokens) and the token id is used as the input to the model:"],"metadata":{"id":"WOBlqBMlKwUp"}},{"cell_type":"code","source":["inputs = tokenizer(\"Today is a very nice\", return_tensors=\"pt\")"],"metadata":{"id":"cnGHCGVdrjuA","executionInfo":{"status":"aborted","timestamp":1747269148188,"user_tz":420,"elapsed":39709,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs"],"metadata":{"id":"g3AdNQCRrjwx","executionInfo":{"status":"aborted","timestamp":1747269148189,"user_tz":420,"elapsed":39709,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We see the five input ids and the corresponding 'attention_masks' (~'should' the model pay attention to the position?').\n","\n","Now we apply the model to the input:"],"metadata":{"id":"D3_81V1CLUF5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"35d03TETpP3I","executionInfo":{"status":"aborted","timestamp":1747269148207,"user_tz":420,"elapsed":39726,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"outputs":[],"source":["output = model(**inputs)"]},{"cell_type":"code","source":["len(output)"],"metadata":{"id":"yrHvPnvzMCR2","executionInfo":{"status":"aborted","timestamp":1747269148218,"user_tz":420,"elapsed":39737,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Why '2'? The [Huggingface documentation ](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2LMHeadModel)is very helpful."],"metadata":{"id":"FRYSdQvLzLcO"}},{"cell_type":"code","source":["output.keys()"],"metadata":{"id":"15IwU4zBzfe0","executionInfo":{"status":"aborted","timestamp":1747269148219,"user_tz":420,"elapsed":39737,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output.logits.shape"],"metadata":{"id":"wCSspZLkt3yx","executionInfo":{"status":"aborted","timestamp":1747269148220,"user_tz":420,"elapsed":39737,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What could be the meaning of these dimensions?\n","\n","Ok, let's the positions by the logits:"],"metadata":{"id":"ZxS8n509uIk_"}},{"cell_type":"code","source":["logits_last_position = (output.logits.detach()[0, -1])\n","np.argsort(logits_last_position)"],"metadata":{"id":"dlzB7dkyt32I","executionInfo":{"status":"aborted","timestamp":1747269148220,"user_tz":420,"elapsed":39736,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the token corresponding to the highest logit?"],"metadata":{"id":"Za0Nxgfez4pr"}},{"cell_type":"code","source":["tokenizer.decode([1110])"],"metadata":{"id":"hBFWX4qYu2Va","executionInfo":{"status":"aborted","timestamp":1747269148220,"user_tz":420,"elapsed":39735,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Does this look right? It does...\n","\n","What are the corresponding *relative* probabilities of the 2 most common words?"],"metadata":{"id":"hWJHdvwzvN13"}},{"cell_type":"code","source":["np.exp(logits_last_position[1110])/ np.exp(logits_last_position[640])"],"metadata":{"id":"8bouDY8JvCnm","executionInfo":{"status":"aborted","timestamp":1747269148221,"user_tz":420,"elapsed":39735,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Substantially more likely to pick token 1.  What was token 2?"],"metadata":{"id":"P3nD0Un206IT"}},{"cell_type":"code","source":["tokenizer.decode([640])"],"metadata":{"id":"1sdkTRgG1DTK","executionInfo":{"status":"aborted","timestamp":1747269148221,"user_tz":420,"elapsed":39734,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["'Today is a very nice **day**' vs 'Today is a very nice **time**'. Makes sense...\n","\n","**Additional Question 2:** How could you possibly use a language model to determine whether 'This was fun' has *positive* or *negative* sentiment? (Note, GPT-2 isn't that great to say the least, but the principle is instructive.)\n"],"metadata":{"id":"FI4HqsMT1NTo"}},{"cell_type":"code","source":[],"metadata":{"id":"yOmzkoqCQXuH","executionInfo":{"status":"aborted","timestamp":1747269148221,"user_tz":420,"elapsed":39734,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}