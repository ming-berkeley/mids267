{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f9b44a71195f407b9a36ebbd17195230":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba46f62b58024d7984d333bc88dc8eba","IPY_MODEL_17f2ea348f6544feb8e3b5d1739f3e15","IPY_MODEL_bc3c4a6b6a76464a89dfac0477b0a30f"],"layout":"IPY_MODEL_b1641972b0d74cff943db94a3d11c655"}},"ba46f62b58024d7984d333bc88dc8eba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0bb97589a614ae59adca0ddf3a4fca5","placeholder":"​","style":"IPY_MODEL_104e7f59c99f4d97802cf7857333c2e2","value":"README.md: 100%"}},"17f2ea348f6544feb8e3b5d1739f3e15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92fb3f902e0b43a18e6081f352a0febb","max":7809,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97e6c1aeb02a40119a2a2a356070b846","value":7809}},"bc3c4a6b6a76464a89dfac0477b0a30f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f98640ecdc54b8c8bc8c28d2709f28f","placeholder":"​","style":"IPY_MODEL_6410341ad74f4f5baeb53a81b951b4a9","value":" 7.81k/7.81k [00:00&lt;00:00, 222kB/s]"}},"b1641972b0d74cff943db94a3d11c655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bb97589a614ae59adca0ddf3a4fca5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"104e7f59c99f4d97802cf7857333c2e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92fb3f902e0b43a18e6081f352a0febb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97e6c1aeb02a40119a2a2a356070b846":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f98640ecdc54b8c8bc8c28d2709f28f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6410341ad74f4f5baeb53a81b951b4a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cef64ef529bf4ee88919c74150d17a7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d280118bc6e24ae19a9c741571c47cf6","IPY_MODEL_12838c834a344540aac5521f1632ed8b","IPY_MODEL_7819ba7cb1b94d3b8374b8dbf2c13b90"],"layout":"IPY_MODEL_e079688916a04c2c8da32c17826d58ad"}},"d280118bc6e24ae19a9c741571c47cf6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d113e0c75ea14638bca419a317116f7a","placeholder":"​","style":"IPY_MODEL_8249a52c30a848fabd7283db137fb643","value":"train-00000-of-00001.parquet: 100%"}},"12838c834a344540aac5521f1632ed8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d226615006d44e789fa764ad26e901c","max":20979968,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc8ce402c06c43a0919e1a65a5addff8","value":20979968}},"7819ba7cb1b94d3b8374b8dbf2c13b90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebce42553a36417ca81230776a6ceca0","placeholder":"​","style":"IPY_MODEL_e7a5fbe84f9c42fabcf78b50d3ab1406","value":" 21.0M/21.0M [00:00&lt;00:00, 193MB/s]"}},"e079688916a04c2c8da32c17826d58ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d113e0c75ea14638bca419a317116f7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8249a52c30a848fabd7283db137fb643":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d226615006d44e789fa764ad26e901c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc8ce402c06c43a0919e1a65a5addff8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebce42553a36417ca81230776a6ceca0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7a5fbe84f9c42fabcf78b50d3ab1406":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d0f96872c2446e88c1f760aa6b30421":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b6f7000bfa442c680d777c709cbc0c1","IPY_MODEL_19f39db4559c46a4baa217cd779d18e6","IPY_MODEL_0c9c295f981a40879f1c144b05c1c6da"],"layout":"IPY_MODEL_c45eb5e078da4da989e81cf89c8d9c1e"}},"5b6f7000bfa442c680d777c709cbc0c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f0cc26f2b774a278d17a3f8aadad63f","placeholder":"​","style":"IPY_MODEL_5d31a32a91764543abdd5ed265486da1","value":"test-00000-of-00001.parquet: 100%"}},"19f39db4559c46a4baa217cd779d18e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efaa832c1fc54583878464cc2b1e6fec","max":20470363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_100751411a45471ea04ab5ae47de72c2","value":20470363}},"0c9c295f981a40879f1c144b05c1c6da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8713d7752f7a414cad398d3851a60642","placeholder":"​","style":"IPY_MODEL_d9dcfefd4bb547738bf595d48973a3db","value":" 20.5M/20.5M [00:00&lt;00:00, 291MB/s]"}},"c45eb5e078da4da989e81cf89c8d9c1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f0cc26f2b774a278d17a3f8aadad63f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d31a32a91764543abdd5ed265486da1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efaa832c1fc54583878464cc2b1e6fec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100751411a45471ea04ab5ae47de72c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8713d7752f7a414cad398d3851a60642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9dcfefd4bb547738bf595d48973a3db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb0e46407d68457f909aaf85ae1dc47b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50e9ecd72c9e4d6db950bb5ee620385d","IPY_MODEL_b280932221c34457ab6d02aa8b58a8c7","IPY_MODEL_7170d9a0cb824b8d8d1a8096d35e5650"],"layout":"IPY_MODEL_f16a8762a8c44ca0b580613668e3b7eb"}},"50e9ecd72c9e4d6db950bb5ee620385d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_201ea24fd6144195a886efa958b4d0e2","placeholder":"​","style":"IPY_MODEL_91a029af54ee4026abfa1de6b4eaae8e","value":"unsupervised-00000-of-00001.parquet: 100%"}},"b280932221c34457ab6d02aa8b58a8c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1e71a04d8344258a19359c300f4b9ae","max":41996509,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eebe7cb10cb40358d89495024c28aa4","value":41996509}},"7170d9a0cb824b8d8d1a8096d35e5650":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_665136d16bd7442994ec27776e78480c","placeholder":"​","style":"IPY_MODEL_564afb6c69ad4710983da17a7a2f175e","value":" 42.0M/42.0M [00:00&lt;00:00, 412MB/s]"}},"f16a8762a8c44ca0b580613668e3b7eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"201ea24fd6144195a886efa958b4d0e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91a029af54ee4026abfa1de6b4eaae8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1e71a04d8344258a19359c300f4b9ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eebe7cb10cb40358d89495024c28aa4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"665136d16bd7442994ec27776e78480c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"564afb6c69ad4710983da17a7a2f175e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e40e019be8c432290db88d0cdb8c2f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e5a1df990d214e31a601b908ba027ba4","IPY_MODEL_962d7a06ff8244429bc9b8da91a407a9","IPY_MODEL_edad17081b25409f8feb331adf489a47"],"layout":"IPY_MODEL_f32b4b7674dc4f7291f0ef627e106744"}},"e5a1df990d214e31a601b908ba027ba4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b321291ae88b4210b50ae71d5833f647","placeholder":"​","style":"IPY_MODEL_a06ebdac112c4106afacf523ec705b0f","value":"Generating train split: 100%"}},"962d7a06ff8244429bc9b8da91a407a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e430f2609f4143588f229e0a2a835844","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b06d5f8225f041a180e5eba982a8a302","value":25000}},"edad17081b25409f8feb331adf489a47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f78602423b54bf4b295c18aa652cd42","placeholder":"​","style":"IPY_MODEL_9a20283b5df94a90b7e99113b835d957","value":" 25000/25000 [00:00&lt;00:00, 62611.98 examples/s]"}},"f32b4b7674dc4f7291f0ef627e106744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b321291ae88b4210b50ae71d5833f647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a06ebdac112c4106afacf523ec705b0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e430f2609f4143588f229e0a2a835844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b06d5f8225f041a180e5eba982a8a302":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f78602423b54bf4b295c18aa652cd42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a20283b5df94a90b7e99113b835d957":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aeb2b0e6434e46cb8f7a7a24d4f3ffef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f15d23b36b243d1b65e8bb34e43f984","IPY_MODEL_b2ee94596f4a41eb957b2e54551277a4","IPY_MODEL_7c26702a83b741089a919476afaa272b"],"layout":"IPY_MODEL_f9f82d976eb84a5aa823f98f1c97d16f"}},"7f15d23b36b243d1b65e8bb34e43f984":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4e5139be4c4e64baf590e5d889e515","placeholder":"​","style":"IPY_MODEL_6cbd003fc7564a95a83a9015d1c0e5ab","value":"Generating test split: 100%"}},"b2ee94596f4a41eb957b2e54551277a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_553371e35cf24cc39145d840fa44d8ce","max":25000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a9ecc1b14dd4a1ea3bf69c7aa5e1ecc","value":25000}},"7c26702a83b741089a919476afaa272b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_deff82b521454906bcde6d4237930435","placeholder":"​","style":"IPY_MODEL_d2f3fadeaf3644a085b120e61437c272","value":" 25000/25000 [00:00&lt;00:00, 58059.91 examples/s]"}},"f9f82d976eb84a5aa823f98f1c97d16f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4e5139be4c4e64baf590e5d889e515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cbd003fc7564a95a83a9015d1c0e5ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"553371e35cf24cc39145d840fa44d8ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a9ecc1b14dd4a1ea3bf69c7aa5e1ecc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"deff82b521454906bcde6d4237930435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2f3fadeaf3644a085b120e61437c272":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3641f7142f1426da80cda02891ad0d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e1b949cdf3345ca982af94411f45622","IPY_MODEL_b20e3f69a95d4cadb47cd5d76329df61","IPY_MODEL_69d43ee961ee4c8f992cbb7581b46c5b"],"layout":"IPY_MODEL_a2301c6c791d4d3d9b10879492704cbc"}},"6e1b949cdf3345ca982af94411f45622":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f286bd7b9dcb419c87cfa1bcae54641a","placeholder":"​","style":"IPY_MODEL_cc4cf9b57fa848739dfcff21be1e57f3","value":"Generating unsupervised split: 100%"}},"b20e3f69a95d4cadb47cd5d76329df61":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b99e99cc7d5749a7b89dbf1eea68d17e","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0ab63b5118848f79c99facd6e11245a","value":50000}},"69d43ee961ee4c8f992cbb7581b46c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0945baeb75cb44d39d4c3e567ecc0ebb","placeholder":"​","style":"IPY_MODEL_15fbb903231442e18fbe167720140470","value":" 50000/50000 [00:00&lt;00:00, 86447.60 examples/s]"}},"a2301c6c791d4d3d9b10879492704cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f286bd7b9dcb419c87cfa1bcae54641a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc4cf9b57fa848739dfcff21be1e57f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b99e99cc7d5749a7b89dbf1eea68d17e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0ab63b5118848f79c99facd6e11245a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0945baeb75cb44d39d4c3e567ecc0ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15fbb903231442e18fbe167720140470":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### Lesson Notebook Week 3 - From Basics to Sentiment Classification with GPT-2\n","\n","This notebook is a subset of the material for this week's special material session. It requires a T4 GPU.\n","\n","\n","\n","Let's again start with a few installs and imports:\n"],"metadata":{"id":"9hZNSDOBJcBm"}},{"cell_type":"markdown","source":["####0. Prep Work (runs for 2 min... plan for it!)\n","\n","These installs do not have do be done in a Google Colab (Pro):"],"metadata":{"id":"Iig7ABWwqCfk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuN074G-JK_T"},"outputs":[],"source":["%%capture\n","\n","!pip install torch\n","!pip install torchtext\n","!pip install transformers   # for our application example in the end\n","!pip install numpy\n"]},{"cell_type":"markdown","source":["These will need to be done:"],"metadata":{"id":"gBrTFoTALCS_"}},{"cell_type":"code","source":["%%capture\n","\n","!pip install portalocker\n","!pip install torchdata\n","!pip install -U datasets fsspec huggingface_hub # Hugging Face's dataset library"],"metadata":{"id":"UPEA1rizKy09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","\n","import numpy as np\n","import random\n","\n","from torch.utils.data import Dataset, DataLoader\n","from datasets import load_dataset\n","\n","from transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2LMHeadModel, BertModel, AutoModelForCausalLM\n","from transformers import AutoTokenizer, BertModel\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"8K_UtTimmUwp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584617591,"user_tz":420,"elapsed":12311,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ab385623-8751-4087-e942-21e36fe49f0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def create_temp_set(base_data, num_examples=1000000000):\n","    num_positive = 0\n","    num_negative = 0\n","    num_other = 0\n","\n","    temp_data = []\n","    out_data = []\n","\n","    for example_num, example in enumerate(base_data):\n","\n","      temp_data.append(example)\n","\n","    random.shuffle(temp_data)\n","\n","    for example_num, example in enumerate(temp_data):\n","\n","      if num_examples != -1 and example_num > num_examples:\n","        break\n","\n","      if example['label'] == 0:\n","        num_negative += 1\n","      elif example['label'] == 1:\n","        num_positive += 1\n","      else:\n","        num_other += 1\n","\n","      out_data.append(example)\n","\n","\n","    print('positive: ', num_positive)\n","    print('negative: ', num_negative)\n","    print('other: ', num_other)\n","\n","    return out_data"],"metadata":{"id":"64YSDOIjrdp7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cos_sim(a, b):\n","  return np.dot(a, b)/(np.sqrt(np.dot(a, a) * np.dot(b, b)))"],"metadata":{"id":"AoqVvNsz0wCy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ClassificationData(Dataset):\n","    def __init__(self, base_data, tokenizer, max_len, use_prompt=False):\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","\n","        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n","        for example_num, example in enumerate(base_data):\n","\n","\n","            try:\n","              token_encoder = tokenizer(example['text'])['input_ids']\n","            except:\n","              print(example_num)\n","              break\n","\n","            try:\n","              if len(token_encoder) <= self.max_len:\n","                continue    # avoids complications with short sentences. No padding is needed then.\n","            except:\n","              print(example_num)\n","              print(token_encoder)\n","              print(len(token_encoder))\n","              break\n","\n","            truncated_encoding = token_encoder[:self.max_len]\n","            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n","\n","            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n","\n","            if use_prompt:\n","                cutoff = self.max_len + 13\n","                prompted_text_line = 'Here is a movie review: ' + truncated_example + ' Is this review positive or negative?'\n","\n","            else:\n","                cutoff = self.max_len\n","                prompted_text_line = truncated_example\n","\n","            if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n","                    continue\n","\n","            tokenized_example = self.tokenizer(prompted_text_line,\n","                                               return_tensors=\"pt\",\n","                                               max_length=cutoff,\n","                                               truncation=True,\n","                                               padding='max_length').to(device)\n","\n","            self.data.append({'label': (float(example['label'])),\n","                              'input':\n","                                  {'input_ids': torch.tensor(torch.squeeze(tokenized_example['input_ids']),\n","                                      device=device),\n","                                   'attention_mask': torch.tensor(torch.squeeze(tokenized_example['attention_mask']),\n","                                      device=device)\n","                                   }\n","                              })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input': self.data[index]['input'],\n","            'label': torch.tensor(self.data[index]['label'],\n","                                  dtype=torch.float,\n","                                  device=device)\n","        }"],"metadata":{"id":"gRFKKK8_rdtr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=25, steps=None):\n","    # size = len(dataloader.dataset)\n","    # Set the model to training mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    epoch_loss = 0\n","    model.train()\n","\n","    for batch, example in enumerate(dataloader):\n","\n","      X =  example['input']\n","      y = example['label']\n","\n","      if steps is not None:\n","        if batch > steps:\n","          break\n","\n","\n","      # Compute prediction and loss\n","\n","      pred = model(X)\n","\n","      loss = loss_fn(pred, y)\n","\n","      # Backpropagation\n","      loss.backward()\n","      optimizer.step()\n","      optimizer.zero_grad()     # the gradients need to be zeroed out after the gradients are applied by the optimizer\n","\n","      epoch_loss += loss.item()\n","\n","      if int(batch + 1) % reporting_interval == 0:\n","        print('\\tFinished batches: ', str(batch + 1))\n","        print('\\tCurrent average loss: ', epoch_loss/batch)\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n","\n","\n","def test_loop(dataloader, model, loss_fn, reporting_interval=100, steps=None):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.eval()\n","    test_loss, correct, total = 0, 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for batch, example in enumerate(dataloader):\n","\n","          X =  example['input']\n","          y = example['label']\n","\n","          if steps is not None:\n","            if int(batch) > steps:\n","              break\n","\n","          pred = model(X)\n","          test_loss += loss_fn(pred, y).item()\n","          predictions = [int(x > 0.5) for x in list(pred)]\n","          labels = [int(label > 0.5) for label in list(y)]\n","          correct += np.sum([x == y for (x, y) in zip(predictions, labels)])\n","          total += np.sum([1 for _ in predictions])\n","\n","          if int(batch + 1) % reporting_interval == 0:\n","            print('Accuracy after', str(batch + 1), 'batches:', str(correct/total))\n","\n","    test_loss /= batch\n","    correct /= total\n","    print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"5fr2i-pkTjWl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Pretraining, Loss, and Perplexity\n","\n","We will briefly look at the problem of pre-training, where a model was trained on predicting the next actual word based on the previous ones. We use both, Phi-2 and GPT2 as examples and look at the losses for a given sentence. We will then calculate the perplexity for both models. Note that GPT2 is from 2018 and Phi-2 is from earlier in 2024 ([note, that we do not use the instruction-tuned version of Phi-2. We use the base version that has not undergone instruction-tuning.](https://huggingface.co/microsoft/phi-2))\n","\n"],"metadata":{"id":"SIDK119_pmBd"}},{"cell_type":"code","source":["%%capture\n","\n","phi_2_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n","phi_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","phi_2_lm_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True).to(device)\n","\n","gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", trust_remote_code=True)\n","gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","gpt2_lm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", torch_dtype=\"auto\", trust_remote_code=True).to(device)"],"metadata":{"id":"qERkkoKtk-p2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's construct a short input and tokenize for both models:"],"metadata":{"id":"tdySmS3LqQMn"}},{"cell_type":"code","source":["text = '''This was really a fun event! I would go there certainly again if I get the chance. Do you know when there will be the next show? I can't wait!'''\n","\n","tokenized_input_gpt2 = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n","tokenized_input_phi_2 = phi_2_tokenizer(text, return_tensors=\"pt\").to(device)"],"metadata":{"id":"v05hxG6Ik-wh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_input_gpt2"],"metadata":{"id":"M31Ht_ggMyE3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584631596,"user_tz":420,"elapsed":23,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0acef3d1-d2e0-4d6e-a456-0c66441ea90a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[1212,  373, 1107,  257, 1257, 1785,    0,  314,  561,  467,  612, 3729,\n","          757,  611,  314,  651,  262, 2863,   13, 2141,  345,  760,  618,  612,\n","          481,  307,  262, 1306,  905,   30,  314,  460,  470, 4043,    0]],\n","       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["tokenized_input_phi_2"],"metadata":{"id":"tpjRduE0MyN6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584631614,"user_tz":420,"elapsed":17,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"672372a6-9562-4174-8832-5537f557b5aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[1212,  373, 1107,  257, 1257, 1785,    0,  314,  561,  467,  612, 3729,\n","          757,  611,  314,  651,  262, 2863,   13, 2141,  345,  760,  618,  612,\n","          481,  307,  262, 1306,  905,   30,  314,  460,  470, 4043,    0]],\n","       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**Question:**\n","1. Do these values make sense?\n","2. What are input_ids and attention_mask?   \n","3. The tokenized values appear to be identical for both models. Did that have to be the case?\n","\n","\n","Now we apply both language models like last week, and look at the outputs. Note that here we use the 'LMHeadModel' versions for both models. These are the Transformer parts **plus the next-token-prediction classification head**."],"metadata":{"id":"LnLx7GJuqi71"}},{"cell_type":"code","source":["output_gpt2 = gpt2_lm_model(**tokenized_input_gpt2)\n","output_phi_2 = phi_2_lm_model(**tokenized_input_phi_2)"],"metadata":{"id":"AdLOk252M13j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_gpt2.logits.shape"],"metadata":{"id":"z9kFFPvSI3qh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633012,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"ca5635db-23f4-49dc-97e5-d9662eae54bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 35, 50257])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["output_phi_2.logits.shape"],"metadata":{"id":"K61zhCt-Ngrv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633021,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"7c95fe44-e81f-4e5c-ac36-c57288fefadf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 35, 51200])"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["**Question:**\n","\n","4. Does this look right?  \n","5. The output shapes are not the same for both models. Do they have to be? Why/why not?  \n","\n","\n","We could now look at all of the logits and calculate the loss for how good the models were in predicting the  next token correctly at each position. Or... we simply use a model capabilitities by giving the correct labels and get the loss for us (which is the average loss over each positions).\n","\n","We will do that for a short text and a long text:"],"metadata":{"id":"CRHrWPu8qtl_"}},{"cell_type":"code","source":["text_long = '''This was really a fun event! I would go there certainly again if I get the chance. Do you know when there will be the next show? I can't wait!'''\n","text_short = '''This is'''\n","\n","\n","tokenized_input_gpt2_long = gpt2_tokenizer(text_long, return_tensors=\"pt\").to(device)\n","tokenized_input_phi_2_long = phi_2_tokenizer(text_long, return_tensors=\"pt\").to(device)\n","\n","tokenized_input_gpt2_short = gpt2_tokenizer(text_short, return_tensors=\"pt\").to(device)\n","tokenized_input_phi_2_short = phi_2_tokenizer(text_short, return_tensors=\"pt\").to(device)\n","\n","\n","output_gpt2_long = gpt2_lm_model(**tokenized_input_gpt2_long,\n","                          labels=tokenized_input_gpt2_long['input_ids'])\n","\n","output_phi_2_long = phi_2_lm_model(**tokenized_input_phi_2_long,\n","                          labels=tokenized_input_phi_2_long['input_ids'])\n","\n","output_gpt2_short = gpt2_lm_model(**tokenized_input_gpt2_short,\n","                          labels=tokenized_input_gpt2_short['input_ids'])\n","\n","output_phi_2_short = phi_2_lm_model(**tokenized_input_phi_2_short,\n","                          labels=tokenized_input_phi_2_short['input_ids'])"],"metadata":{"id":"BfVZoWR5nNPi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633479,"user_tz":420,"elapsed":457,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"b8d405d3-9111-431c-ddf5-7f20aef68657"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"G_JIXUIHOo9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we look at the average losses for each model and input:"],"metadata":{"id":"DFY8bqelO_L8"}},{"cell_type":"code","source":["print('GPT2 loss - short input: ', output_gpt2_short.loss)\n","print('Phi-2 loss - short input: ', output_phi_2_short.loss)\n","\n","print()\n","\n","print('GPT2  loss - long input: ', output_gpt2_long.loss)\n","print('Phi-2 loss - long input: ', output_phi_2_long.loss)\n"],"metadata":{"id":"aRKKVtfkorx8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633850,"user_tz":420,"elapsed":367,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"f7b19a61-7809-4240-b604-64fbac6d9a22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT2 loss - short input:  tensor(3.0817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Phi-2 loss - short input:  tensor(1.4561, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","GPT2  loss - long input:  tensor(3.2498, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Phi-2 loss - long input:  tensor(2.8042, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["**Questions:**\n","\n","6. Why are the losses different for the longer inputs?   \n","7. How come the loss for Phi-2 is actually different than the one for GPT2 for the short text? What does that say about in what way Phi-2 is better than GPT2?  "],"metadata":{"id":"X8f_EiLkPd7n"}},{"cell_type":"markdown","source":["What about the corresponding perplexities?"],"metadata":{"id":"dZcCYchsrKk_"}},{"cell_type":"code","source":["print('GPT2 perplexity - short input: ', str(np.exp(output_gpt2_short.loss.cpu().detach().numpy())))\n","print('Phi-2 perplexity - short input: ', np.exp(output_phi_2_short.loss.cpu().detach().numpy()))\n","\n","print()\n","\n","print('GPT2 perplexity - long input: ', np.exp(output_gpt2_long.loss.cpu().detach().numpy()))\n","print('Phi-2 perplexity - long input: ', np.exp(output_phi_2_long.loss.cpu().detach().numpy()))"],"metadata":{"id":"naZrk7YQpDY6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633851,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"846cc1ae-692b-44d3-c752-529b6c21b88c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT2 perplexity - short input:  21.795923\n","Phi-2 perplexity - short input:  4.2890596\n","\n","GPT2 perplexity - short input:  25.78493\n","Phi-2 perplexity - short input:  16.513083\n"]}]},{"cell_type":"markdown","source":["**Questions:**.\n","\n","8. What does this 'mean'? How certain is the model to pick the correct word?  \n","\n","\n","Next, let's see how good/not good these two models are in 'classifying sentiment out-of-the-box' by simply using pre- and post-modifiers:"],"metadata":{"id":"1IcgvP3NraLi"}},{"cell_type":"code","source":["text = '''Here is a sentence: \"This was interesting event, but I did not like all of it.\" Sentences can have positive or negative sentiment only! Pleasde repond only with one of those two words! The sentiment of this sentence is'''\n","\n","\n","\n","\n","gpt2_tokenized_input = gpt2_tokenizer(text, return_tensors=\"pt\").to(device)\n","phi_2_tokenized_input = phi_2_tokenizer(text, return_tensors=\"pt\").to(device)\n","\n","output_gpt2 = gpt2_lm_model(**gpt2_tokenized_input)\n","output_phi_2 = phi_2_lm_model(**phi_2_tokenized_input)\n","\n","last_token_logits_gpt2 = output_gpt2.logits[0, -1, :].cpu().detach().numpy()\n","last_token_logits_phi_2 = output_phi_2.logits[0, -1, :].cpu().detach().numpy()\n","\n","top_predictions_gpt2 = list(np.argsort(last_token_logits_gpt2)[-5:])\n","top_predictions_phi_2 = list(np.argsort(last_token_logits_phi_2)[-5:])\n","\n","\n","print('GPT2 sentiment: \\n\\t', ' '.join([gpt2_tokenizer.decode(x) for x in top_predictions_gpt2]))\n","print()\n","print('Phi-2 sentiment: \\n\\t', ' '.join([gpt2_tokenizer.decode(x) for x in top_predictions_phi_2]))"],"metadata":{"id":"V1nUPbMDVx6s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584633975,"user_tz":420,"elapsed":126,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"5d461e05-9c93-4fe3-e172-854df13a6c31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPT2 sentiment: \n","\t  the  that  \"  not :\n","\n","Phi-2 sentiment: \n","\t  positive \n","  \"  negative  neutral\n"]}]},{"cell_type":"markdown","source":["Clearly, Phi-2 is much better than GPT2, as one should expect! (Note that there appears to be a 'return' predicted for Phi-2)."],"metadata":{"id":"_0hF_nkOZr0z"}},{"cell_type":"markdown","source":["\n","### 2.Fine-Tuning - Sentiment Classification with GPT-2 - Option A: Straight Last Token Prediction Classification\n","\n","We will now use the idea of Sentence Embeddings for our Sentiment Classification problem. We will however use the GPT-2 model from HuggingFace (provided by OpenAI). This gives us an opportunity to also look at a generative AI model explicitly. Note that here we use the base 'Model' versions for both models. These are the Transformer parts **WITHOUT the next-token-prediction classification head**.\n","\n","#### a. Downloading and working with GPT-2: Tokenizer & Model\n","\n","We will first get the model, see here: https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Model\n"],"metadata":{"id":"6qglF4yw2lBP"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["%%capture\n","gpt_2_model = GPT2Model.from_pretrained(\"gpt2\").to(device)"],"metadata":{"id":"aRHvtWbtw_jp"}},{"cell_type":"code","source":["device"],"metadata":{"id":"OWEYB9EB_YXc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584635007,"user_tz":420,"elapsed":28,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"7c1b0692-51b7-46f2-f0a6-8d714c75c941"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([[1212,  318, 1049]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1]], device='cuda:0')}"]},"metadata":{},"execution_count":21}],"source":["tokenized_input = gpt2_tokenizer('This is great',\n","                              return_tensors=\"pt\").to(device)\n","tokenized_input"],"metadata":{"id":"XwHMzRyBw_jq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584635031,"user_tz":420,"elapsed":18,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"1f04c802-7c97-400b-f236-91db1dcc1da2"}},{"cell_type":"code","source":["output = gpt_2_model(**tokenized_input)\n","output.keys()"],"metadata":{"id":"WS-InhNSs-Br","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584635099,"user_tz":420,"elapsed":59,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"4f90c27c-ab0a-4751-a2e2-ff055cae6975"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'past_key_values'])"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Note that now we don't have 'logits' as the output anymore but 'last_hidden_state's. These are the output vectors of the last transformer layer. Let's look at the shapes:"],"metadata":{"id":"XN-N4D4SRq9l"}},{"cell_type":"code","source":["output.last_hidden_state.shape"],"metadata":{"id":"Rn2sjyDZveBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584635125,"user_tz":420,"elapsed":24,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"95e46081-e8c2-41a2-e510-548b65c95e5c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 768])"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["**Questions:**  \n","\n","2.a. What do these dimensions mean?   \n","2.b. Which 'last hidden state' \"knows\" about the entire context?   \n","2.c. How would we get the vector that has seen the full context?"],"metadata":{"id":"X3aJLzoRvj65"}},{"cell_type":"code","source":["output.last_hidden_state[:, -1].shape"],"metadata":{"id":"xUkeux0qveSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584635169,"user_tz":420,"elapsed":43,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"2555eea7-b499-43a7-8b51-b40279d02277"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 768])"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["#### b. Data Preparation\n","\n","Now we need the data and create the Dataset with all pre-processing and the Dataloader. We will use the IMDB dataset.\n","\n","The IMDB data was already created as **my_imdb_data_train**\n","\n","Let's look at it first:"],"metadata":{"collapsed":false,"id":"z89fkrunw_js"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b44a71195f407b9a36ebbd17195230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cef64ef529bf4ee88919c74150d17a7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0f96872c2446e88c1f760aa6b30421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0e46407d68457f909aaf85ae1dc47b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e40e019be8c432290db88d0cdb8c2f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb2b0e6434e46cb8f7a7a24d4f3ffef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3641f7142f1426da80cda02891ad0d7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["positive:  4931\n","negative:  5070\n","other:  0\n","positive:  1012\n","negative:  989\n","other:  0\n"]},{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1]"]},"metadata":{},"execution_count":25}],"source":["imdb_dataset = load_dataset(\"IMDB\")\n","\n","my_imdb_data_train = create_temp_set(imdb_dataset['train'], 10000)\n","my_imdb_data_test = create_temp_set(imdb_dataset['test'], 2000)\n","\n","[x['label'] for x in my_imdb_data_train[:20]]"],"metadata":{"id":"uCxQCXpSw_jt","colab":{"base_uri":"https://localhost:8080/","height":366,"referenced_widgets":["f9b44a71195f407b9a36ebbd17195230","ba46f62b58024d7984d333bc88dc8eba","17f2ea348f6544feb8e3b5d1739f3e15","bc3c4a6b6a76464a89dfac0477b0a30f","b1641972b0d74cff943db94a3d11c655","e0bb97589a614ae59adca0ddf3a4fca5","104e7f59c99f4d97802cf7857333c2e2","92fb3f902e0b43a18e6081f352a0febb","97e6c1aeb02a40119a2a2a356070b846","5f98640ecdc54b8c8bc8c28d2709f28f","6410341ad74f4f5baeb53a81b951b4a9","cef64ef529bf4ee88919c74150d17a7c","d280118bc6e24ae19a9c741571c47cf6","12838c834a344540aac5521f1632ed8b","7819ba7cb1b94d3b8374b8dbf2c13b90","e079688916a04c2c8da32c17826d58ad","d113e0c75ea14638bca419a317116f7a","8249a52c30a848fabd7283db137fb643","7d226615006d44e789fa764ad26e901c","dc8ce402c06c43a0919e1a65a5addff8","ebce42553a36417ca81230776a6ceca0","e7a5fbe84f9c42fabcf78b50d3ab1406","2d0f96872c2446e88c1f760aa6b30421","5b6f7000bfa442c680d777c709cbc0c1","19f39db4559c46a4baa217cd779d18e6","0c9c295f981a40879f1c144b05c1c6da","c45eb5e078da4da989e81cf89c8d9c1e","4f0cc26f2b774a278d17a3f8aadad63f","5d31a32a91764543abdd5ed265486da1","efaa832c1fc54583878464cc2b1e6fec","100751411a45471ea04ab5ae47de72c2","8713d7752f7a414cad398d3851a60642","d9dcfefd4bb547738bf595d48973a3db","cb0e46407d68457f909aaf85ae1dc47b","50e9ecd72c9e4d6db950bb5ee620385d","b280932221c34457ab6d02aa8b58a8c7","7170d9a0cb824b8d8d1a8096d35e5650","f16a8762a8c44ca0b580613668e3b7eb","201ea24fd6144195a886efa958b4d0e2","91a029af54ee4026abfa1de6b4eaae8e","f1e71a04d8344258a19359c300f4b9ae","1eebe7cb10cb40358d89495024c28aa4","665136d16bd7442994ec27776e78480c","564afb6c69ad4710983da17a7a2f175e","4e40e019be8c432290db88d0cdb8c2f9","e5a1df990d214e31a601b908ba027ba4","962d7a06ff8244429bc9b8da91a407a9","edad17081b25409f8feb331adf489a47","f32b4b7674dc4f7291f0ef627e106744","b321291ae88b4210b50ae71d5833f647","a06ebdac112c4106afacf523ec705b0f","e430f2609f4143588f229e0a2a835844","b06d5f8225f041a180e5eba982a8a302","9f78602423b54bf4b295c18aa652cd42","9a20283b5df94a90b7e99113b835d957","aeb2b0e6434e46cb8f7a7a24d4f3ffef","7f15d23b36b243d1b65e8bb34e43f984","b2ee94596f4a41eb957b2e54551277a4","7c26702a83b741089a919476afaa272b","f9f82d976eb84a5aa823f98f1c97d16f","5a4e5139be4c4e64baf590e5d889e515","6cbd003fc7564a95a83a9015d1c0e5ab","553371e35cf24cc39145d840fa44d8ce","5a9ecc1b14dd4a1ea3bf69c7aa5e1ecc","deff82b521454906bcde6d4237930435","d2f3fadeaf3644a085b120e61437c272","d3641f7142f1426da80cda02891ad0d7","6e1b949cdf3345ca982af94411f45622","b20e3f69a95d4cadb47cd5d76329df61","69d43ee961ee4c8f992cbb7581b46c5b","a2301c6c791d4d3d9b10879492704cbc","f286bd7b9dcb419c87cfa1bcae54641a","cc4cf9b57fa848739dfcff21be1e57f3","b99e99cc7d5749a7b89dbf1eea68d17e","e0ab63b5118848f79c99facd6e11245a","0945baeb75cb44d39d4c3e567ecc0ebb","15fbb903231442e18fbe167720140470"]},"executionInfo":{"status":"ok","timestamp":1747584651344,"user_tz":420,"elapsed":16176,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"1c7ece38-c271-4ef7-ecdc-19b3925bda36"}},{"cell_type":"markdown","source":["We will now create the actual Dataset and the Dataloader.\n","\n"],"metadata":{"id":"2TnHlOC30FHj"}},{"cell_type":"code","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-8c02ab630396>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  {'input_ids': torch.tensor(torch.squeeze(tokenized_example['input_ids']),\n","<ipython-input-6-8c02ab630396>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  'attention_mask': torch.tensor(torch.squeeze(tokenized_example['attention_mask']),\n","Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["train_data = ClassificationData(my_imdb_data_train, tokenizer=gpt2_tokenizer, max_len=100)\n","test_data = ClassificationData(my_imdb_data_test, tokenizer=gpt2_tokenizer, max_len=100)"],"metadata":{"id":"z3-86YHfw_ju","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584677909,"user_tz":420,"elapsed":26564,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0a6ade93-400b-480a-fd0a-9129f21679cb"}},{"cell_type":"code","source":["batch_size = 8\n","train_texts = DataLoader(train_data, batch_size=batch_size, shuffle=True) # usually we shuffle, but we shuffled already above\n","test_texts = DataLoader(test_data, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"HN5V3AXv_gJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next(iter(train_texts))"],"metadata":{"id":"yNM6myF2gNn0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584677943,"user_tz":420,"elapsed":30,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"6bd94abc-a88e-4608-8a9b-8d89c1f21755"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': {'input_ids': tensor([[40277, 25028, 36997,  5341,   262,  2597,   286,   257,  4346,   286,\n","             257, 11783, 19500,   326,  4078,  2387,   262,  1208,   286,   734,\n","            5510,  1938,   290,  8404,   284,  3677,   606,   329,   845,  1310,\n","            1637,   284,   262,  8976,  3430,   286,   262,  3932,    69,  3970,\n","             357, 18664,   286,   663,  2612,   828,  4851,  4347,    78,    11,\n","            4361,   777,  1938,   750,   407,   711,   880,    11,   290,   340,\n","            2227,   326,   262, 10029,  4347,   373,  2642,   276,   351,   428,\n","              13,   887,   644,  4325,   318,   326,   777,   734,  1938,   706,\n","             477,   389,   922,   290, 10029,  4347,    78,  3677,   606,   329,\n","             881,  1637,   284,   257,  3215,  3430,    11,  1642,   257,   922],\n","          [ 1212,  2646,   561,  3221, 36509,   355,   262,  5290,  3807,  3227,\n","            1683,    13, 10776,    13,   887,   287,   616,  4459,   340,   318,\n","            5457,   262, 36090,  6386,    13,   383, 31906,  4571,   290, 39769,\n","            1838,   428,  2646, 49083,    13,   314,  5839,   262,  3807, 14590,\n","             264, 13309,   832,   262, 22803, 12490,   338,   379,   616,  1957,\n","           14896,  6128,    13,  2185,   290,   617,  2460,   788,  7342,   340,\n","              11, 33603, 14590,  2138, 10785,    13,   632,  2582,  5091,   326,\n","             428,  2492,   470,   597,  3487,  2646,    13,  5455,   257, 49083,\n","           26341,   286,   644,   481,  2192,   307,  3700, 44398,   359,   338,\n","             938,  2646,    13,  1629,   717,   356,   547, 10416,   290,   547],\n","          [ 1212,   318,   257,  1049,  2126,   329,   257,  2646,   475,   340,\n","              11, 12716,    11,  1595,   470,  1210,   503,   284,   307,   257,\n","            1049,  3807,    13,  1867,  4940,   503,   355,   257,  6029,   290,\n","            2048, 44089, 14348, 10997,   546,   257,  1610, 13712,   287,  1842,\n","             351,   465,  1610,  1648,  1453,  9158,   874,   503,   286,  1630,\n","             656,   257, 13699,  6087,   286, 27962,   290,   257,  3326,  4674,\n","           20798,   286, 21528,    11,   351,  7270,  8804,   654,   422,   347,\n","            6684,    38, 10008,   399, 34874,    11,  3336,    43,  5673,  5357,\n","             406,  2606, 24352,    11, 37041,    38,  4663,  6561,    11,  8782,\n","            1797,    42,    11,   290,   772,   257, 11040,   366, 42460,     1],\n","          [  464, 31900,  2297, 11397,   357, 21063,   494,  6602,     8,  1279,\n","            1671,  1220,  6927,  1671, 11037,  7594,   366,   464,  1869,  3574,\n","           11397,  1395,   553,   428,   318,   257, 13699,  3783, 10165, 12838,\n","           37429,   276,   422,   281,  6980,   810,  8842,   290,  3783, 10165,\n","             547,   991, 12270,  1474,   262,   976,  1517,    13, 30563,    11,\n","             356,   423,   617,  4047, 47623,  2041,  3048,   290, 26262, 24543,\n","              12, 16801, 12422, 10819,  4980,   355,  3783, 10165,    13,   843,\n","            3763,    11,   340,   338,  1194,   366, 49421,     1,  2716,   287,\n","             257,  1029,  3081,  4351,   351,   257, 23453,  4286,   290,  7786,\n","            2128,   438,  1525,  7215,    77,   578, 27151, 29847,  1671,  1220],\n","          [   40,  6348,   510,   319,   428,  3807,   290,   314,   460,  3505,\n","             618,   616,  3956,   290,   314,   973,   284,   711,   287,   262,\n","           24296,   290, 16614,   356,   547,   287,  7276,    12,    64,    12,\n","           26487,    13,  2735,    11,   706,   523,   867,   812,   423,  3804,\n","              11,   314,   651,   284,  2342,   262,  3807,   351,   616,  4957,\n","             290,  2342,   607,  2883,   340,    13,  1002,   345,   389,  2560,\n","             290,   345,   423,   407,  7342,   428,  3807,   351,   534,  1751,\n","              11,   788,   345,   815,    11,   655,   523,   345,  1745,   606,\n","             287,   534,  5101,   290,  2342,   606,   651, 20536,   625,   262,\n","            1337, 13062,   290,  1337,    12,    64,    12, 26487,     0,   383],\n","          [  464,  7016,  2928,   286,   428,  3807,   825,   444,  2456,    13,\n","             632,   318, 19992,    11, 11800,    11,  4950,    11,   290, 15444,\n","             477, 11686,   656,   734,  2250,    13,   770,   318,  2561,  4176,\n","             355,   339,  2603,   942,   656,   465,  7205,  2694,    11,   262,\n","            1336,  2837,   286,   340,    13,  5338,  2993,    30,   314,  2497,\n","             383, 34089,  5013,   286, 38456,   290,  1807,    11,   428,  1276,\n","             307,   257,   781,  4649,   329,   262, 32034,    11,   625,    12,\n","            1169,    12,  4852,  8674,    11,  4176,    13,  2399, 13289,   287,\n","            1111,  6918, 10993,   257,  2187,   584, 15793,   284,  4176,    11,\n","             257, 47517,   286,  7401,    11,   262,  2922,  3458,   286, 14750],\n","          [   40,   423,   407,  1100,   262,  5337,    11,   996,   314,  1833,\n","             326,   428,   318,  6454,  1180,   422,   340,    26,   262,  1109,\n","             326,   314,  2138,  8359,   428,    11, 18064,   351,   262,  1109,\n","             326,   428,  1107,   318,   407,   616, 12121,    11,  5983,   502,\n","             284,   262,  2551,   286,   407, 15461,  3555,   262,  1492,    13,\n","           11136,   407,  1100,   257,  2060,  1573,   286,  2517,   268,   338,\n","            3597,    11,   314,  1107,   460,   470,  8996,   428,   284,   597,\n","             286,   607,   670,    13,  1867,   314,   460,   910,   318,   326,\n","            2048,   790,  1627,   286, 17310,   287,   428,   318, 14169,    11,\n","           45466,    11,   290,   880,    12, 12381,  6396,    11,   355,   880],\n","          [15001,   319,  7760, 11563,   338,  3223,  1266, 32932,    11,   428,\n","            1757,   357, 35192,  1137,  3268,  3336,   370, 22808,    50,     8,\n","             367,   619,    12, 34762, 13076,   468,  1310,  1016,   329,   340,\n","           29847,  1671,  1220,  6927,  1671, 11037, 10915,   340,   857,   407,\n","            3092,   308,   652,  3685,    11,   340,  3092,  8689,  3054,  2247,\n","             290,   366, 10641, 19858,  1911,    27,  1671,  1220,  6927,  1671,\n","           11037,   464,   366, 25517,   549,   385,     1,   286,   262,  3670,\n","             318,   257,  3222, 44134,   351,   257, 44268, 16360,   326, 20611,\n","            2266, 19311,   656, 14334, 24252,  1141, 22409,  1377,   393,    11,\n","             284,   307,   517,  7141,    11,  7262, 29847,  1671,  1220,  6927]],\n","         device='cuda:0'),\n","  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1]], device='cuda:0')},\n"," 'label': tensor([1., 1., 0., 0., 1., 1., 1., 0.], device='cuda:0')}"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Let's build the network. Of course, ideally we should have cleaned the text during dataset generation and below we should use regularization like Dropout, but let's just do the simplest approach to run our first classification using LMs."],"metadata":{"id":"-9Uibw4Lq-cr"}},{"cell_type":"code","source":["class MyTextClassificationNetworkClass(torch.nn.Module):\n","    def __init__(self, embedding_model, embedding_model_dim):\n","        super().__init__()\n","\n","        self.lm = embedding_model\n","        self.linear =  torch.nn.Linear(embedding_model_dim, 1)\n","        self.activation = torch.nn.Sigmoid()\n","\n","\n","    def forward(self, x):                             # x stands for the input that the network will use/act on later\n","        ### Code here!\n","\n","        model_out = self.lm(**x)['last_hidden_state']\n","        last_vector = model_out[:, -1]\n","\n","        linear_out = self.linear(last_vector)\n","        output = self.activation(linear_out)[:, 0]\n","\n","        return output\n","\n","my_text_classification_network = MyTextClassificationNetworkClass(embedding_model=gpt_2_model,\n","                                                                  embedding_model_dim=768)\n","\n","my_text_classification_network.to(device)"],"metadata":{"id":"RGXqvAkpBPr6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584677947,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"0dd58801-1249-4b6a-9eb3-f9872cbffa6f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MyTextClassificationNetworkClass(\n","  (lm): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D(nf=2304, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=768)\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D(nf=3072, nx=768)\n","          (c_proj): Conv1D(nf=768, nx=3072)\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (linear): Linear(in_features=768, out_features=1, bias=True)\n","  (activation): Sigmoid()\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["example = next(iter(train_texts))"],"metadata":{"id":"JVGdEB2f0vhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example"],"metadata":{"id":"L8m6hZtyQ_2n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584677974,"user_tz":420,"elapsed":25,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"349aee82-3f88-461e-c34c-69a2b882e087"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': {'input_ids': tensor([[ 2504,   373,  4753,   262,  1339,   351, 17654,   287,   262,  3806,\n","            3245,    13,   632,   373,   319,  3195,   938,  1755,   290,   314,\n","            1975,   314,  8020,   470,  1775,   262,  2646,  1201,   616, 24505,\n","             614,   287,  1029,  1524,   290,   314,  1101,   783,   287,   616,\n","             604,   400,   614,   286,  4152,    13,  4900,   262,  2646,   468,\n","             867, 17978,    11,   340,   318,   655,   523, 15241,   326,   345,\n","             460,   470,  1037,   475,  1650,   866,    11,  2342,   340,    11,\n","             290,  2883,  3511,    13,   632,   318,   635, 20105,    13, 15105,\n","           43487,   338,   374, 20482,   318,   655,   523,   625,   262,  1353,\n","             326,   345,   460,   470,  1037,   475,  6487,   503,  7812,   379],\n","          [  464,   691, 26831,   373,   262,  1402,   636,   416, 13633,  7920,\n","              13,   632,  3947,   326,   262,  3807,   373,  2111,  1165,  1327,\n","             284,   307,   366, 22210,  7994,  5335,   553,   475,   314,  1422,\n","             470,   772,   588,   326,  3807,   290,   340,   991,  3214,  1790,\n","             286,   883,  5423,    13,   383,  8674,   508,  5341,  3362,   373,\n","            1049,    11,   475, 15300,  2611, 18520,   318,  7819,   287,   262,\n","           37775,   286,   607, 44153, 39168,   507,  2095,    13,  3700,  2806,\n","            2815,   373,  1049,    11,   475,  3362,   338,  2988,  3947,   588,\n","             339,   373,  2111,  1165,  1327,   284,   307,   262, 21248,  2264,\n","            1698,  2095,   422,   262,  2351, 28607,  2049,   338, 25709,   341],\n","          [   72,  1682,  1807,   428,   318,   257, 10997,   290,  3332,  4964,\n","             340, 12451,   284,  6487,   616,   840,   572,    13,  2495,  2582,\n","             287,  2627,  1598,   428,   318,   645, 10997,    11,   393,   379,\n","            1551,   407,   257,   705, 18050,  1879,  4364,  2099,     6,   530,\n","              13,   644,  4030,   356,  4964,   373,   262,  3435,   532,   262,\n","            3807,  4940,   351,   617,  2495, 18288,    11, 17840,   661,    11,\n","            9272,  1978,   284,  1949,   290,  1907,   530,   286,   511,  4096,\n","           10251,   532,  3252,   286,  1660,    11,  3252,   286, 14899,    13,\n","             356,   923,   284,   651,  1643,   416,  1643,   656,   511,  3160,\n","              11,  4973,   511, 14979,    11,  4724,   286,   511,  6066, 29847],\n","          [10723,   262, 13745,   351,   262,   976,  3670,   422, 16994,     0,\n","             770,   925,   329,  3195,  3807,    11,   318,   655,  5770,    12,\n","             707,   913,     0,  4900,   340,   857,   779,   357,   292,  1290,\n","             355,   314,   460,  1560,     8,  2048,   262,   976, 17310,    11,\n","             340,   655,  1595,   470,   670,     0,  1148,   340,   262,  7205,\n","              11,   262,  3595, 21024,    30,  7477,   523,   340,   338,   925,\n","             329,  3195,    11,   475,  1521,  2342,   257,  2089,  4866,    11,\n","             618,   345,   460,   651,   534,  2832,   319,   262, 21840,  2656,\n","              30, 18948,   355,   345,  1183,   307, 34781,   284,   262,  7110,\n","             290,  1839,   470,  2883,   262,  2656,   355,   881,    11,   355],\n","          [   16,  1374,   318,   340,   326,  2506,   460,  1833,  1123,   584,\n","            7138,  1231,  4410,   588, 10112,  4779,  2024,   393, 33417, 33712,\n","              30,  7731,   262, 16294,   286,   428,   905,  6537,   326,   661,\n","             508,   547,  2077,   422,  1180,  3354,   286,   262,  4534,    11,\n","             287,  1180,   640, 13431,   357,  8086,  5049,   262,  5900,  2492,\n","             470,   257, 11811,   286,   662, 17201,   378,  5783, 35866, 13817,\n","              11,  4249,   547,   262, 16420, 11811,   284,   262, 41450, 31606,\n","               8,  2740,  1180,  8950,   290,   460,  1239,  1205,   257,  3303,\n","             523,  2092,   284,  3660,  1110,  3594,     7, 16341,   329,   262,\n","            1167, 26448,   484,   366,  4598,   407,     1,   779,   828,   543],\n","          [   40,  1392,   428,  3807,   780,   314,  3111,   379,   257,  3807,\n","            3650,   523,   314,  1392,  1479, 39457,    13,   632,  1625,   287,\n","              11,   290,   262,  3002,   925,   340,   804, 23036,    13,  6964,\n","           17943,    11,  6872,   257,  4282,    11, 23036,    11,   314,  1183,\n","            2198,   340,   503, 29847,  1671,  1220,  6927,  1671, 11037,  5812,\n","             582,    11,  2089,  1445,    13,   770,   373,   523, 12361,    11,\n","             314,  3377,  2063,   262,  3807,  4964,   287,  3049,    12, 11813,\n","             284,   651,   284,   262, 42156,    11,   543,   373, 10926,    13,\n","             314,   892, 26720, 12473,  1115,  8188,   286, 13027, 42156, 29847,\n","            1671,  1220,  6927,  1671, 11037,  7376,  9259, 10721,    11, 40805],\n","          [ 3666, 19502,   329,   569,  3824, 18429,  1058,    12,  6363,   257,\n","           23332,    11,  7306,  2569,  1842,  1621, 20495, 18381,   312, 27344,\n","            2675,   290,  1703,   799,    64, 48395,    13,   383,  2646,  2753,\n","             514,   736,   284,  1402, 35396,   588,   262, 26619,   290, 26619,\n","              70,  3823,   338,  4172, 11029,   319,   262,  4314,    11,  2712,\n","            1830,  1978,    11,   511,  8030, 46420,   290, 13584,  2461,    13,\n","           25313,   993,   318,   546,   262, 25417,   414,   286,  4845,   290,\n","             262,  6817,   286,  7901,  1022,   734,  3925,    13,  3363,    11,\n","             262,  4318, 19661,   318, 12385,  2280,  5874,  1143,    13,   887,\n","             262, 10505,  4335,    12,   259, 14348,  7188,  1022,   262,   284],\n","          [   42, 14232,   290,  5462,  1956,   319,   257, 21757,  5440,   810,\n","             262,  4252,   318,   546,   284, 22818,    13,  1119, 14765,   284,\n","           36316,   262, 17622,   475,  1064,   262,  1295, 36043,  2845,   329,\n","             257,  1770,    13,   317,  1462,    89,   508, 14051,   617,  3297,\n","             286,  1029,    12, 13670,  5888,    13,  7945,  2111,   284,   651,\n","             257,  3892,  3280,   422,   683,   546,  2506,   338, 33035,    11,\n","             317,  1462,    89,   318, 31655,   284,   511,  2683,   290, 16361,\n","             484,  1276,  2952,   705, 15883,   257,  6356,   981,   612,   318,\n","             991,   640,  4458,  1119,   423,   645,  2126,   644,   339,   338,\n","            3375,   546,   475, 27776,   546,  2045,   379,   262,  1021, 10162]],\n","         device='cuda:0'),\n","  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1],\n","          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","           1, 1, 1, 1]], device='cuda:0')},\n"," 'label': tensor([1., 0., 1., 0., 0., 0., 1., 1.], device='cuda:0')}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["example['input']['input_ids'].shape"],"metadata":{"id":"ef0QSpamh3V3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584678010,"user_tz":420,"elapsed":35,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"b6ae83f1-ba62-451e-c959-5541dfc13b5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 100])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["sample_out = my_text_classification_network(example['input'])"],"metadata":{"id":"tTdARk8F0o-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_out"],"metadata":{"id":"54oj4PzGi6NU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584678102,"user_tz":420,"elapsed":13,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"2799f25b-aad2-4535-c418-e12fd222a26e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0038, 0.0022, 0.0005, 0.0004, 0.0010, 0.0032, 0.0032, 0.0015],\n","       device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["NOTE: we can obviously build the network layer-by-layer and test whether we get the expected output dimensions!"],"metadata":{"collapsed":false,"id":"lSdGywvHuf0-"}},{"cell_type":"markdown","source":["Let's set up the loss function and the optimizer:"],"metadata":{"id":"6lMTx6IUMFyv"}},{"cell_type":"code","source":["loss_fn = torch.nn.BCELoss()\n","adam_optimizer = torch.optim.Adam(my_text_classification_network.parameters(), lr=0.0001)"],"metadata":{"id":"Zer3mvWGK89C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's train a little bit."],"metadata":{"id":"wA2F_LP-5Wqe"}},{"cell_type":"code","source":["my_text_classification_network = my_text_classification_network.to(device)\n","\n","epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_texts, my_text_classification_network, loss_fn, adam_optimizer, steps=200)\n","    test_loop(test_texts, my_text_classification_network, loss_fn, steps=50) # no optimizer use here!\n","print(\"Done!\")"],"metadata":{"id":"MkwTXQo-NgHc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584715319,"user_tz":420,"elapsed":37215,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"9e1b4852-98dd-4ca7-8562-6cdcc5ccb14d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","\tFinished batches:  25\n","\tCurrent average loss:  1.091543046136697\n","\tFinished batches:  50\n","\tCurrent average loss:  0.9147673200587837\n","\tFinished batches:  75\n","\tCurrent average loss:  0.821073550227526\n","\tFinished batches:  100\n","\tCurrent average loss:  0.7639714377095\n","\tFinished batches:  125\n","\tCurrent average loss:  0.7007329838891183\n","\tFinished batches:  150\n","\tCurrent average loss:  0.6800354116114994\n","\tFinished batches:  175\n","\tCurrent average loss:  0.6474665130178133\n","\tFinished batches:  200\n","\tCurrent average loss:  0.623677848571509\n","Training Results: \n","  Avg train loss: 0.620181 \n","\n","Test Results: \n"," Test Accuracy: 69.1%, Avg test loss: 0.543997 \n","\n","Done!\n"]}]},{"cell_type":"markdown","source":["Clearly not trained enough, but we see that the model trained. (Obviously, this can be done muxh better (dropout, etc.)\n","\n","\n","####3. The 'Other Language Model': Masked Language Models and simple Sentence Embeddings\n","\n","We will now look at Masked Language Models, specifically BERT as an old & famous one."],"metadata":{"id":"GSmRX2HOsOvH"}},{"cell_type":"code","source":["%%capture\n","\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)"],"metadata":{"id":"EO4iOP3VF72e","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1747584723881,"user_tz":420,"elapsed":8560,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"1696a1e4-e7b7-4d2f-e0cc-0192ed083a99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]}]},{"cell_type":"code","source":["inputs = bert_tokenizer([\"This was fun and useful\", \"I enjoyed the cool event\", \"Cars are slow and expensive\" ],\n","                        return_tensors=\"pt\").to(device)\n","outputs = bert_model(**inputs)"],"metadata":{"id":"DYghDqhwx1Xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs.keys()"],"metadata":{"id":"yZCddvxhyMQ3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584723927,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"c4668ee6-148f-48c5-df35-6a479f2b2c84"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output'])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["What are these? Let's first look at the dimensions:"],"metadata":{"id":"csBcWsus2qST"}},{"cell_type":"code","source":["outputs['last_hidden_state'].shape"],"metadata":{"id":"Uhx4llZnyNVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584723939,"user_tz":420,"elapsed":7,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"a26e0900-e453-4eb7-c905-2e6725e49c80"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 7, 768])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["outputs['pooler_output'].shape"],"metadata":{"id":"KTWnybu92dRo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584723944,"user_tz":420,"elapsed":4,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"9a9b1811-f537-440f-a428-056e5292d29b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 768])"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["The BERT model has by default 2 outputs: the last hidden state and the Pooler output. The Pooler output is derived from the last hidden state of the CLS token with some task-specific fine-tuning. I.e, it should be a 'better' representation of the overall text. Let's look at that:"],"metadata":{"id":"xd0GVnpt2FiZ"}},{"cell_type":"code","source":["pooler_outputs = outputs['pooler_output'].to('cpu').detach().numpy()\n","\n","cls_outputs = outputs['last_hidden_state'][:, 0, :].to('cpu').detach().numpy()\n"],"metadata":{"id":"4AXzFOTxySEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Pooler comp 0-1: ', cos_sim(pooler_outputs[0], pooler_outputs[1]))\n","print('CLS comp 0-1: ', cos_sim(cls_outputs[0], cls_outputs[1]))"],"metadata":{"id":"ERHKqj7TymIt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584723998,"user_tz":420,"elapsed":6,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"14e494fb-008a-4ae4-c75b-c201e2580755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pooler comp 0-1:  0.9004953\n","CLS comp 0-1:  0.9113325\n"]}]},{"cell_type":"markdown","source":["Ok. That seems reasonable. But what about comps with the much less similar sentence?"],"metadata":{"id":"6llI1phI17Gv"}},{"cell_type":"code","source":["print('Pooler comp 0-2: ', cos_sim(pooler_outputs[0], pooler_outputs[2]))\n","print('CLS comp 0-2: ', cos_sim(cls_outputs[0], cls_outputs[2]))"],"metadata":{"id":"eu76rhiEy4x_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747584723998,"user_tz":420,"elapsed":3,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"08bc3a31-39c5-4d11-fb04-1836ee3fca18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pooler comp 0-2:  0.74846876\n","CLS comp 0-2:  0.8500113\n"]}]},{"cell_type":"markdown","source":["Clearly, the Pooler output has much better contrast. So without fine-tuning, the Pooler output appears to be more suitable for text representations than the CLS token output. (However, fine-tuning on various tasks would take care of that too for specific situations.)"],"metadata":{"id":"SiskaroV1aiM"}}]}