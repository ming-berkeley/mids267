{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# DATASCI267 - Week 4 - Lesson Notebook: Instruction Tuning (RLHF/PPO)\n","\n","\n","In this notebook we will use Instruction Tuning to train our our old friend GPT-2 to have more positive responses. The notebook is motivated by (and the RLHF training part is - with some modifications - essentially directly taken from) **an example from Huggingface's TRL library**, published [here](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb) and released under a Apache 2 license.\n","\n","\n","Here is the logic of what we want to do:\n","\n","1. We will use a further fine-tune version of GPT-2, which has been further pre-trained on the IMDB dataset language ('[lvwerra/gpt2-imdb](https://huggingface.co/lvwerra/gpt2-imdb)').\n","\n","2. We use an existing sentiment classification model as a **reward model**. The reference notebook uses the fine-tuned BERT-like model '[lvwerra/distilbert-imdb](https://huggingface.co/lvwerra/distilbert-imdb)'. This saves us the effort to train a separate reward model. (Not that we do not want to train for actual instructions but completions, which in the end is quite a bit simpler in that one actually can train directly a reward model directly on example/label pairs vs preference data (instruction - chosen answer - rejected answer).)\n","\n","3. We will then use the IMDB dataset as a training dataset. (We will only look at a few batches, as this procedure is really slow for RLHF and computation-wise expensive.) We will prepare the dataset as follows:  \n","   a) for each example we take a short segment at the very beginning as the 'prompt'.   \n","   b) We will then use the language model to generate **two** completions.    \n","   c) We will then use the reward model to: i) assign a score for either completion, and ii) determine which one of the two completions is preferred.   \n","\n","4. Then we will train our models (keeping the reference models fixed). We will do so for        \n","   a) RHLF/PPO  \n","   \n","\n","5. We will then compare completions and their sentiment before and after having done a few training batches.\n","\n","This notebook is designed for a quick illustration. To get more reliable results you should run the training longer (see the original notebook). We would also encourage you to look at some of the other details.\n","\n","This notebook can be run on a T4 processor and higher. (A L4 (on Colab Pro) is maybe 20% faster for the notebook that T4).\n","\n","A Hugging Face blog post that compares DPO, IPO (a DPO variant) and paired KTO tests can be found here: https://huggingface.co/blog/pref-tuning . (For this term's notebook version, we excluded the original DPO and pair-wise KTO computation - which used the DPO Trainer with a KTO loss as described in the blog post - as they are unstable.)\n","\n","**Notes:**\n","\n","1. Some text in this notebook will be generated dynamically. As we use a simple language model, the ethical quality and general appropriateness cannot be guaranteed.   \n","2. It appears that the Hugging Face PPO implementation became more efficient. The results for that method appear to be quite good for this toy example. (And it is just a toy example.)  \n"],"metadata":{"id":"FsAJUM858NVU"}},{"cell_type":"markdown","source":["## 0. Setup\n","\n","We run a few installations and imports. We will also define how to create the dataset."],"metadata":{"id":"caZcWIVaitH-"}},{"cell_type":"code","source":["%%capture\n","#!pip install transformers  # not required in Colab as already installed"],"metadata":{"id":"hnejikKY3NS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","!pip install trl bitsandbytes\n","!pip install -U trl==0.8.6"],"metadata":{"id":"yW3JY-zW821A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from tqdm import tqdm\n","import pandas as pd\n","import warnings\n","#warnings.filterwarnings('ignore') # this is added relative to original notebook. We don't want to see a lot of warnings.\n","\n","tqdm.pandas()\n","\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n","from datasets import load_dataset\n","from datasets import Dataset\n","\n","from trl import PPOTrainer, PPOConfig,  AutoModelForCausalLMWithValueHead\n","from trl.core import LengthSampler"],"metadata":{"id":"39cwcYxNFdKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_dataset(config, dataset_name=\"imdb\", input_min_text_length=6, input_max_text_length=14):\n","    \"\"\"\n","    This function builds the initial dataset from `load_dataset`, specifically\n","    it creates the initial chunk from each review.\n","\n","    \"\"\"\n","    def tokenize(sample):\n","        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n","        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n","        return sample\n","\n","\n","    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    # load imdb with datasets\n","    ds = load_dataset(dataset_name, split=\"train\")\n","    ds = ds.rename_columns({\"text\": \"review\"})\n","    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n","\n","    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n","\n","\n","    ds = ds.map(tokenize, batched=False)\n","    ds.set_format(type=\"torch\")\n","    return ds"],"metadata":{"id":"K3SrRnJgZt6i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function below we use for our KL-Penalty calucltions:"],"metadata":{"id":"nCDNCW1cG3to"}},{"cell_type":"code","source":["def log_pi(model, generated_output_tokens_x_y, len_x):\n","  \"\"\"\n","  model: the model to be used for your pi calculation\n","  generated_output_tokens_x_y: the returned output tokens including the intial prompt tokens. Shape: (len(x) + len(y), )\n","  len_x: the number of input tokens\n","  We calculate the average log_likelihood of the y tokens.\n","  \"\"\"\n","\n","  len_x_y = len(generated_output_tokens_x_y)\n","\n","  logits = model(generated_output_tokens_x_y)[0]\n","  probs = torch.nn.Softmax(dim=-1)(logits).detach().cpu().numpy()\n","\n","  loglikelihood_score = 0\n","  for pos in range(len_x - 1, len_x_y - 1):     # only the y tokens matter\n","    log_prob = np.log(probs[pos, generated_output_tokens_x_y[pos + 1]])\n","    loglikelihood_score += log_prob\n","  return loglikelihood_score"],"metadata":{"id":"wbCASUg-G4Vd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Initial Model Creation, PPO (=RL Training) Config & Base Dataset Preparation\n","\n","Hugging Face makes it easy to set up RLHF training using PPO:"],"metadata":{"id":"AU5o64kvZuu2"}},{"cell_type":"code","source":["config = PPOConfig(\n","    model_name=\"lvwerra/gpt2-imdb\",\n","    learning_rate=1.41e-5,\n","    #log_with=\"wandb\",\n",")\n","\n","sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"],"metadata":{"id":"A7ifZH09FdMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySDUKCebJW35","outputId":"498682f9-11d8-45ab-80a1-7c19bbbacc47","executionInfo":{"status":"ok","timestamp":1748308245984,"user_tz":420,"elapsed":138,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PPOConfig(exp_name='colab_kernel_launcher', seed=0, log_with=None, task_name=None, model_name='lvwerra/gpt2-imdb', query_dataset='imdb', reward_model='sentiment-analysis:lvwerra/distilbert-imdb', remove_unused_columns=True, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', push_to_hub_if_best_kwargs={}, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=128, forward_batch_size=None, mini_batch_size=128, gradient_accumulation_steps=1, world_size=None, ppo_epochs=4, max_grad_norm=None, optimize_cuda_cache=None, optimize_device_cache=False, early_stopping=False, target_kl=1, compare_steps=1, ratio_threshold=10.0, use_score_scaling=False, use_score_norm=False, score_clip=None, whiten_rewards=False, is_encoder_decoder=None, is_peft_model=None, backward_batch_size=128, global_backward_batch_size=None, global_batch_size=None)"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Now let's create and look at the dataset a bit:"],"metadata":{"id":"doie90_7aQsZ"}},{"cell_type":"code","source":["%%capture\n","\n","dataset = build_dataset(config)\n","\n","\n","def collator(data):\n","    return dict((key, [d[key] for d in data]) for key in data[0])"],"metadata":{"id":"wd5naNIkFdRl","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"6ee7035d-fa73-4b91-e3b6-345c452b0402","executionInfo":{"status":"ok","timestamp":1748308294588,"user_tz":420,"elapsed":48664,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["len(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rxcBPGRHJ9oi","outputId":"24453e8d-2ab8-445d-e426-d42c86edaa2d","executionInfo":{"status":"ok","timestamp":1748308294643,"user_tz":420,"elapsed":53,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24895"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["dataset[2]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUIr_XCEaWVv","outputId":"435d00e2-fa83-4fe0-ad0c-39b1ee9aaf69","executionInfo":{"status":"ok","timestamp":1748308294723,"user_tz":420,"elapsed":76,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'review': \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\",\n"," 'label': tensor(0),\n"," 'input_ids': tensor([1532,  691,  284, 3368, 1642,  428]),\n"," 'query': 'If only to avoid making this'}"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["So the dataset has the full review, the label, and **also** the start of the prompt, which will later be given to the model to construct completions.\n","\n","Here are the models to tune and the reference models (including the ones we'll use later):"],"metadata":{"id":"nQtZohAOabbB"}},{"cell_type":"code","source":["config.model_name"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"8rehAoVLZgyw","outputId":"65958dd1-be75-48f9-cdc8-dc386dba2b2d","executionInfo":{"status":"ok","timestamp":1748308294806,"user_tz":420,"elapsed":84,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'lvwerra/gpt2-imdb'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["%%capture\n","\n","# create the model to tune and the reference models\n","\n","# Models to fine-tune: RLHF\n","rlhf_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","\n","\n","\n","# Reference model\n","ref_rlhf_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n","\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n","\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"id":"oxlptMwWGgpe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that the RLHF framework has a separate trainer class (PPOTrainer) from the DPO/KTO/IPO framework (DPOTrainer). Therefore the reference models are structured a bit different. But at this point, they are all 'lvwerra/gpt2-imdb' models.\n","\n","Now we can set up the Hugging Face Trainer [PPOTrainer](https://huggingface.co/docs/trl/main/en/ppo_trainer). (We **do not** want to do this from scratch in straight PyTorch!). It takes the configuration, the model, the original reference model, the dataset and a few additional parameters as input:"],"metadata":{"id":"vwmsGvo_ax2y"}},{"cell_type":"code","source":["ppo_trainer = PPOTrainer(config, rlhf_model, ref_rlhf_model, tokenizer, dataset=dataset, data_collator=collator)"],"metadata":{"id":"MARp5BPFGj2L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hugging Face's [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) is also useful to use. It makes it very easy to generate model outputs. In this case, we set up a pipeline that can take various examples as inputs and return the sentiment for each example using the specified model 'lvwerra/distilbert-imdb' in this case:"],"metadata":{"id":"orgJe_ymbr_J"}},{"cell_type":"code","source":["%%capture\n","\n","device = ppo_trainer.accelerator.device\n","if ppo_trainer.accelerator.num_processes == 1:\n","    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug\n","sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device=device)"],"metadata":{"id":"YSy-6KHgGnj8","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1748308307963,"user_tz":420,"elapsed":5106,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}},"outputId":"38025d1e-462d-4ae0-bbff-dc080b7098e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]}]},{"cell_type":"markdown","source":["How do we use this?"],"metadata":{"id":"SxZaRjvtcciB"}},{"cell_type":"code","source":["text = \"this movie was really poor!!\"\n","sentiment_pipe(text, **sent_kwargs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":108},"id":"4ss9yhRRGrFf","outputId":"29103c99-aef2-4980-ff91-40f934cd7bcf","executionInfo":{"status":"ok","timestamp":1748308309182,"user_tz":420,"elapsed":1206,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[[{'label': 'NEGATIVE', 'score': 2.368640184402466},\n","  {'label': 'POSITIVE', 'score': -2.758239984512329}]]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Now we can create the actual dataset batches that we want to use (note that this is a bit messy due to preparing for multiple training approaches). This will take about 6 min on a T4 processor."],"metadata":{"id":"tffYcqyKwwcv"}},{"cell_type":"code","source":["output_min_length = 10\n","output_max_length = 20\n","output_length_sampler = LengthSampler(output_min_length, output_max_length)\n","num_batches = 6  # each batch has 256 examples\n","\n","\n","generation_kwargs = {\n","    \"min_length\": -1,\n","    \"top_k\": 0.0,\n","    \"top_p\": 1.0,\n","    \"do_sample\": True,\n","    \"pad_token_id\": tokenizer.eos_token_id,\n","    #\"temperature\": 0.5\n","}\n","\n","\n","training_batches = []\n","\n","for batch_nr, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n","\n","    batch_data = {}\n","\n","    # Let's just use <num_batches> batches while we are in class. You should comment this out when you want to get to a much better results.\n","    if batch_nr == num_batches:\n","      break\n","\n","    query_tensors = batch[\"input_ids\"]\n","\n","    #### Get response from gpt2\n","    response_tensors_1, response_tensors_2 = [], []\n","    response_texts_1, response_texts_2 = [], []\n","\n","    for query in query_tensors:\n","\n","        gen_len = output_length_sampler()\n","        generation_kwargs[\"max_new_tokens\"] = gen_len\n","        response_1 = ppo_trainer.generate(query, **generation_kwargs)    #create two possible answers. You can use the PPO trainer for that as it comtains the base model that at this point is the same for all three approaches.\n","        response_2 = ppo_trainer.generate(query, **generation_kwargs)\n","\n","        response_tensors_1.append(response_1.squeeze()[-gen_len:])\n","        response_tensors_2.append(response_2.squeeze()[-gen_len:])\n","\n","        batch[\"response_1\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors_1]\n","        batch[\"response_2\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors_2]\n","\n","\n","    # Now where we have the responses added to the batch, let's look how the batch looks like:\n","    if batch_nr == 0:\n","      print('This is how a batch looks like')\n","      print('\\tBatch size: ', len(batch['label']))\n","      print('\\tA few labels: ', batch['label'][:2])\n","      print('\\tA few starters: ', batch['query'][:2])\n","      print('\\tThe first corresponding completions: ', batch['response_1'][:2])\n","      print('\\tThe second corresponding completions: ', batch['response_2'][:2])\n","      print('\\tThe input ids for the queries: ', batch['input_ids'][:2])\n","\n","\n","    #### Compute sentiment score\n","    texts_1 = [q + r for q, r in zip(batch[\"query\"], batch[\"response_1\"])]\n","    texts_2 = [q + r for q, r in zip(batch[\"query\"], batch[\"response_2\"])]\n","    pipe_outputs_1 = sentiment_pipe(texts_1, **sent_kwargs)\n","    pipe_outputs_2 = sentiment_pipe(texts_2, **sent_kwargs)\n","    rewards_1 = [torch.tensor(output[1][\"score\"] - output[0][\"score\"]) for output in pipe_outputs_1]\n","    rewards_2 = [torch.tensor(output[1][\"score\"] - output[0][\"score\"]) for output in pipe_outputs_2]\n","\n","    preferred_completions = []\n","    rejected_completions = []\n","\n","    for batch_example_nr, (score_1, score_2) in enumerate(zip(rewards_1, rewards_2)):\n","      if score_1 > score_2:\n","        preferred_completions.append(batch[\"response_1\"][batch_example_nr])\n","        rejected_completions.append(batch[\"response_2\"][batch_example_nr])\n","      else:\n","        preferred_completions.append(batch[\"response_2\"][batch_example_nr])\n","        rejected_completions.append(batch[\"response_1\"][batch_example_nr])\n","\n","    batch_data['rewards_1'] = rewards_1\n","    batch_data['rewards_2'] = rewards_2\n","    batch_data['query_tensors'] = query_tensors\n","    batch_data['response_tensors_1'] = response_tensors_1\n","    batch_data['response_tensors_2'] = response_tensors_2\n","    batch_data['query'] = batch['query']\n","    batch_data['preferred_completions'] = preferred_completions\n","    batch_data['rejected_completions'] = rejected_completions\n","    batch_data['batch'] = batch\n","\n","    training_batches.append(batch_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYEA54MjG5GA","outputId":"7bfef965-e778-40e6-eca0-b127a4137e10","executionInfo":{"status":"ok","timestamp":1748308535575,"user_tz":420,"elapsed":226391,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r0it [00:00, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["This is how a batch looks like\n","\tBatch size:  128\n","\tA few labels:  [tensor(0, device='cuda:0'), tensor(0, device='cuda:0')]\n","\tA few starters:  ['Pay no attention to the comments behind the curtain! The majority', 'This movie was terrible. The first half hour is much like a']\n","\tThe first corresponding completions:  [' of this film is conventional actors. Even so, you can see how reliable', ' Bruce Willis episode Bill Pullman invented. Basically,']\n","\tThe second corresponding completions:  [' of television shows together are delivered from garbage sources. Half of them are inept', ' bad romantic comedy with watered-down music. Half']\n","\tThe input ids for the queries:  [tensor([19197,   645,  3241,   284,   262,  3651,  2157,   262, 29461,     0,\n","          383,  3741], device='cuda:0'), tensor([1212, 3807,  373, 7818,   13,  383,  717, 2063, 1711,  318,  881,  588,\n","         257], device='cuda:0')]\n"]},{"output_type":"stream","name":"stderr","text":["4it [02:31, 37.76s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","6it [03:46, 37.72s/it]\n"]}]},{"cell_type":"code","source":["len(training_batches)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwPM9ZwaIE7F","outputId":"8d478247-b9db-4caa-806e-0d3a0597d51c","executionInfo":{"status":"ok","timestamp":1748308535610,"user_tz":420,"elapsed":39,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["len(training_batches[0]['query'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80O7rPY9L6ph","outputId":"7e22b7da-09c3-48f8-9c00-3636c55cade4","executionInfo":{"status":"ok","timestamp":1748308535670,"user_tz":420,"elapsed":57,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["128"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["So with limiting our batches to 4 we will have 512 paired training examples."],"metadata":{"id":"36eIJeRDLrQ0"}},{"cell_type":"markdown","source":["##2. Ok, Let's Train!\n","\n"],"metadata":{"id":"JCGTTmWoK0Dt"}},{"cell_type":"markdown","source":["### 2.1 RLHF with PPO Training\n","\n","This part is fully based on the original TRL notebook.\n","\n","#### 2.1.a Training\n","\n","Let's run a training loop using our PPO trainer:"],"metadata":{"id":"-B6C2pK-9hQZ"}},{"cell_type":"code","source":["for epoch in range(3):\n","  for batch_nr, batch in tqdm(enumerate(training_batches)):\n","    query_tensors = batch['query_tensors']\n","    response_tensors_1 = batch['response_tensors_1']\n","    response_tensors_2 = batch['response_tensors_2']\n","    rewards_1 = batch['rewards_1']\n","    rewards_2 = batch['rewards_2']\n","\n","    #### Run PPO step\n","    stats_1 = ppo_trainer.step(query_tensors, response_tensors_1, rewards_1)\n","    ppo_trainer.log_stats(stats_1, batch, rewards_1)\n","    stats_2 = ppo_trainer.step(query_tensors, response_tensors_2, rewards_2)\n","    ppo_trainer.log_stats(stats_2, stats_2, rewards_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87-VNaQcm1R-","outputId":"22172127-422c-4e50-8b48-4039205d61c1","executionInfo":{"status":"ok","timestamp":1748308732980,"user_tz":420,"elapsed":197307,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1368: UserWarning: The game logs will not be logged because the batch does not contain the keys 'query' and 'response'. \n","  warnings.warn(\n","3it [00:32, 10.78s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.11 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","4it [00:43, 10.81s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -1.55 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.18 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","5it [00:54, 10.95s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.20 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -2.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","6it [01:05, 10.95s/it]\n","0it [00:00, ?it/s]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.35 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.78 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","1it [00:10, 10.84s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -3.61 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.04 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","2it [00:21, 10.99s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.73 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -4.48 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","3it [00:32, 10.99s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -5.65 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","4it [00:43, 10.84s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -6.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -8.12 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","5it [00:54, 10.78s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -7.71 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -8.75 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","6it [01:05, 10.91s/it]\n","0it [00:00, ?it/s]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -10.90 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","1it [00:10, 10.89s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.56 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -11.31 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","2it [00:22, 11.14s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -12.95 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -12.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","3it [00:33, 11.19s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.28 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -13.88 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","4it [00:44, 11.01s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -15.14 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -16.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","5it [00:54, 10.91s/it]/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -15.23 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1289: UserWarning: KL divergence is starting to become negative: -17.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n","  warnings.warn(\n","6it [01:06, 11.01s/it]\n"]}]},{"cell_type":"markdown","source":["#### 2.1.b Testing\n","\n","Now we compare the completions of the original model with the one that was fine-tuned using RL. To do so, we first create a base set that we will also use for the other models:"],"metadata":{"id":"stZzg-Wufx-x"}},{"cell_type":"code","source":["# Let's set potentially different generation parameters for our test generations\n","gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}\n","\n","#### get a batch from the dataset\n","bs = 100\n","\n","game_data = dict()\n","dataset.set_format(\"pandas\")\n","\n","# Below, we sample from our dataset that we also trained on. Certainly not what we would do in reality,\n","# but ok for illustration. We only use the short segments at the beginning as prompts anyway.\n","\n","df_batch = dataset[:].sample(bs)\n","\n","game_data[\"query\"] = df_batch[\"query\"].tolist()\n","query_tensors = df_batch[\"input_ids\"].tolist()"],"metadata":{"id":"xqYPRXWiLmXh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we generate answers for for the prompts (query) with the reference model and the one we tuned with RLHF:"],"metadata":{"id":"Ha_q2MdyMSm9"}},{"cell_type":"code","source":["response_tensors_ref, response_tensors, KL = [], [], []\n","\n","#### get response from gpt2 and gpt2_ref\n","for i in range(bs):\n","    gen_len = output_length_sampler()\n","    output = ref_rlhf_model.generate(\n","        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n","    ).squeeze()[-gen_len:]\n","    response_tensors_ref.append(output)\n","    output = rlhf_model.generate(\n","        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n","    ).squeeze()[-gen_len:]\n","    response_tensors.append(output)\n","\n","    try:\n","      log_pi_ref = log_pi(ref_rlhf_model, output, query_tensors[i].shape[0])\n","      log_pi_tuned = log_pi(rlhf_model, output, query_tensors[i].shape[0])\n","      KL.append(log_pi_tuned - log_pi_ref)\n","    except:\n","      print(f'Iteration {i} gave an error in log pi calculations. Incorrect ratio here.')\n","      KL.append(0)\n","\n","\n","#### decode responses\n","game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n","game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n","\n","#### sentiment analysis of query/response pairs before/after\n","## We changed the reward from (output[1][\"score\"])\n","##    to (output[1][\"score\"] - output[0][\"score\"]) relative to TRL article.\n","texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n","game_data[\"reward (before)\"] = [output[1][\"score\"] - output[0][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n","\n","texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n","game_data[\"reward (after)\"] = [output[1][\"score\"] - output[0][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n","\n","\n","game_data[\"reward boost\"] = [a - b for (a, b) in zip(game_data[\"reward (after)\"],  game_data[\"reward (before)\"])]\n","\n","# measuring the KL penalty using beta = 0.2\n","game_data[\"KL Penalty (0.2 x KL)\"] = [0.2 * np.round(x, 3) for x in KL]\n","\n","\n","# store results in a dataframe\n","df_results = pd.DataFrame(game_data)\n","df_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":615},"id":"0xBB2XHcG5JS","outputId":"8f299d53-6d90-42f8-afb9-14919e50050c","executionInfo":{"status":"ok","timestamp":1748308767757,"user_tz":420,"elapsed":34627,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                query  \\\n","0   I really enjoyed this movie. It took a pretty ...   \n","1                            I loathed this film. The   \n","2              With nothing better to do I decided to   \n","3                      \"Stories of the Century\" was a   \n","4   It's common practice for a film about repressi...   \n","..                                                ...   \n","95                  Somerset Maugham's characters are   \n","96                             No, I haven't read the   \n","97               The movie looked like a walk-through   \n","98  My left foot is an epic outstanding film expla...   \n","99               This movie probably would only get a   \n","\n","                                    response (before)  \\\n","0   singing couple to give us this boring little f...   \n","1    Affairs lived up to my expectations. It's a w...   \n","2    give Fairchild's OP a try, everything's going...   \n","3          joy for me all the way through and it gets   \n","4             . That doesn't seem to be the case here   \n","..                                                ...   \n","95   a handful.<br /><br />Obviously this won't be...   \n","96   movie and will watch it again. I am attending...   \n","97   with a bunch of bad-looking bad bunch of bure...   \n","98   the power of sameness. Subtle but watchable b...   \n","99             0.5 since it been the sixth entry, but   \n","\n","                                     response (after)  reward (before)  \\\n","0   line movie like Happiness Island to get my The...         1.664254   \n","1    actors seemed to be perfectly formed and well...         4.839497   \n","2    make this film ............porn movie ....it ...        -2.681391   \n","3                  sleeper '80s gem which, as always,         5.001833   \n","4    compared to today's blockbuster flicks like G...         0.287928   \n","..                                                ...              ...   \n","95   top quality actors - Mergers was a great soun...        -3.122725   \n","96   V collection of lithographs yet - almost all ...         0.613023   \n","97  s movie.<br /><br />Ivy Williamson is great as...        -5.081180   \n","98   the loss of my incarnation family in one chap...         5.532346   \n","99   thumbs up out of 72 points generated Kubrick ...        -2.416297   \n","\n","    reward (after)  reward boost  KL Penalty (0.2 x KL)  \n","0         4.935911      3.271657                -0.0280  \n","1         1.294552     -3.544945                -0.1072  \n","2        -4.286988     -1.605597                 1.2242  \n","3         4.884953     -0.116880                -0.0898  \n","4         1.852924      1.564997                 0.0000  \n","..             ...           ...                    ...  \n","95        4.904378      8.027103                 2.8094  \n","96        0.622417      0.009394                -1.3556  \n","97        2.437100      7.518280                -0.6064  \n","98        5.141888     -0.390458                 2.3860  \n","99       -1.172838      1.243460                 0.3862  \n","\n","[100 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-3d973c66-827e-4ea9-8bca-851364306da9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>query</th>\n","      <th>response (before)</th>\n","      <th>response (after)</th>\n","      <th>reward (before)</th>\n","      <th>reward (after)</th>\n","      <th>reward boost</th>\n","      <th>KL Penalty (0.2 x KL)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I really enjoyed this movie. It took a pretty ...</td>\n","      <td>singing couple to give us this boring little f...</td>\n","      <td>line movie like Happiness Island to get my The...</td>\n","      <td>1.664254</td>\n","      <td>4.935911</td>\n","      <td>3.271657</td>\n","      <td>-0.0280</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I loathed this film. The</td>\n","      <td>Affairs lived up to my expectations. It's a w...</td>\n","      <td>actors seemed to be perfectly formed and well...</td>\n","      <td>4.839497</td>\n","      <td>1.294552</td>\n","      <td>-3.544945</td>\n","      <td>-0.1072</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>With nothing better to do I decided to</td>\n","      <td>give Fairchild's OP a try, everything's going...</td>\n","      <td>make this film ............porn movie ....it ...</td>\n","      <td>-2.681391</td>\n","      <td>-4.286988</td>\n","      <td>-1.605597</td>\n","      <td>1.2242</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"Stories of the Century\" was a</td>\n","      <td>joy for me all the way through and it gets</td>\n","      <td>sleeper '80s gem which, as always,</td>\n","      <td>5.001833</td>\n","      <td>4.884953</td>\n","      <td>-0.116880</td>\n","      <td>-0.0898</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>It's common practice for a film about repressi...</td>\n","      <td>. That doesn't seem to be the case here</td>\n","      <td>compared to today's blockbuster flicks like G...</td>\n","      <td>0.287928</td>\n","      <td>1.852924</td>\n","      <td>1.564997</td>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Somerset Maugham's characters are</td>\n","      <td>a handful.&lt;br /&gt;&lt;br /&gt;Obviously this won't be...</td>\n","      <td>top quality actors - Mergers was a great soun...</td>\n","      <td>-3.122725</td>\n","      <td>4.904378</td>\n","      <td>8.027103</td>\n","      <td>2.8094</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>No, I haven't read the</td>\n","      <td>movie and will watch it again. I am attending...</td>\n","      <td>V collection of lithographs yet - almost all ...</td>\n","      <td>0.613023</td>\n","      <td>0.622417</td>\n","      <td>0.009394</td>\n","      <td>-1.3556</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>The movie looked like a walk-through</td>\n","      <td>with a bunch of bad-looking bad bunch of bure...</td>\n","      <td>s movie.&lt;br /&gt;&lt;br /&gt;Ivy Williamson is great as...</td>\n","      <td>-5.081180</td>\n","      <td>2.437100</td>\n","      <td>7.518280</td>\n","      <td>-0.6064</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>My left foot is an epic outstanding film expla...</td>\n","      <td>the power of sameness. Subtle but watchable b...</td>\n","      <td>the loss of my incarnation family in one chap...</td>\n","      <td>5.532346</td>\n","      <td>5.141888</td>\n","      <td>-0.390458</td>\n","      <td>2.3860</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>This movie probably would only get a</td>\n","      <td>0.5 since it been the sixth entry, but</td>\n","      <td>thumbs up out of 72 points generated Kubrick ...</td>\n","      <td>-2.416297</td>\n","      <td>-1.172838</td>\n","      <td>1.243460</td>\n","      <td>0.3862</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows Ã— 7 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d973c66-827e-4ea9-8bca-851364306da9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3d973c66-827e-4ea9-8bca-851364306da9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3d973c66-827e-4ea9-8bca-851364306da9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-2ba1cb43-95fd-48d0-9d03-bb60655a6419\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2ba1cb43-95fd-48d0-9d03-bb60655a6419')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-2ba1cb43-95fd-48d0-9d03-bb60655a6419 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","  <div id=\"id_cc008325-c1bc-4264-94cb-02c582e8b3cb\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_cc008325-c1bc-4264-94cb-02c582e8b3cb button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_results');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_results","summary":"{\n  \"name\": \"df_results\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"Call me stupid, but I absolutely loved the 2001\",\n          \"Reading a wide variety of \\\"Scoop\\\" reviews over\",\n          \"The most positive thing I can say for this dull witted\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response (before)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \" documentary \\\"When Women Look At Border.\\\" Buzz Aldrin wonderful performance was fantastic too.<|endoftext|>\",\n          \" and over again for a few good reasons;1) I really enjoy loud music;2\",\n          \" guy is that Eben Peters has done just his\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response (after)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \" film Django. It prides itself - its awesome control of nature, aplomb lush\",\n          \" the years. Name your own artistic palette, medium and go! Crystal Gordon Curry is perfect\",\n          \" maceronscombe masterpiece (especially after 108 years\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reward (before)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3948473346197043,\n        \"min\": -5.620850563049316,\n        \"max\": 5.53234601020813,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          4.382951259613037,\n          4.341569662094116,\n          -4.277134418487549\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reward (after)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.78579129841842,\n        \"min\": -5.108062505722046,\n        \"max\": 5.363964319229126,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          4.8544981479644775,\n          5.203696250915527,\n          -2.6056853532791138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reward boost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0364707623240417,\n        \"min\": -6.426461338996887,\n        \"max\": 9.221847295761108,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          0.47154688835144043,\n          0.8621265888214111,\n          1.671449065208435\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KL Penalty (0.2 x KL)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1146775682393943,\n        \"min\": -3.2513999938964844,\n        \"max\": 3.5536000728607178,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          -1.5793999433517456,\n          1.0916000604629517,\n          1.0124000310897827\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Ok.... Some better some worse. But did we improve in aggregate?"],"metadata":{"id":"eCvpuiJ7h7Mv"}},{"cell_type":"code","source":["print(\"mean:\")\n","display(df_results[[\"reward (before)\", \"reward (after)\"]].mean())\n","print()\n","print(\"median:\")\n","display(df_results[[\"reward (before)\", \"reward (after)\"]].median())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"dP9JaBgFIuTU","outputId":"f27e70b4-47d3-443b-c597-440c163ceaf8","executionInfo":{"status":"ok","timestamp":1748308767794,"user_tz":420,"elapsed":40,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean:\n"]},{"output_type":"display_data","data":{"text/plain":["reward (before)    0.458877\n","reward (after)     2.099053\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>reward (before)</th>\n","      <td>0.458877</td>\n","    </tr>\n","    <tr>\n","      <th>reward (after)</th>\n","      <td>2.099053</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","median:\n"]},{"output_type":"display_data","data":{"text/plain":["reward (before)    1.007019\n","reward (after)     2.695770\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>reward (before)</th>\n","      <td>1.007019</td>\n","    </tr>\n","    <tr>\n","      <th>reward (after)</th>\n","      <td>2.695770</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{}}]},{"cell_type":"markdown","source":["What about average reward boost vs penalties? Let us get the mean reward and the mean KL penalty (in this case 0.2 x KL Divergence) as well:"],"metadata":{"id":"9EgwmwQlOxNl"}},{"cell_type":"code","source":["print(\"mean Reward boost:\")\n","display(df_results[[\"reward boost\"]].mean())\n","print()\n","print(\"mean KL Penalty (0.2 x KL):\")\n","display(df_results[[\"KL Penalty (0.2 x KL)\"]].mean())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"iGcde0pAOvD7","outputId":"3fbe2c7f-c9bf-4c3f-c8a0-864f0ec14de4","executionInfo":{"status":"ok","timestamp":1748308767894,"user_tz":420,"elapsed":99,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mean Reward boost:\n"]},{"output_type":"display_data","data":{"text/plain":["reward boost    1.640176\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>reward boost</th>\n","      <td>1.640176</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","mean KL Penalty (0.2 x KL):\n"]},{"output_type":"display_data","data":{"text/plain":["KL Penalty (0.2 x KL)    0.134226\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>KL Penalty (0.2 x KL)</th>\n","      <td>0.134226</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{}}]},{"cell_type":"markdown","source":["We did! (KL Divergence term negative? Must have been a bit of a statistical fluke!)\n","\n"],"metadata":{"id":"l4mseSMUiAUp"}},{"cell_type":"markdown","source":["#### 2.1.c Some Tests & Loose Ends\n","\n","Let us now play with some examples ourselves:"],"metadata":{"id":"vQAaAp_0wmOM"}},{"cell_type":"code","source":["my_starter = \"When I was sitting in\"\n","\n","inputs = tokenizer(my_starter, return_tensors=\"pt\").to(device)\n","\n","orinal_output = ref_rlhf_model.generate(**inputs, labels=inputs.input_ids, max_new_tokens=gen_len, **gen_kwargs\n","    )\n","new_output = rlhf_model.generate(**inputs, labels=inputs.input_ids, max_new_tokens=gen_len, **gen_kwargs\n","    )\n","\n","original_completion = tokenizer.decode(orinal_output[0])\n","new_completion = tokenizer.decode(new_output[0])\n","\n","print('')\n","print(\"Completion from original model: \", original_completion)\n","print(\"Completion from tuned model: \", new_completion)\n","\n","print('')\n","sentiment_vals = sentiment_pipe([original_completion,\n","                                 new_completion],\n","                                **sent_kwargs)\n","\n","original_completion_sentiments, new_completion_sentiments = sentiment_vals\n","\n","print(\"Logit values for original completion: \", str(original_completion_sentiments))\n","print(\"Logit values for new completion: \", str(new_completion_sentiments))\n","\n","sentiment_vals\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9cX6ePGpRMC","outputId":"752b7da0-6629-4c4d-d0df-8ef4b9d5c9c4","executionInfo":{"status":"ok","timestamp":1748308768185,"user_tz":420,"elapsed":292,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Completion from original model:  When I was sitting in the living room a few hours ago, and I watched\n","Completion from tuned model:  When I was sitting in the theatre looking shocked by the love song by Buddy Holly\n","\n","Logit values for original completion:  [{'label': 'NEGATIVE', 'score': -0.7268210053443909}, {'label': 'POSITIVE', 'score': 0.8587529063224792}]\n","Logit values for new completion:  [{'label': 'NEGATIVE', 'score': -1.3171314001083374}, {'label': 'POSITIVE', 'score': 1.5196523666381836}]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["[[{'label': 'NEGATIVE', 'score': -0.7268210053443909},\n","  {'label': 'POSITIVE', 'score': 0.8587529063224792}],\n"," [{'label': 'NEGATIVE', 'score': -1.3171314001083374},\n","  {'label': 'POSITIVE', 'score': 1.5196523666381836}]]"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["What does this imply for the likelihood that either example is positive (as viewed by our reward model):"],"metadata":{"id":"62yQMyOnw5QR"}},{"cell_type":"code","source":["???"],"metadata":{"id":"u3gskzJ1rJHy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3256a516-f8fe-4856-e05c-be3117eb02de","executionInfo":{"status":"ok","timestamp":1748308768256,"user_tz":420,"elapsed":70,"user":{"displayName":"Mark H Butler","userId":"07074293337156307795"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Object `?` not found.\n"]}]},{"cell_type":"markdown","source":["Cool! Now we saw some of the components of the InstructGPT paper directly!\n"],"metadata":{"id":"sT2y0Osk25UI"}}]}